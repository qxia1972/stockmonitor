"""
æ¯æ—¥æ•°æ®å¤„ç†ä½œä¸šå®šä¹‰ (Daily Data Processing Job Definitions)

å®ç°å®Œæ•´çš„æ¯æ—¥æ•°æ®å¤„ç†æµç¨‹ï¼š
1. æ•°æ®åŒæ­¥ (Sync+Completion) - å¢é‡æ›´æ–° + ç¼ºå¤±è¡¥å…¨
2. æŠ€æœ¯æŒ‡æ ‡è®¡ç®— (Technical Indicators)
3. è¯„åˆ†å­˜ç›˜ (Scoring & Save)

åŸºäºDagsteræ¡†æ¶ï¼Œæ”¯æŒè°ƒåº¦ã€ç›‘æ§å’Œé”™è¯¯å¤„ç†
"""

from typing import Dict, Any, List, Optional
from pathlib import Path
from datetime import datetime, timedelta, date
import logging
from dagster import (
    job, op, OpExecutionContext, AssetMaterialization, Output, Failure,
    RetryPolicy, RunConfig, run_status_sensor, DagsterRunStatus,
    schedule
)
from dagster import get_dagster_logger
import polars as pl

from modules.orchestration.pipeline_manager import pipeline_manager
from modules.processing_functions import (
    calculate_indicators, calculate_scores, save_data
)
from networks.rqdatac_data_loader import get_rqdatac_data_loader
from modules.util.utilities import (
    get_trading_dates, get_latest_trading_date, get_previous_trading_date
)
from networks.trading_calendar_api import (
    get_latest_trading_day, get_trading_day_n_days_ago,
    get_next_trading_day, get_trading_calendar
)

logger = logging.getLogger(__name__)

# ===== é…ç½®å¸¸é‡ =====

# é‡è¯•ç­–ç•¥
STANDARD_RETRY = RetryPolicy(max_retries=3, delay=30.0)
CRITICAL_RETRY = RetryPolicy(max_retries=5, delay=60.0)

# é»˜è®¤å‚æ•°
DEFAULT_COMPLETION_DAYS = 30
DEFAULT_TARGET_STOCKS = []    # é»˜è®¤ç›®æ ‡è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œè¿è¡Œæ—¶åŠ¨æ€è·å–

# ===== æ•°æ®è¡¥å…¨è¾…åŠ©å‡½æ•° =====

def _calculate_completion_range(existing_data: Optional[pl.DataFrame], max_fill_days: int = 30) -> Optional[tuple[date, date]]:
    """
    è®¡ç®—éœ€è¦è¡¥å…¨çš„æ•°æ®æ—¶é—´èŒƒå›´

    Args:
        existing_data: ç°æœ‰æ•°æ®
        max_fill_days: æœ€å¤§è¡¥å…¨å¤©æ•°

    Returns:
        (start_date, end_date): è¡¥å…¨çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼Œå¦‚æœæ— éœ€è¡¥å…¨è¿”å›None
    """
    # è·å–æ•°æ®åŠ è½½å™¨
    data_loader = get_rqdatac_data_loader()

    if existing_data is None or existing_data.is_empty():
        # åœºæ™¯1: æ— å†å²æ•°æ®
        latest_trading_str = get_latest_trading_date(data_loader)
        if latest_trading_str:
            end_date = datetime.strptime(latest_trading_str, '%Y-%m-%d').date()
        else:
            # å¦‚æœæ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
            end_date = datetime.now().date()
            logger.warning("æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºç»“æŸæ—¥æœŸ")

        start_date = end_date - timedelta(days=max_fill_days)
        return start_date, end_date
    else:
        # åœºæ™¯2: æœ‰å†å²æ•°æ®
        existing_dates = existing_data.select('date').unique().to_series().to_list()
        existing_dates = [d.date() if hasattr(d, 'date') else d for d in existing_dates]

        # è¿‡æ»¤æ‰Noneå€¼
        existing_dates = [d for d in existing_dates if d is not None]

        if not existing_dates:
            return None

        latest_existing = max(existing_dates)
        latest_trading_str = get_latest_trading_date(data_loader)
        if latest_trading_str:
            latest_trading = datetime.strptime(latest_trading_str, '%Y-%m-%d').date()
        else:
            # å¦‚æœæ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
            latest_trading = datetime.now().date()
            logger.warning("æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸè¿›è¡Œæ¯”è¾ƒ")

        # ç¡®ä¿åŒ…å«æœ€æ–°å·²å®Œæˆäº¤æ˜“æ—¥
        if latest_trading > latest_existing:
            end_date = latest_trading
            start_date = latest_existing + timedelta(days=1)
            return start_date, end_date
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„æ—¥æœŸ
            missing_dates = _find_missing_trading_dates(existing_dates, data_loader)
            if missing_dates:
                start_date = min(missing_dates)
                end_date = max(missing_dates)
                return start_date, end_date
            else:
                return None  # æ— éœ€è¡¥å…¨


def _find_missing_trading_dates(existing_dates: List[date], data_loader) -> List[date]:
    """
    è¯†åˆ«ç¼ºå¤±çš„äº¤æ˜“æ—¥

    Args:
        existing_dates: ç°æœ‰æ•°æ®æ—¥æœŸåˆ—è¡¨
        data_loader: æ•°æ®åŠ è½½å™¨å®ä¾‹

    Returns:
        missing_dates: ç¼ºå¤±çš„äº¤æ˜“æ—¥åˆ—è¡¨
    """
    # è·å–äº¤æ˜“æ—¥å†
    min_date_str = min(existing_dates).strftime('%Y-%m-%d')
    max_date_str = max(existing_dates).strftime('%Y-%m-%d')
    trading_calendar_strs = get_trading_dates(data_loader, min_date_str, max_date_str)

    # è½¬æ¢ä¸ºdatetime.dateå¯¹è±¡
    trading_calendar = [datetime.strptime(d, '%Y-%m-%d').date() for d in trading_calendar_strs]
    existing_set = set(existing_dates)

    # æ‰¾åˆ°ç°æœ‰æ•°æ®æ—¶é—´èŒƒå›´å†…çš„ç¼ºå¤±äº¤æ˜“æ—¥
    min_date = min(existing_dates)
    max_date = max(existing_dates)

    expected_dates = [
        date for date in trading_calendar
        if min_date <= date <= max_date
    ]

    missing_dates = [
        date for date in expected_dates
        if date not in existing_set
    ]

    return missing_dates
    """
    åˆ†ææ•°æ®çš„å®Œæ•´æ€§

    Args:
        data: æ•°æ®DataFrame
        expected_stocks: æœŸæœ›çš„è‚¡ç¥¨åˆ—è¡¨

    Returns:
        å®Œæ•´æ€§åˆ†æç»“æœ
    """
    analysis = {
        'is_complete': True,
        'missing_stocks': [],
        'missing_dates': {},
        'total_records': len(data),
        'expected_stocks': len(expected_stocks),
        'actual_stocks': 0
    }

    if data.is_empty():
        analysis['is_complete'] = False
        analysis['missing_stocks'] = expected_stocks.copy()
        return analysis

    # æ£€æŸ¥è‚¡ç¥¨å®Œæ•´æ€§
    actual_stocks = data.select('order_book_id').unique().to_series().to_list()
    analysis['actual_stocks'] = len(actual_stocks)

    missing_stocks = [stock for stock in expected_stocks if stock not in actual_stocks]
    if missing_stocks:
        analysis['is_complete'] = False
        analysis['missing_stocks'] = missing_stocks

    # æ£€æŸ¥æ—¥æœŸå®Œæ•´æ€§
    if not data.is_empty():
        dates = data.select('date').unique().to_series().to_list()
        dates = [d.date() if hasattr(d, 'date') else datetime.strptime(d, '%Y-%m-%d').date() for d in dates if d is not None]

        if dates:
            min_date = min(dates)
            max_date = max(dates)

            # ä¸ºæ¯ä¸ªè‚¡ç¥¨æ£€æŸ¥ç¼ºå¤±çš„æ—¥æœŸ
            for stock in expected_stocks:
                stock_data = data.filter(pl.col('order_book_id') == stock)
                if not stock_data.is_empty():
                    stock_dates = stock_data.select('date').to_series().to_list()
                    stock_dates = [d.date() if hasattr(d, 'date') else datetime.strptime(d, '%Y-%m-%d').date() for d in stock_dates if d is not None]

                    expected_dates = []
                    current = min_date
                    while current <= max_date:
                        if current.weekday() < 5:  # å‘¨ä¸€åˆ°å‘¨äº”
                            expected_dates.append(current)
                        current += timedelta(days=1)

                    missing_dates = [d for d in expected_dates if d not in stock_dates]
                    if missing_dates:
                        analysis['is_complete'] = False
                        analysis['missing_dates'][stock] = missing_dates

    return analysis
    """
    è¯†åˆ«ç¼ºå¤±çš„äº¤æ˜“æ—¥

    Args:
        existing_dates: ç°æœ‰æ•°æ®æ—¥æœŸåˆ—è¡¨
        data_loader: æ•°æ®åŠ è½½å™¨å®ä¾‹

    Returns:
        missing_dates: ç¼ºå¤±çš„äº¤æ˜“æ—¥åˆ—è¡¨
    """
    # è·å–äº¤æ˜“æ—¥å†
    min_date_str = min(existing_dates).strftime('%Y-%m-%d')
    max_date_str = max(existing_dates).strftime('%Y-%m-%d')
    trading_calendar_strs = get_trading_dates(data_loader, min_date_str, max_date_str)

    # è½¬æ¢ä¸ºdatetime.dateå¯¹è±¡
    trading_calendar = [datetime.strptime(d, '%Y-%m-%d').date() for d in trading_calendar_strs]
    existing_set = set(existing_dates)

    # æ‰¾åˆ°ç°æœ‰æ•°æ®æ—¶é—´èŒƒå›´å†…çš„ç¼ºå¤±äº¤æ˜“æ—¥
    min_date = min(existing_dates)
    max_date = max(existing_dates)

    expected_dates = [
        date for date in trading_calendar
        if min_date <= date <= max_date
    ]

    missing_dates = [
        date for date in expected_dates
        if date not in existing_set
    ]

    return missing_dates


def _merge_completion_data(existing_data: Optional[pl.DataFrame], new_data: pl.DataFrame) -> pl.DataFrame:
    """
    åˆå¹¶è¡¥å…¨æ•°æ®åˆ°ç°æœ‰æ•°æ®

    Args:
        existing_data: ç°æœ‰æ•°æ®DataFrame
        new_data: æ–°è·å–çš„æ•°æ®DataFrame

    Returns:
        merged_data: åˆå¹¶åçš„æ•°æ®
    """
    if existing_data is None or existing_data.is_empty():
        return new_data

    # åˆå¹¶æ•°æ®
    merged_data = pl.concat([existing_data, new_data])

    # å»é‡å¹¶æ’åº
    merged_data = merged_data.unique(subset=['date', 'code'])
    merged_data = merged_data.sort(['code', 'date'])

    return merged_data


def _calculate_quality_score(data: pl.DataFrame) -> float:
    """
    è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†

    Args:
        data: æ•°æ®DataFrame

    Returns:
        quality_score: è´¨é‡è¯„åˆ† (0-1)
    """
    if data.is_empty():
        return 0.0

    # åŸºç¡€è´¨é‡æŒ‡æ ‡
    total_records = len(data)

    # è®¡ç®—ç©ºå€¼æ€»æ•°
    null_counts = []
    for col in data.columns:
        null_count = data.select(pl.col(col).is_null().sum()).item()
        null_counts.append(null_count)

    total_nulls = sum(null_counts)

    # è®¡ç®—ç©ºå€¼ç‡
    total_cells = total_records * len(data.columns)
    null_ratio = total_nulls / total_cells if total_cells > 0 else 0.0

    # ç®€å•è´¨é‡è¯„åˆ†ï¼š1 - ç©ºå€¼ç‡
    quality_score = 1.0 - null_ratio

    return max(0.0, min(1.0, quality_score))


def complete_market_data_inline(order_book_ids: List[str], existing_data: Optional[pl.DataFrame] = None,
                               max_fill_days: int = 30, quality_threshold: float = 0.8) -> pl.DataFrame:
    """
    å†…è”æ•°æ®è¡¥å…¨å‡½æ•° - ç›´æ¥é›†æˆåˆ°æ—¥åŒæ­¥ä»»åŠ¡ä¸­

    Args:
        order_book_ids: RQDatacè‚¡ç¥¨ä»£ç åˆ—è¡¨
        existing_data: ç°æœ‰æ•°æ®ï¼Œå¦‚æœä¸ºNoneè¡¨ç¤ºæ— å†å²æ•°æ®
        max_fill_days: æœ€å¤§è¡¥å…¨å¤©æ•°
        quality_threshold: è´¨é‡é˜ˆå€¼

    Returns:
        completed_data: è¡¥å…¨åçš„æ•°æ®
    """
    logger = logging.getLogger(__name__)

    try:
        logger.info(f"å¼€å§‹æ•°æ®è¡¥å…¨ï¼Œè‚¡ç¥¨æ•°é‡: {len(order_book_ids)}")

        # å¦‚æœæ²¡æœ‰ç°æœ‰æ•°æ®ï¼Œç›´æ¥è·å–æ–°æ•°æ®
        if existing_data is None or existing_data.is_empty():
            logger.info("æ— å†å²æ•°æ®ï¼Œæ‰§è¡Œå…¨é‡è·å–")
            loader = get_rqdatac_data_loader()
            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=max_fill_days)

            completed_data = loader.get_ohlcv_data(
                symbols=order_book_ids,
                start_date=start_date.strftime('%Y-%m-%d'),
                end_date=end_date.strftime('%Y-%m-%d')
            )

            if completed_data is None or completed_data.is_empty():
                logger.warning("æœªè·å–åˆ°ä»»ä½•æ•°æ®")
                return pl.DataFrame()

            return completed_data

        # åˆ†æç°æœ‰æ•°æ®çš„å®Œæ•´æ€§ - ç®€åŒ–ä¸ºåŸºæœ¬æ£€æŸ¥
        if existing_data.is_empty():
            missing_analysis = {'is_complete': False, 'missing_stocks': order_book_ids, 'missing_dates': {}}
        else:
            # ç®€å•æ£€æŸ¥ï¼šå¦‚æœè®°å½•æ•°æ˜æ˜¾å°‘äºæœŸæœ›å€¼ï¼Œè®¤ä¸ºä¸å®Œæ•´
            expected_records = len(order_book_ids) * 5  # ç²—ç•¥ä¼°è®¡
            is_complete = len(existing_data) >= expected_records * 0.8  # 80%å®Œæ•´æ€§é˜ˆå€¼
            missing_analysis = {
                'is_complete': is_complete,
                'missing_stocks': [],
                'missing_dates': {}
            }

        logger.info(f"æ•°æ®å®Œæ•´æ€§åˆ†æ: è®°å½•æ•°={len(existing_data)}, å®Œæ•´={missing_analysis['is_complete']}")

        # å¦‚æœæ•°æ®å·²ç»å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨
        if missing_analysis['is_complete']:
            logger.info("æ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨")
            return existing_data

        # æ ¹æ®ç¼ºå¤±æƒ…å†µç¡®å®šè¡¥å…¨ç­–ç•¥
        completion_range = _calculate_completion_range(existing_data, max_fill_days)

        if completion_range is None:
            logger.info("æ— éœ€è¡¥å…¨æ•°æ®")
            return existing_data

        # æ‰§è¡Œè¡¥å…¨
        loader = get_rqdatac_data_loader()
        start_date, end_date = completion_range
        logger.info(f"è¡¥å…¨æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")

        # è·å–è¡¥å…¨æ•°æ®
        missing_data = loader.get_ohlcv_data(
            symbols=order_book_ids,
            start_date=start_date.strftime('%Y-%m-%d'),
            end_date=end_date.strftime('%Y-%m-%d')
        )

        if missing_data is None or missing_data.is_empty():
            logger.warning("æœªè·å–åˆ°è¡¥å…¨æ•°æ®")
            return existing_data

        # åˆå¹¶æ•°æ®
        completed_data = pl.concat([existing_data, missing_data])
        completed_data = completed_data.unique(subset=['order_book_id', 'date'])
        completed_data = completed_data.sort(['order_book_id', 'date'])

        logger.info(f"è¡¥å…¨åè®°å½•æ•°: {len(completed_data)} (å¢åŠ : {len(completed_data) - len(existing_data)})")

        return completed_data

    except Exception as e:
        logger.error(f"æ•°æ®è¡¥å…¨å¤±è´¥: {e}")
        raise


# ===== åŸºç¡€æ“ä½œå®šä¹‰ =====

@op(
    name="get_target_stocks",
    description="è·å–ç›®æ ‡è‚¡ç¥¨åˆ—è¡¨",
    retry_policy=STANDARD_RETRY
)
def get_target_stocks_op(context: OpExecutionContext) -> List[str]:
    """è·å–ç›®æ ‡è‚¡ç¥¨åˆ—è¡¨"""
    logger = get_dagster_logger()

    try:
        # ä»é…ç½®è·å–ç›®æ ‡è‚¡ç¥¨
        target_stocks = None
        if context.op_config:
            target_stocks = context.op_config.get("target_stocks")

        # å¦‚æœé…ç½®ä¸­æ²¡æœ‰æŒ‡å®šç›®æ ‡è‚¡ç¥¨æˆ–ä¸ºç©ºï¼Œåˆ™ä»RQDatacè·å–å…¨éƒ¨æ²ªæ·±è‚¡ç¥¨
        if not target_stocks:
            logger.info("âš ï¸ æœªé…ç½®ç›®æ ‡è‚¡ç¥¨ï¼Œä»RQDatacè·å–å…¨éƒ¨æ²ªæ·±è‚¡ç¥¨åˆ—è¡¨")
            loader = get_rqdatac_data_loader()

            # è·å–å…¨éƒ¨æ²ªæ·±è‚¡ç¥¨ï¼ˆAè‚¡ï¼‰
            instruments_df = loader.get_instruments(
                instrument_type="CS",  # CS = Common Stock
                market="cn"
            )

            # è¿‡æ»¤å‡ºéSTï¼Œæ´»è·ƒçš„æ²ªæ·±è‚¡ç¥¨
            if len(instruments_df) > 0:
                # è¿‡æ»¤æ¡ä»¶ï¼šçŠ¶æ€ä¸ºæ´»è·ƒï¼Œä¸”ä¸ºæ²ªæ·±å¸‚åœºï¼Œæ’é™¤STã€PTç­‰ç‰¹æ®Šè‚¡ç¥¨
                active_stocks = instruments_df.filter(
                    (pl.col("status") == "Active") &
                    (pl.col("order_book_id").str.contains(r"\.(XSHG|XSHE)$")) &
                    (~pl.col("symbol").str.contains(r"ST|PT|\*"))  # æ’é™¤STã€PTã€*STç­‰è‚¡ç¥¨
                )

                target_stocks = active_stocks.select("order_book_id").to_series().to_list()
                logger.info(f"ğŸ“Š ä»RQDatacè·å–åˆ° {len(target_stocks)} åªæ´»è·ƒæ²ªæ·±è‚¡ç¥¨ï¼ˆå·²æ’é™¤ST/PTè‚¡ç¥¨ï¼‰")
            else:
                # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨åˆ—è¡¨
                logger.warning("âŒ ä»RQDatacè·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨åˆ—è¡¨")
                target_stocks = DEFAULT_TARGET_STOCKS
        else:
            logger.info(f"âœ… ä»é…ç½®è·å–åˆ° {len(target_stocks)} åªç›®æ ‡è‚¡ç¥¨")

        logger.info(f"ğŸ¯ æœ€ç»ˆç›®æ ‡è‚¡ç¥¨æ•°é‡: {len(target_stocks)}")
        return target_stocks

    except Exception as e:
        logger.error(f"âŒ è·å–ç›®æ ‡è‚¡ç¥¨å¤±è´¥: {e}")
        # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤è‚¡ç¥¨åˆ—è¡¨
        logger.warning(f"âš ï¸ ä½¿ç”¨é»˜è®¤è‚¡ç¥¨åˆ—è¡¨: {DEFAULT_TARGET_STOCKS}")
        return DEFAULT_TARGET_STOCKS


@op(
    name="get_trading_calendar",
    description="è·å–äº¤æ˜“æ—¥å†",
    retry_policy=STANDARD_RETRY
)
def get_trading_calendar_op(context: OpExecutionContext) -> List[str]:
    """è·å–äº¤æ˜“æ—¥å†ï¼Œç¡®å®šæœ€æ–°çš„å·²æ”¶ç›˜äº¤æ˜“æ—¥"""
    logger = get_dagster_logger()

    try:
        loader = get_rqdatac_data_loader()

        # è·å–æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥çš„æ—¥å†
        if context.op_config:
            calendar_days = context.op_config.get("calendar_days", 30)
        else:
            calendar_days = 30
        end_date = datetime.now().strftime("%Y-%m-%d")

        # ä»RQDatacè·å–äº¤æ˜“æ—¥å†
        trading_dates = loader.get_trading_calendar(
            market="cn",
            start_date=(datetime.now() - timedelta(days=calendar_days)).strftime("%Y-%m-%d"),
            end_date=end_date
        )

        # è¿‡æ»¤å‡ºå·²æ”¶ç›˜çš„äº¤æ˜“æ—¥ï¼ˆä¸åŒ…æ‹¬ä»Šå¤©ï¼Œå¦‚æœä»Šå¤©è¿˜æ²¡æ”¶ç›˜ï¼‰
        today = datetime.now().date()
        completed_trading_dates = [
            date for date in trading_dates
            if datetime.strptime(date, "%Y-%m-%d").date() < today
        ]

        if not completed_trading_dates:
            raise ValueError("æ²¡æœ‰å·²æ”¶ç›˜çš„äº¤æ˜“æ—¥")

        latest_trading_date = max(completed_trading_dates)
        logger.info(f"è·å–åˆ°äº¤æ˜“æ—¥å†: æœ€æ–°å·²æ”¶ç›˜äº¤æ˜“æ—¥ {latest_trading_date}")

        return completed_trading_dates

    except Exception as e:
        logger.error(f"è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
        raise Failure(f"Failed to get trading calendar: {e}")


@op(
    name="check_data_integrity",
    description="æ£€æŸ¥æ•°æ®å®Œæ•´æ€§",
    retry_policy=STANDARD_RETRY
)
def check_data_integrity_op(context: OpExecutionContext, symbols: List[str], trading_dates: List[str]) -> Dict[str, Any]:
    """æ£€æŸ¥å­˜ç›˜æ–‡ä»¶ä¸­çš„æ•°æ®å®Œæ•´æ€§ï¼Œç¡®å®šéœ€è¦åŒæ­¥çš„å†…å®¹"""
    logger = get_dagster_logger()

    try:
        # æ£€æŸ¥æ•°æ®ç›®å½•
        data_dir = Path("data")
        if not data_dir.exists():
            logger.warning("æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œå°†è¿›è¡Œå…¨é‡åŒæ­¥")
            return {
                "needs_full_sync": True,
                "missing_dates": trading_dates,
                "latest_date": None,
                "symbols_to_sync": symbols
            }

        # æ£€æŸ¥æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        latest_data_file = None
        latest_date = None

        for file_path in data_dir.glob("daily_results_*.parquet"):
            if file_path.is_file():
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                date_str = file_path.stem.replace("daily_results_", "")
                try:
                    file_date = datetime.strptime(date_str, "%Y%m%d_%H%M%S").date()
                    if latest_date is None or file_date > latest_date:
                        latest_date = file_date
                        latest_data_file = file_path
                except ValueError:
                    continue

        if latest_data_file is None:
            logger.info("æœªæ‰¾åˆ°å†å²æ•°æ®æ–‡ä»¶ï¼Œå°†è¿›è¡Œå…¨é‡åŒæ­¥")
            return {
                "needs_full_sync": True,
                "missing_dates": trading_dates,
                "latest_date": None,
                "symbols_to_sync": symbols
            }

        # è¯»å–æœ€æ–°æ•°æ®æ–‡ä»¶ï¼Œæ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        try:
            existing_data = pl.read_parquet(str(latest_data_file))
            existing_dates = existing_data.select("date").unique().to_series().to_list()
            existing_symbols = existing_data.select("order_book_id").unique().to_series().to_list()

            # è½¬æ¢ä¸ºæ—¥æœŸå¯¹è±¡è¿›è¡Œæ¯”è¾ƒ
            existing_date_objs = [datetime.strptime(d, "%Y-%m-%d").date() for d in existing_dates]
            trading_date_objs = [datetime.strptime(d, "%Y-%m-%d").date() for d in trading_dates]

            # æ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸ
            missing_dates = [
                d.strftime("%Y-%m-%d") for d in trading_date_objs
                if d not in existing_date_objs
            ]

            # æ‰¾å‡ºç¼ºå¤±çš„è‚¡ç¥¨
            missing_symbols = [s for s in symbols if s not in existing_symbols]

            # ç¡®å®šéœ€è¦åŒæ­¥çš„è‚¡ç¥¨ï¼ˆå…¨éƒ¨è‚¡ç¥¨éƒ½éœ€è¦æ£€æŸ¥æœ€æ–°æ•°æ®ï¼‰
            symbols_to_sync = symbols

            result = {
                "needs_full_sync": False,
                "missing_dates": missing_dates,
                "latest_date": latest_date.strftime("%Y-%m-%d") if latest_date else None,
                "symbols_to_sync": symbols_to_sync,
                "missing_symbols": missing_symbols,
                "existing_records": len(existing_data),
                "existing_dates_count": len(existing_dates),
                "existing_symbols_count": len(existing_symbols)
            }

            logger.info(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ: ç¼ºå¤± {len(missing_dates)} ä¸ªäº¤æ˜“æ—¥, ç¼ºå¤± {len(missing_symbols)} åªè‚¡ç¥¨")
            return result

        except Exception as e:
            logger.warning(f"è¯»å–å†å²æ•°æ®æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†è¿›è¡Œå…¨é‡åŒæ­¥")
            return {
                "needs_full_sync": True,
                "missing_dates": trading_dates,
                "latest_date": None,
                "symbols_to_sync": symbols
            }

    except Exception as e:
        logger.error(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
        raise Failure(f"Failed to check data integrity: {e}")


@op(
    name="get_sync_date_range",
    description="ç¡®å®šåŒæ­¥æ—¥æœŸèŒƒå›´",
    retry_policy=STANDARD_RETRY
)
def get_sync_date_range_op(context: OpExecutionContext, integrity_check: Dict[str, Any], trading_dates: List[str]) -> Dict[str, str]:
    """æ ¹æ®æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ç»“æœç¡®å®šéœ€è¦åŒæ­¥çš„æ—¥æœŸèŒƒå›´"""
    logger = get_dagster_logger()

    try:
        if integrity_check["needs_full_sync"]:
            # å…¨é‡åŒæ­¥ï¼šä½¿ç”¨æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥
            if context.op_config:
                sync_days = context.op_config.get("sync_days", 30)
            else:
                sync_days = 30
            end_date = max(trading_dates)
            start_date = min(trading_dates[-sync_days:])  # æœ€è¿‘sync_daysä¸ªäº¤æ˜“æ—¥

            date_range = {
                "start_date": start_date,
                "end_date": end_date,
                "sync_type": "full_sync"
            }
        else:
            # å¢é‡åŒæ­¥ï¼šä»æœ€æ–°æ•°æ®æ—¥æœŸåˆ°æœ€æ–°äº¤æ˜“æ—¥
            if integrity_check["latest_date"]:
                start_date = integrity_check["latest_date"]
            else:
                # å¦‚æœæ²¡æœ‰æœ€æ–°æ—¥æœŸï¼Œä½¿ç”¨æœ€è¿‘7ä¸ªäº¤æ˜“æ—¥
                start_date = min(trading_dates[-7:])

            end_date = max(trading_dates)

            date_range = {
                "start_date": start_date,
                "end_date": end_date,
                "sync_type": "incremental_sync"
            }

        logger.info(f"ç¡®å®šåŒæ­¥æ—¥æœŸèŒƒå›´: {date_range['start_date']} åˆ° {date_range['end_date']} ({date_range['sync_type']})")
        return date_range

    except Exception as e:
        logger.error(f"ç¡®å®šåŒæ­¥æ—¥æœŸèŒƒå›´å¤±è´¥: {e}")
        raise Failure(f"Failed to get sync date range: {e}")


# ===== æ•°æ®åŒæ­¥æ“ä½œ =====

@op(
    name="sync_and_complete_ohlcv_data",
    description="åŒæ­¥å¹¶è¡¥å…¨OHLCVæ•°æ®",
    retry_policy=CRITICAL_RETRY,
    tags={"type": "sync", "data_type": "ohlcv"}
)
def sync_and_complete_ohlcv_data_op(context: OpExecutionContext, symbols: List[str], date_range: Dict[str, str]):
    """åŒæ­¥å¹¶è¡¥å…¨OHLCVæ•°æ® - ä¸€æ¬¡æ€§å®Œæˆå¢é‡æ›´æ–°å’Œç¼ºå¤±è¡¥å…¨"""
    logger = get_dagster_logger()

    try:
        loader = get_rqdatac_data_loader()

        logger.info(f"å¼€å§‹åŒæ­¥å¹¶è¡¥å…¨OHLCVæ•°æ®: {len(symbols)} åªè‚¡ç¥¨, {date_range['start_date']} åˆ° {date_range['end_date']}")
        logger.info(f"åŒæ­¥ç±»å‹: {date_range.get('sync_type', 'unknown')}")

        # ä»»åŠ¡1: å¢é‡æ›´æ–° - ä»æ•°æ®æºæ‹‰å–æœ€æ–°æ•°æ®
        logger.info("ğŸ“¥ æ‰§è¡Œå¢é‡æ›´æ–°ä»»åŠ¡...")
        raw_data = loader.get_ohlcv_data(
            symbols=symbols,
            start_date=date_range["start_date"],
            end_date=date_range["end_date"]
        )

        if raw_data.is_empty():
            raise ValueError("ä»RQDatacæ‹‰å–åˆ°çš„OHLCVæ•°æ®ä¸ºç©º")

        logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: æ‹‰å–åˆ° {len(raw_data)} æ¡è®°å½•")

        # ä»»åŠ¡2: ç¼ºå¤±è¡¥å…¨ - è¡¥å…¨æ•°æ®ä¸­çš„ç¼ºå¤±å€¼
        logger.info("ğŸ”§ æ‰§è¡Œç¼ºå¤±è¡¥å…¨ä»»åŠ¡...")
        completed_data = complete_market_data_inline(
            order_book_ids=symbols,
            existing_data=raw_data,
            max_fill_days=DEFAULT_COMPLETION_DAYS
        )

        logger.info(f"âœ… ç¼ºå¤±è¡¥å…¨å®Œæˆ: {len(completed_data)} æ¡è®°å½•")

        # è®°å½•èµ„äº§ç‰©åŒ–
        yield AssetMaterialization(
            asset_key="synced_completed_ohlcv_data",
            description="OHLCV data synced and completed with incremental update and missing value completion",
            metadata={
                "record_count": len(completed_data),
                "symbol_count": len(symbols),
                "date_range": f"{date_range['start_date']} to {date_range['end_date']}",
                "sync_type": date_range.get("sync_type", "unknown"),
                "raw_records": len(raw_data),
                "completed_records": len(completed_data),
                "sync_tasks": ["incremental_update", "missing_completion"]
            }
        )

        yield Output(completed_data)

    except Exception as e:
        logger.error(f"OHLCVæ•°æ®åŒæ­¥è¡¥å…¨å¤±è´¥: {e}")
        raise Failure(f"OHLCV sync and completion failed: {e}")


@op(
    name="sync_and_complete_fundamental_data",
    description="åŒæ­¥å¹¶è¡¥å…¨åŸºæœ¬é¢æ•°æ®",
    retry_policy=STANDARD_RETRY,
    tags={"type": "sync", "data_type": "fundamental"}
)
def sync_and_complete_fundamental_data_op(context: OpExecutionContext, symbols: List[str], date_range: Dict[str, str]):
    """åŒæ­¥å¹¶è¡¥å…¨åŸºæœ¬é¢æ•°æ® - ä¸€æ¬¡æ€§å®Œæˆå¢é‡æ›´æ–°å’Œç¼ºå¤±è¡¥å…¨"""
    logger = get_dagster_logger()

    try:
        loader = get_rqdatac_data_loader()

        logger.info(f"å¼€å§‹åŒæ­¥å¹¶è¡¥å…¨åŸºæœ¬é¢æ•°æ®: {len(symbols)} åªè‚¡ç¥¨, {date_range['start_date']} åˆ° {date_range['end_date']}")
        logger.info(f"åŒæ­¥ç±»å‹: {date_range.get('sync_type', 'unknown')}")

        # ä»»åŠ¡1: å¢é‡æ›´æ–° - ä»æ•°æ®æºæ‹‰å–æœ€æ–°æ•°æ®
        logger.info("ğŸ“¥ æ‰§è¡Œå¢é‡æ›´æ–°ä»»åŠ¡...")
        raw_data = loader.get_fundamental_data(
            symbols=symbols,
            start_date=date_range["start_date"],
            end_date=date_range["end_date"]
        )

        if raw_data.is_empty():
            logger.warning("ä»RQDatacæ‹‰å–åˆ°çš„åŸºæœ¬é¢æ•°æ®ä¸ºç©º")
            # åˆ›å»ºç©ºçš„DataFrameä½†ä¿æŒç»“æ„
            raw_data = pl.DataFrame({
                "order_book_id": [],
                "date": []
            })

        logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: æ‹‰å–åˆ° {len(raw_data)} æ¡è®°å½•")

        # ä»»åŠ¡2: ç¼ºå¤±è¡¥å…¨ - å¯¹äºåŸºæœ¬é¢æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨è·å–åˆ°çš„æ•°æ®
        logger.info("ğŸ”§ æ£€æŸ¥åŸºæœ¬é¢æ•°æ®å®Œæ•´æ€§...")
        completed_data = raw_data  # åŸºæœ¬é¢æ•°æ®æš‚ä¸è¿›è¡Œå¤æ‚çš„è¡¥å…¨é€»è¾‘

        logger.info(f"âœ… æ•°æ®æ£€æŸ¥å®Œæˆ: {len(completed_data)} æ¡è®°å½•")

        # è®°å½•èµ„äº§ç‰©åŒ–
        yield AssetMaterialization(
            asset_key="synced_completed_fundamental_data",
            description="Fundamental data synced and completed with incremental update and data validation",
            metadata={
                "record_count": len(completed_data),
                "symbol_count": len(symbols),
                "date_range": f"{date_range['start_date']} to {date_range['end_date']}",
                "sync_type": date_range.get("sync_type", "unknown"),
                "raw_records": len(raw_data),
                "completed_records": len(completed_data),
                "sync_tasks": ["incremental_update", "data_validation"]
            }
        )

        yield Output(completed_data)

    except Exception as e:
        logger.error(f"åŸºæœ¬é¢æ•°æ®åŒæ­¥è¡¥å…¨å¤±è´¥: {e}")
        raise Failure(f"Fundamental sync and completion failed: {e}")


@op(
    name="sync_and_complete_factor_data",
    description="åŒæ­¥å¹¶è¡¥å…¨å› å­æ•°æ®",
    retry_policy=CRITICAL_RETRY,
    tags={"type": "sync", "data_type": "factor"}
)
def sync_and_complete_factor_data_op(context: OpExecutionContext, symbols: List[str], date_range: Dict[str, str]):
    """åŒæ­¥å¹¶è¡¥å…¨å› å­æ•°æ® - ä»RQDatacå› å­åº“è·å–"""
    logger = get_dagster_logger()

    try:
        loader = get_rqdatac_data_loader()

        logger.info(f"å¼€å§‹åŒæ­¥å¹¶è¡¥å…¨å› å­æ•°æ®: {len(symbols)} åªè‚¡ç¥¨")
        logger.info(f"åŒæ­¥ç±»å‹: {date_range.get('sync_type', 'unknown')}")

        # ä»»åŠ¡1: ä»å› å­åº“è·å–å› å­æ•°æ®
        logger.info("ğŸ“¥ ä»RQDatacå› å­åº“è·å–å› å­æ•°æ®...")
        factor_data = loader.get_factor_data(
            symbols=symbols,
            start_date=date_range["start_date"],
            end_date=date_range["end_date"]
        )

        if factor_data.is_empty():
            logger.warning("ä»RQDatacå› å­åº“è·å–åˆ°çš„å› å­æ•°æ®ä¸ºç©º")
            # åˆ›å»ºç©ºçš„DataFrameä½†ä¿æŒç»“æ„
            factor_data = pl.DataFrame({
                "order_book_id": [],
                "date": []
            })

        logger.info(f"âœ… ä»å› å­åº“è·å–åˆ° {len(factor_data)} æ¡å› å­è®°å½•")

        # ä»»åŠ¡2: ç¼ºå¤±è¡¥å…¨ - å¯¹äºè·å–å¤±è´¥çš„å› å­ï¼Œç”±æŒ‡æ ‡è®¡ç®—æ¨¡å—è‡ªè¡Œè®¡ç®—
        logger.info("ğŸ”§ æ£€æŸ¥å› å­æ•°æ®å®Œæ•´æ€§...")
        available_factors = [col for col in factor_data.columns if col not in ["order_book_id", "date"]]
        logger.info(f"æˆåŠŸè·å–çš„å› å­: {available_factors}")

        # è®°å½•èµ„äº§ç‰©åŒ–
        yield AssetMaterialization(
            asset_key="synced_completed_factor_data",
            description="Factor data synced from RQDatac factor library",
            metadata={
                "record_count": len(factor_data),
                "symbol_count": len(symbols),
                "date_range": f"{date_range['start_date']} to {date_range['end_date']}",
                "sync_type": date_range.get("sync_type", "unknown"),
                "available_factors": available_factors,
                "factor_count": len(available_factors),
                "sync_tasks": ["factor_library_sync"]
            }
        )

        yield Output(factor_data)

    except Exception as e:
        logger.error(f"å› å­æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        raise Failure(f"Factor sync failed: {e}")


# ===== æ•°æ®è¡¥å…¨æ“ä½œ =====

# ===== æŒ‡æ ‡è®¡ç®—æ“ä½œ =====

@op(
    name="calculate_and_save_technical_indicators",
    description="è®¡ç®—å¹¶ä¿å­˜æŠ€æœ¯æŒ‡æ ‡",
    retry_policy=STANDARD_RETRY,
    tags={"type": "calculation", "indicator_type": "technical", "operation": "calculate_and_save"}
)
def calculate_and_save_technical_indicators_op(context: OpExecutionContext, ohlcv_data: pl.DataFrame):
    """è®¡ç®—å¹¶ä¿å­˜æŠ€æœ¯æŒ‡æ ‡"""
    logger = get_dagster_logger()

    try:
        logger.info("å¼€å§‹è®¡ç®—æŠ€æœ¯æŒ‡æ ‡")

        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        if context.op_config:
            indicators_config = context.op_config.get("indicators", ["sma_5", "sma_10", "sma_20", "sma_60", "rsi_14", "macd", "price_angles", "volatility", "volume_indicators", "stoch", "bollinger", "risk_indicators"])
        else:
            indicators_config = ["sma_5", "sma_10", "sma_20", "sma_60", "rsi_14", "macd", "price_angles", "volatility", "volume_indicators", "stoch", "bollinger", "risk_indicators"]

        indicators_data = calculate_indicators(
            data=ohlcv_data,
            indicators=indicators_config
        )

        # ç¡®ä¿è¿”å›çš„æ˜¯DataFrame
        if isinstance(indicators_data, dict):
            # å¦‚æœè¿”å›å­—å…¸ï¼Œåˆå¹¶æ‰€æœ‰DataFrame
            if indicators_data:
                indicators_data = pl.concat(list(indicators_data.values()))
            else:
                indicators_data = pl.DataFrame()

        logger.info(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ: {len(indicators_data)} æ¡è®°å½•")

        # ä¿å­˜æŠ€æœ¯æŒ‡æ ‡æ•°æ®
        logger.info("å¼€å§‹ä¿å­˜æŠ€æœ¯æŒ‡æ ‡æ•°æ®")
        from datetime import datetime
        output_path = f"data/technical_indicators_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
        save_data(
            data=indicators_data,
            output_path=output_path
        )
        logger.info(f"æŠ€æœ¯æŒ‡æ ‡æ•°æ®ä¿å­˜å®Œæˆ: {output_path}")

        # è®°å½•èµ„äº§ç‰©åŒ–
        yield AssetMaterialization(
            asset_key="technical_indicators_calculated_and_saved",
            description="Technical indicators calculated and saved",
            metadata={
                "record_count": len(indicators_data),
                "indicators": indicators_config,
                "saved": True,
                "data_type": "technical_indicators"
            }
        )

        yield Output(indicators_data)

    except Exception as e:
        logger.error(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å’Œä¿å­˜å¤±è´¥: {e}")
        raise Failure(f"Technical indicators calculation and save failed: {e}")


def _load_latest_indicator_file(file_pattern: str, indicator_type: str) -> pl.DataFrame:
    """é€šç”¨å‡½æ•°ï¼šåŠ è½½æœ€æ–°çš„æŒ‡æ ‡æ•°æ®æ–‡ä»¶"""
    logger = get_dagster_logger()

    try:
        logger.info(f"å¼€å§‹åŠ è½½å·²æœ‰çš„{indicator_type}æ•°æ®")

        # æŸ¥æ‰¾æœ€æ–°çš„æŒ‡æ ‡æ–‡ä»¶
        data_dir = Path("data")
        latest_file = None
        latest_date = None

        for file_path in data_dir.glob(file_pattern):
            if file_path.is_file():
                date_str = file_path.stem.replace(file_pattern.replace("*", ""), "")
                try:
                    file_date = datetime.strptime(date_str, "%Y%m%d_%H%M%S")
                    if latest_date is None or file_date > latest_date:
                        latest_date = file_date
                        latest_file = file_path
                except ValueError:
                    continue

        if latest_file is None:
            raise ValueError(f"æœªæ‰¾åˆ°å·²æœ‰çš„{indicator_type}æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡ŒæŒ‡æ ‡è®¡ç®—ä½œä¸š")

        # åŠ è½½æŒ‡æ ‡æ•°æ®
        indicators_data = pl.read_parquet(str(latest_file))
        logger.info(f"{indicator_type}æ•°æ®åŠ è½½å®Œæˆ: {len(indicators_data)} æ¡è®°å½•")

        return indicators_data

    except Exception as e:
        logger.error(f"åŠ è½½{indicator_type}æ•°æ®å¤±è´¥: {e}")
        raise Failure(f"Failed to load existing {indicator_type.lower()}: {e}")


@op(
    name="load_existing_technical_indicators",
    description="åŠ è½½å·²æœ‰çš„æŠ€æœ¯æŒ‡æ ‡æ•°æ®",
    retry_policy=STANDARD_RETRY,
    tags={"type": "load", "data_type": "technical_indicators"}
)
def load_existing_technical_indicators_op(context: OpExecutionContext):
    """åŠ è½½æœ€è¿‘çš„æŠ€æœ¯æŒ‡æ ‡æ•°æ®æ–‡ä»¶"""
    indicators_data = _load_latest_indicator_file("technical_indicators_*.parquet", "æŠ€æœ¯æŒ‡æ ‡")
    yield Output(indicators_data)


@op(
    name="merge_all_data",
    description="åˆå¹¶æ‰€æœ‰æ•°æ®",
    retry_policy=STANDARD_RETRY,
    tags={"type": "merge"}
)
def merge_all_data_op(context: OpExecutionContext,
                     ohlcv_data: pl.DataFrame,
                     fundamental_data: pl.DataFrame,
                     technical_indicators: pl.DataFrame,
                     fundamental_data_for_merge: pl.DataFrame):
    """åˆå¹¶æ‰€æœ‰å¤„ç†åçš„æ•°æ®"""
    logger = get_dagster_logger()

    try:
        logger.info("å¼€å§‹åˆå¹¶æ‰€æœ‰æ•°æ®")

        # ä¿®å¤å­—æ®µå†²çªé—®é¢˜ï¼šé‡å‘½åæŠ€æœ¯æŒ‡æ ‡æ•°æ®ä¸­çš„OHLCVå­—æ®µä»¥é¿å…è¦†ç›–åŸå§‹æ•°æ®
        conflict_fields = ['open', 'high', 'low', 'close', 'volume', 'amount', 'vwap', 'returns']
        rename_mapping = {field: f"{field}_technical" for field in conflict_fields if field in technical_indicators.columns}

        if rename_mapping:
            logger.info(f"æ£€æµ‹åˆ°å­—æ®µå†²çªï¼Œé‡å‘½åæŠ€æœ¯æŒ‡æ ‡å­—æ®µ: {list(rename_mapping.keys())}")
            technical_indicators = technical_indicators.rename(rename_mapping)

        # åˆå¹¶æ•°æ®ï¼ˆåŸºäºorder_book_idå’Œdateï¼‰
        merged_data = ohlcv_data.join(
            fundamental_data,
            on=["order_book_id", "date"],
            how="left"
        ).join(
            technical_indicators,
            on=["order_book_id", "date"],
            how="left"
        ).join(
            fundamental_data_for_merge,
            on=["order_book_id", "date"],
            how="left"
        )

        logger.info(f"æ•°æ®åˆå¹¶å®Œæˆ: {len(merged_data)} æ¡è®°å½•")

        yield Output(merged_data)

    except Exception as e:
        logger.error(f"æ•°æ®åˆå¹¶å¤±è´¥: {e}")
        raise Failure(f"Data merge failed: {e}")


@op(
    name="calculate_final_scores",
    description="è®¡ç®—æœ€ç»ˆè¯„åˆ†",
    retry_policy=STANDARD_RETRY,
    tags={"type": "scoring"}
)
def calculate_final_scores_op(context: OpExecutionContext, merged_data: pl.DataFrame):
    """è®¡ç®—æœ€ç»ˆçš„è‚¡ç¥¨è¯„åˆ†"""
    logger = get_dagster_logger()

    try:
        logger.info("å¼€å§‹è®¡ç®—æœ€ç»ˆè¯„åˆ†")

        # è®¡ç®—æŠ€æœ¯è¯„åˆ†
        scored_data = calculate_scores(
            data=merged_data,
            score_type="technical"
        )

        logger.info(f"è¯„åˆ†è®¡ç®—å®Œæˆ: {len(scored_data)} åªè‚¡ç¥¨")

        # è®°å½•èµ„äº§ç‰©åŒ–
        yield AssetMaterialization(
            asset_key="final_scores",
            description="Final stock scores calculated",
            metadata={"stock_count": len(scored_data)}
        )

        yield Output(scored_data)

    except Exception as e:
        logger.error(f"è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
        raise Failure(f"Score calculation failed: {e}")


@op(
    name="save_final_scores",
    description="ä¿å­˜æœ€ç»ˆè¯„åˆ†æ•°æ®",
    retry_policy=STANDARD_RETRY,
    tags={"type": "save", "data_type": "scores"}
)
def save_final_scores_op(context: OpExecutionContext, scored_data: pl.DataFrame):
    """ä¿å­˜æœ€ç»ˆè¯„åˆ†æ•°æ®åˆ°å•ç‹¬çš„æ–‡ä»¶"""
    logger = get_dagster_logger()

    try:
        # ç”Ÿæˆè¯„åˆ†æ•°æ®è¾“å‡ºè·¯å¾„ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        scores_output_path = Path.cwd() / "data" / "scores" / f"final_scores_{timestamp}.parquet"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        scores_output_path.parent.mkdir(parents=True, exist_ok=True)

        logger.info(f"å¼€å§‹ä¿å­˜è¯„åˆ†æ•°æ®åˆ°: {scores_output_path}")

        # ä¿å­˜è¯„åˆ†æ•°æ®
        try:
            scored_data.write_parquet(scores_output_path)
            logger.info(f"è¯„åˆ†æ•°æ®ä¿å­˜å®Œæˆ: {scores_output_path}")
        except Exception as save_error:
            logger.error(f"ä¿å­˜è¯„åˆ†æ•°æ®å¤±è´¥: {save_error}")
            raise Exception(f"Failed to save scores data: {save_error}")

        # è®°å½•èµ„äº§ç‰©åŒ–
        yield AssetMaterialization(
            asset_key="saved_final_scores",
            description="Final scores data saved to storage",
            metadata={
                "output_path": str(scores_output_path),
                "record_count": len(scored_data),
                "file_size": scores_output_path.stat().st_size if scores_output_path.exists() else 0,
                "data_type": "final_scores"
            }
        )

        yield Output(str(scores_output_path))

    except Exception as e:
        logger.error(f"è¯„åˆ†æ•°æ®ä¿å­˜å¤±è´¥: {e}")
        raise Failure(f"Save scores data failed: {e}")


@op(
    name="save_processed_data",
    description="ä¿å­˜å¤„ç†åçš„æ•°æ®",
    retry_policy=CRITICAL_RETRY,
    tags={"type": "save"}
)
def save_processed_data_op(context: OpExecutionContext, final_data: pl.DataFrame):
    """ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°å­˜å‚¨"""
    logger = get_dagster_logger()

    try:
        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"data/processed/daily_results_{timestamp}.parquet"

        logger.info(f"å¼€å§‹ä¿å­˜æ•°æ®åˆ°: {output_path}")

        # ä¿å­˜åˆ°Parquet
        success = save_data(
            data=final_data,
            output_path=output_path,
            compression="snappy"
        )

        if not success:
            raise Exception("Failed to save data")

        logger.info("æ•°æ®ä¿å­˜å®Œæˆ")

        # è®°å½•èµ„äº§ç‰©åŒ–
        yield AssetMaterialization(
            asset_key="processed_data",
            description="Processed data saved to storage",
            metadata={
                "output_path": output_path,
                "record_count": len(final_data),
                "file_size": Path(output_path).stat().st_size if Path(output_path).exists() else 0
            }
        )

        yield Output(output_path)

    except Exception as e:
        logger.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {e}")
        raise Failure(f"Data save failed: {e}")


# ===== ä½œä¸šå®šä¹‰ =====

@job(
    name="daily_data_sync_job",
    description="æ¯æ—¥æ•°æ®åŒæ­¥ä½œä¸š",
    tags={"type": "sync", "frequency": "daily", "priority": "high"}
)
def daily_data_sync_job():
    """æ¯æ—¥æ•°æ®åŒæ­¥ä½œä¸š - æ™ºèƒ½å¢é‡æ›´æ–°å’Œç¼ºå¤±è¡¥å…¨"""
    # 1. è·å–ç›®æ ‡è‚¡ç¥¨åˆ—è¡¨
    symbols = get_target_stocks_op()

    # 2. è·å–äº¤æ˜“æ—¥å†
    trading_dates = get_trading_calendar_op()

    # 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    integrity_check = check_data_integrity_op(symbols, trading_dates)

    # 4. ç¡®å®šåŒæ­¥æ—¥æœŸèŒƒå›´
    date_range = get_sync_date_range_op(integrity_check, trading_dates)

    # 5. åŒæ­¥å¹¶è¡¥å…¨ä¸åŒç±»å‹æ•°æ®
    ohlcv_data = sync_and_complete_ohlcv_data_op(symbols, date_range)
    fundamental_data = sync_and_complete_fundamental_data_op(symbols, date_range)

    # è¿”å›åŒæ­¥è¡¥å…¨åçš„æ•°æ®
    return ohlcv_data, fundamental_data


@job(
    name="data_completion_job",
    description="æ•°æ®è¡¥å…¨ä½œä¸š",
    tags={"type": "completion", "frequency": "daily", "priority": "high"}
)
def data_completion_job():
    """æ•°æ®è¡¥å…¨ä½œä¸š - ç‹¬ç«‹çš„è¡¥å…¨æ“ä½œï¼ˆå¦‚æœéœ€è¦ï¼‰"""
    # 1. è·å–ç›®æ ‡è‚¡ç¥¨åˆ—è¡¨
    symbols = get_target_stocks_op()

    # 2. è·å–äº¤æ˜“æ—¥å†
    trading_dates = get_trading_calendar_op()

    # 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    integrity_check = check_data_integrity_op(symbols, trading_dates)

    # 4. ç¡®å®šåŒæ­¥æ—¥æœŸèŒƒå›´
    date_range = get_sync_date_range_op(integrity_check, trading_dates)

    # 5. åŒæ­¥å¹¶è¡¥å…¨æ•°æ®
    ohlcv_data = sync_and_complete_ohlcv_data_op(symbols, date_range)
    fundamental_data = sync_and_complete_fundamental_data_op(symbols, date_range)

    return ohlcv_data, fundamental_data


# ç§»é™¤äº†é‡å¤çš„ scoring_and_save_jobï¼Œç°åœ¨ä½¿ç”¨ç»Ÿä¸€çš„ daily_full_pipeline_job


@job(
    name="daily_full_pipeline_job",
    description="å®Œæ•´çš„æ¯æ—¥æ•°æ®å¤„ç†ç®¡é“ - æ•°æ®åŒæ­¥ã€æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€è¯„åˆ†å­˜ç›˜ä¸€ä½“åŒ–",
    tags={"type": "pipeline", "frequency": "daily", "priority": "critical"}
)
def daily_full_pipeline_job():
    """å®Œæ•´çš„æ¯æ—¥æ•°æ®å¤„ç†ç®¡é“ä½œä¸š - ä¸€ç«™å¼å®Œæˆæ‰€æœ‰å¤„ç†æ­¥éª¤
    æµç¨‹ï¼š
    1. æ•°æ®åŒæ­¥ï¼ˆå¢é‡æ›´æ–° + ç¼ºå¤±è¡¥å…¨ï¼‰å¹¶ä¿å­˜
    2. æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¹¶ä¿å­˜ï¼ˆåŸºæœ¬é¢æ•°æ®ç›´æ¥ä½¿ç”¨æ‹‰å–çš„æ•°æ®ï¼‰
    3. è¯„åˆ†è®¡ç®—
    4. è¯„åˆ†å•ç‹¬ä¿å­˜
    5. å®Œæ•´æ•°æ®å­˜ç›˜
    """
    # 1. è·å–åŸºç¡€ä¿¡æ¯
    symbols = get_target_stocks_op()
    trading_dates = get_trading_calendar_op()
    integrity_check = check_data_integrity_op(symbols, trading_dates)
    date_range = get_sync_date_range_op(integrity_check, trading_dates)

    # 2. æ•°æ®åŒæ­¥è¡¥å…¨
    ohlcv_data = sync_and_complete_ohlcv_data_op(symbols, date_range)
    fundamental_data = sync_and_complete_fundamental_data_op(symbols, date_range)

    # 3. æŒ‡æ ‡è®¡ç®—å¹¶ä¿å­˜
    technical_indicators = calculate_and_save_technical_indicators_op(ohlcv_data)
    # åŸºæœ¬é¢æ•°æ®ç›´æ¥ä½¿ç”¨æ‹‰å–çš„æ•°æ®ï¼Œä¸è¿›è¡Œé¢å¤–è®¡ç®—

    # 4. æ•°æ®åˆå¹¶
    merged_data = merge_all_data_op(
        ohlcv_data, fundamental_data,
        technical_indicators, fundamental_data  # ä½¿ç”¨åŸå§‹åŸºæœ¬é¢æ•°æ®ä½œä¸º"æŒ‡æ ‡"
    )

    # 5. è¯„åˆ†è®¡ç®—
    final_scores = calculate_final_scores_op(merged_data)

    # 6. è¯„åˆ†å•ç‹¬ä¿å­˜
    scores_output_path = save_final_scores_op(final_scores)

    # 7. å®Œæ•´æ•°æ®å­˜ç›˜
    output_path = save_processed_data_op(final_scores)


# ===== ä¼ æ„Ÿå™¨å®šä¹‰ =====

@run_status_sensor(
    monitored_jobs=[daily_full_pipeline_job],
    run_status=DagsterRunStatus.SUCCESS,
    name="daily_processing_success_monitor"
)
def daily_processing_success_monitor(context):
    """ç›‘å¬å®Œæ•´æ¯æ—¥å¤„ç†ç®¡é“ä½œä¸šçš„æ‰§è¡ŒçŠ¶æ€"""
    logger.info("å®Œæ•´çš„æ¯æ—¥æ•°æ®å¤„ç†ç®¡é“ä½œä¸šæ‰§è¡ŒæˆåŠŸ")
    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ åç»­å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚å‘é€é€šçŸ¥æˆ–è§¦å‘ä¸‹æ¸¸ä½œä¸š
    return None


@run_status_sensor(
    monitored_jobs=[daily_full_pipeline_job],
    run_status=DagsterRunStatus.FAILURE,
    name="daily_processing_failure_monitor"
)
def daily_processing_failure_monitor(context):
    """ç›‘å¬å®Œæ•´æ¯æ—¥å¤„ç†ç®¡é“ä½œä¸šçš„å¤±è´¥çŠ¶æ€"""
    logger.error("å®Œæ•´çš„æ¯æ—¥æ•°æ®å¤„ç†ç®¡é“ä½œä¸šæ‰§è¡Œå¤±è´¥")
    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é”™è¯¯å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚å‘é€å‘Šè­¦é€šçŸ¥
    return None


# ===== è°ƒåº¦å™¨å®šä¹‰ =====

@schedule(
    job=daily_full_pipeline_job,
    cron_schedule="0 9 * * 1-5",  # æ¯å‘¨ä¸€åˆ°å‘¨äº”ä¸Šåˆ9ç‚¹æ‰§è¡Œï¼ˆå¼€ç›˜å‰ï¼‰
    name="daily_full_pipeline_schedule"
)
def daily_full_pipeline_schedule(context):
    """æ¯æ—¥å®Œæ•´æ•°æ®å¤„ç†ç®¡é“è°ƒåº¦å™¨ - å·¥ä½œæ—¥è‡ªåŠ¨æ‰§è¡Œ"""
    logger.info("è§¦å‘æ¯æ—¥å®Œæ•´æ•°æ®å¤„ç†ç®¡é“ä½œä¸š")

    # è¿”å›è¿è¡Œæ—¶é…ç½®
    run_config = {
        "ops": {
            "get_target_stocks": {
                "config": {
                    "target_stocks": []  # ç©ºåˆ—è¡¨è¡¨ç¤ºè‡ªåŠ¨è·å–æ‰€æœ‰æ´»è·ƒè‚¡ç¥¨
                }
            },
            "get_trading_calendar": {
                "config": {
                    "calendar_days": 30  # è·å–æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥
                }
            },
            "get_sync_date_range": {
                "config": {
                    "sync_days": 30  # å…¨é‡åŒæ­¥æ—¶çš„æ•°æ®å¤©æ•°
                }
            },
            "calculate_and_save_technical_indicators": {
                "config": {
                    "indicators": [
                        "sma_5", "sma_10", "sma_20", "sma_60",
                        "rsi_14", "macd", "price_angles",
                        "volatility", "volume_indicators",
                        "stoch", "bollinger", "risk_indicators"
                    ]
                }
            }
        }
    }

    return run_config


# ===== æ³¨å†Œåˆ°ç®¡é“ç®¡ç†å™¨ =====

def register_daily_jobs():
    """æ³¨å†Œæ‰€æœ‰æ¯æ—¥ä½œä¸šåˆ°ç®¡é“ç®¡ç†å™¨"""
    # æ³¨å†Œä¸»è¦çš„å®Œæ•´ç®¡é“ä½œä¸š
    pipeline_manager.register_job("daily_full_pipeline", daily_full_pipeline_job)

    # æ³¨å†Œç‹¬ç«‹çš„ç»„ä»¶ä½œä¸šï¼ˆç”¨äºè°ƒè¯•æˆ–å•ç‹¬æ‰§è¡Œï¼‰
    pipeline_manager.register_job("daily_sync", daily_data_sync_job)

    logger.info("æ‰€æœ‰æ¯æ—¥ä½œä¸šå·²æ³¨å†Œåˆ°ç®¡é“ç®¡ç†å™¨")


# åˆå§‹åŒ–æ—¶æ³¨å†Œä½œä¸š
register_daily_jobs()


# ===== ä½¿ç”¨ç¤ºä¾‹ =====

if __name__ == "__main__":
    print("=== æ¯æ—¥æ•°æ®å¤„ç†ç®¡é“æ¼”ç¤º ===")

    # è¿è¡Œå®Œæ•´ç®¡é“
    try:
        print("âœ… ç®¡é“å®šä¹‰å®Œæˆï¼Œå¯ä»¥é€šè¿‡Dagster UIæˆ–APIæ‰§è¡Œ")
        print("ğŸ“‹ å¯ç”¨ä½œä¸š:")
        print("  - daily_full_pipeline_job: â­ æ¨è - å®Œæ•´æ¯æ—¥å¤„ç†ç®¡é“")
        print("    1. æ•°æ®åŒæ­¥è¡¥å…¨ (å¢é‡æ›´æ–°+ç¼ºå¤±è¡¥å…¨)")
        print("    2. æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¹¶ä¿å­˜ (åŸºæœ¬é¢æ•°æ®ç›´æ¥ä½¿ç”¨æ‹‰å–çš„æ•°æ®)")
        print("    3. è¯„åˆ†å­˜ç›˜ (æœ€ç»ˆè¯„åˆ†è®¡ç®—å’Œæ•°æ®ä¿å­˜)")
        print("")
        print("  - ç‹¬ç«‹ç»„ä»¶ä½œä¸š (ç”¨äºè°ƒè¯•):")
        print("    - daily_data_sync_job: æ•°æ®åŒæ­¥è¡¥å…¨")
        print("    - indicator_calculation_job: æŒ‡æ ‡è®¡ç®—å¹¶ä¿å­˜")
        print("    - load_existing_technical_indicators_op: åŠ è½½å·²æœ‰æŠ€æœ¯æŒ‡æ ‡")

    except Exception as e:
        print(f"âŒ ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
