"""
Dagsteré…ç½®å’Œè¿è¡Œæ—¶è®¾ç½® (Dagster Configuration and Runtime Settings)

é…ç½®Dagsterä½œä¸šçš„è¿è¡Œå‚æ•°ã€èµ„æºã€è°ƒåº¦å’Œç›‘æ§è®¾ç½®
"""

from typing import Dict, Any, List, Optional
from pathlib import Path
import os
from dagster import (
    ConfigurableResource, RunConfig, resource, InitResourceContext,
    make_values_resource, define_asset_job, AssetSelection
)
from dagster import get_dagster_logger
import polars as pl

# å¯¼å…¥ç½‘ç»œé…ç½®
from networks.rqdatac_config import RQDatacConfig

# ===== èµ„æºå®šä¹‰ =====

@resource(
    config_schema={
        "data_dir": str,
        "cache_dir": str,
        "log_dir": str,
        "max_workers": int,
        "timeout": int
    }
)
def file_system_resource(context: InitResourceContext) -> Dict[str, Any]:
    """æ–‡ä»¶ç³»ç»Ÿèµ„æºé…ç½®"""
    config = context.resource_config

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    for dir_path in [config["data_dir"], config["cache_dir"], config["log_dir"]]:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

    return {
        "data_dir": Path(config["data_dir"]),
        "cache_dir": Path(config["cache_dir"]),
        "log_dir": Path(config["log_dir"]),
        "max_workers": config["max_workers"],
        "timeout": config["timeout"]
    }


@resource(
    config_schema={
        "host": str,
        "port": int,
        "database": str,
        "username": str,
        "password": str,
        "pool_size": int
    }
)
def database_resource(context: InitResourceContext) -> Dict[str, Any]:
    """æ•°æ®åº“èµ„æºé…ç½®"""
    config = context.resource_config

    return {
        "connection_string": f"postgresql://{config['username']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}",
        "pool_size": config["pool_size"]
    }


@resource(
    config_schema={
        "api_key": str,
        "base_url": str,
        "timeout": int,
        "retry_attempts": int
    }
)
def rqdatac_resource(context: InitResourceContext) -> Dict[str, Any]:
    """RQDatacèµ„æºé…ç½®"""
    config = context.resource_config

    return {
        "api_key": config["api_key"],
        "base_url": config["base_url"],
        "timeout": config["timeout"],
        "retry_attempts": config["retry_attempts"]
    }


# ===== é…ç½®ç±»å®šä¹‰ =====

class ProcessingConfig:
    """å¤„ç†é…ç½®ç±»"""

    def __init__(self):
        self.data_dir = "data"
        self.cache_dir = "cache"
        self.log_dir = "logs"
        self.max_workers = 4
        self.timeout = 300
        self.target_stocks = []  # ä¸ºç©ºæ—¶è‡ªåŠ¨è·å–å…¨é‡æ²ªæ·±è‚¡ç¥¨ï¼ˆæ’é™¤ST/PTï¼‰
        self.completion_days = 30
        self.indicators = ["sma_20", "rsi_14", "macd", "pe_ratio", "pb_ratio", "roe"]
        self.compression = "snappy"
        self.batch_size = 1000

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "data_dir": self.data_dir,
            "cache_dir": self.cache_dir,
            "log_dir": self.log_dir,
            "max_workers": self.max_workers,
            "timeout": self.timeout,
            "target_stocks": self.target_stocks,
            "completion_days": self.completion_days,
            "indicators": self.indicators,
            "compression": self.compression,
            "batch_size": self.batch_size
        }


class DatabaseConfig:
    """æ•°æ®åº“é…ç½®ç±»"""

    def __init__(self):
        self.host = os.getenv("DB_HOST", "localhost")
        self.port = int(os.getenv("DB_PORT", "5432"))
        self.database = os.getenv("DB_NAME", "stockmonitor")
        self.username = os.getenv("DB_USER", "postgres")
        self.password = os.getenv("DB_PASSWORD", "")
        self.pool_size = 10

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "host": self.host,
            "port": self.port,
            "database": self.database,
            "username": self.username,
            "password": self.password,
            "pool_size": self.pool_size
        }


# ===== è¿è¡Œé…ç½®ç”Ÿæˆå™¨ =====

def create_run_config(job_type: str, custom_config: Optional[Dict[str, Any]] = None) -> RunConfig:
    """åˆ›å»ºä½œä¸šè¿è¡Œé…ç½®"""
    base_config = ProcessingConfig().to_dict()

    if custom_config:
        base_config.update(custom_config)

    # æ ¹æ®ä½œä¸šç±»å‹è®¾ç½®ç‰¹å®šé…ç½®
    if job_type == "sync":
        config = {
            "ops": {
                "get_target_stocks": {"config": {"target_stocks": base_config["target_stocks"]}},
                "get_trading_dates": {"config": {"days_back": base_config["completion_days"]}}
            },
            "resources": {
                "file_system": {"config": {
                    "data_dir": base_config["data_dir"],
                    "cache_dir": base_config["cache_dir"],
                    "log_dir": base_config["log_dir"],
                    "max_workers": base_config["max_workers"],
                    "timeout": base_config["timeout"]
                }},
                "rqdatac": {"config": RQDatacConfig().to_dict()}
            }
        }
    elif job_type == "completion":
        config = {
            "ops": {
                "get_target_stocks": {"config": {"target_stocks": base_config["target_stocks"]}},
                "get_trading_dates": {"config": {"days_back": base_config["completion_days"]}}
            },
            "resources": {
                "file_system": {"config": {
                    "data_dir": base_config["data_dir"],
                    "cache_dir": base_config["cache_dir"],
                    "log_dir": base_config["log_dir"],
                    "max_workers": base_config["max_workers"],
                    "timeout": base_config["timeout"]
                }},
                "rqdatac": {"config": RQDatacConfig().to_dict()}
            }
        }
    elif job_type == "calculation":
        config = {
            "ops": {
                "get_target_stocks": {"config": {"target_stocks": base_config["target_stocks"]}},
                "get_trading_dates": {"config": {"days_back": base_config["completion_days"]}},
                "calculate_technical_indicators": {"config": {"indicators": base_config["indicators"]}}
            },
            "resources": {
                "file_system": {"config": {
                    "data_dir": base_config["data_dir"],
                    "cache_dir": base_config["cache_dir"],
                    "log_dir": base_config["log_dir"],
                    "max_workers": base_config["max_workers"],
                    "timeout": base_config["timeout"]
                }},
                "rqdatac": {"config": RQDatacConfig().to_dict()}
            }
        }
    elif job_type == "scoring":
        config = {
            "ops": {
                "get_target_stocks": {"config": {"target_stocks": base_config["target_stocks"]}},
                "get_trading_dates": {"config": {"days_back": base_config["completion_days"]}}
            },
            "resources": {
                "file_system": {"config": {
                    "data_dir": base_config["data_dir"],
                    "cache_dir": base_config["cache_dir"],
                    "log_dir": base_config["log_dir"],
                    "max_workers": base_config["max_workers"],
                    "timeout": base_config["timeout"]
                }},
                "rqdatac": {"config": RQDatacConfig().to_dict()},
                "database": {"config": DatabaseConfig().to_dict()}
            }
        }
    else:  # full_pipeline
        config = {
            "ops": {
                "get_target_stocks": {"config": {"target_stocks": base_config["target_stocks"]}},
                "get_trading_dates": {"config": {"days_back": base_config["completion_days"]}},
                "calculate_technical_indicators": {"config": {"indicators": base_config["indicators"]}}
            },
            "resources": {
                "file_system": {"config": {
                    "data_dir": base_config["data_dir"],
                    "cache_dir": base_config["cache_dir"],
                    "log_dir": base_config["log_dir"],
                    "max_workers": base_config["max_workers"],
                    "timeout": base_config["timeout"]
                }},
                "rqdatac": {"config": RQDatacConfig().to_dict()},
                "database": {"config": DatabaseConfig().to_dict()}
            }
        }

    return RunConfig(**config)


# ===== ç¯å¢ƒé…ç½® =====

def get_environment_config(env: str = "development") -> Dict[str, Any]:
    """è·å–ç¯å¢ƒé…ç½®"""
    if env == "production":
        return {
            "processing": ProcessingConfig(),
            "database": DatabaseConfig(),
            "rqdatac": RQDatacConfig(),
            "concurrency": {"max_concurrent_runs": 5},
            "logging": {"level": "INFO", "format": "json"}
        }
    elif env == "staging":
        return {
            "processing": ProcessingConfig(),
            "database": DatabaseConfig(),
            "rqdatac": RQDatacConfig(),
            "concurrency": {"max_concurrent_runs": 3},
            "logging": {"level": "DEBUG", "format": "text"}
        }
    else:  # development
        return {
            "processing": ProcessingConfig(),
            "database": DatabaseConfig(),
            "rqdatac": RQDatacConfig(),
            "concurrency": {"max_concurrent_runs": 2},
            "logging": {"level": "DEBUG", "format": "text"}
        }


# ===== ä½œä¸šå®šä¹‰é›†åˆ =====

def get_all_job_definitions():
    """è·å–æ‰€æœ‰ä½œä¸šå®šä¹‰"""
    from .pipeline_definitions import (
        stock_analysis_job,
        daily_data_sync_job,
        data_completion_job,
        daily_full_pipeline_job
    )

    return [
        stock_analysis_job,
        daily_data_sync_job,
        data_completion_job,
        daily_full_pipeline_job
    ]


def get_all_schedule_definitions():
    """è·å–æ‰€æœ‰è°ƒåº¦å®šä¹‰"""
    from .pipeline_definitions import (
        daily_full_pipeline_schedule
    )

    return [
        daily_full_pipeline_schedule
    ]


def get_all_sensor_definitions():
    """è·å–æ‰€æœ‰ä¼ æ„Ÿå™¨å®šä¹‰"""
    from .pipeline_definitions import (
        daily_processing_success_monitor,
        daily_processing_failure_monitor
    )

    return [
        daily_processing_success_monitor,
        daily_processing_failure_monitor
    ]


# ===== èµ„æºå®šä¹‰é›†åˆ =====

def get_all_resource_definitions():
    """è·å–æ‰€æœ‰èµ„æºå®šä¹‰"""
    return {
        "file_system": file_system_resource,
        "database": database_resource,
        "rqdatac": rqdatac_resource
    }


# ===== é…ç½®éªŒè¯ =====

def validate_configurations():
    """éªŒè¯é…ç½®å®Œæ•´æ€§"""
    logger = get_dagster_logger()

    try:
        # éªŒè¯å¤„ç†é…ç½®
        processing_config = ProcessingConfig()
        assert processing_config.data_dir, "data_dir is required"
        assert processing_config.target_stocks, "target_stocks is required"

        # éªŒè¯æ•°æ®åº“é…ç½®
        db_config = DatabaseConfig()
        assert db_config.host, "DB_HOST is required"
        assert db_config.database, "DB_NAME is required"

        # éªŒè¯RQDatacé…ç½®
        rq_config = RQDatacConfig()
        assert rq_config.api_key, "RQDATAC_API_KEY is required"

        logger.info("âœ… æ‰€æœ‰é…ç½®éªŒè¯é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
        return False


# ===== ä½¿ç”¨ç¤ºä¾‹ =====

if __name__ == "__main__":
    print("=== Dagsteré…ç½®æ¼”ç¤º ===")

    # éªŒè¯é…ç½®
    if validate_configurations():
        print("âœ… é…ç½®éªŒè¯æˆåŠŸ")

        # æ˜¾ç¤ºé…ç½®ç¤ºä¾‹
        processing_config = ProcessingConfig()
        print(f"ğŸ“ æ•°æ®ç›®å½•: {processing_config.data_dir}")
        print(f"ğŸ¯ ç›®æ ‡è‚¡ç¥¨æ•°é‡: {len(processing_config.target_stocks)}")
        print(f"ğŸ“Š æŒ‡æ ‡åˆ—è¡¨: {processing_config.indicators}")

        # åˆ›å»ºè¿è¡Œé…ç½®ç¤ºä¾‹
        run_config = create_run_config("sync")
        print("âœ… è¿è¡Œé…ç½®åˆ›å»ºæˆåŠŸ")

    else:
        print("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡å’Œé…ç½®")
