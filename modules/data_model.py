"""
æ•°æ®æ¨¡å‹ (Data Model)
é›†æˆå­˜å‚¨å±‚å’ŒæŸ¥è¯¢å±‚çš„ç»Ÿä¸€æ•°æ®è®¿é—®æ¥å£
"""

import logging
from typing import Dict, Any, Optional, Union, List
from pathlib import Path
from datetime import datetime, timedelta
import polars as pl

from modules.storage.parquet_manager import ParquetManager
from modules.storage.schema_manager import SchemaManager
from modules.query.query_engine import QueryEngine
from modules.compute.data_processor import DataProcessor
from modules.orchestration.pipeline_manager import pipeline_manager

logger = logging.getLogger(__name__)


class DataModel:
    """æ•°æ®æ¨¡å‹ - é›†æˆå­˜å‚¨å±‚å’ŒæŸ¥è¯¢å±‚"""

    def __init__(self):
        # åˆå§‹åŒ–å„å±‚ç»„ä»¶
        self.parquet_manager = ParquetManager()
        self.schema_manager = SchemaManager()
        self.query_engine = QueryEngine()
        self.data_processor = DataProcessor()

        # æ•°æ®ç¼“å­˜
        self._data_cache: Dict[str, pl.DataFrame] = {}
        self._cache_expiry: Dict[str, datetime] = {}

        logger.info("ğŸ¯ æ•°æ®æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def load_market_data(self,
                        symbols: Optional[List[str]] = None,
                        start_date: Optional[str] = None,
                        end_date: Optional[str] = None) -> pl.DataFrame:
        """åŠ è½½å¸‚åœºæ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æ‰€æœ‰
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            å¸‚åœºæ•°æ®DataFrame
        """
        try:
            # é¦–å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
            cache_key = f"market_data_{symbols}_{start_date}_{end_date}"
            if self._is_cache_valid(cache_key):
                logger.info("ğŸ“‹ ä»ç¼“å­˜åŠ è½½å¸‚åœºæ•°æ®")
                return self._data_cache[cache_key]

            # ä»å­˜å‚¨å±‚åŠ è½½æ•°æ®
            if symbols:
                # åŠ è½½æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
                data_frames = []
                for symbol in symbols:
                    try:
                        df = self.parquet_manager.load_parquet(f"market_data_{symbol}")
                        if df is not None:
                            data_frames.append(df)
                    except Exception as e:
                        logger.warning(f"âš ï¸ åŠ è½½è‚¡ç¥¨ {symbol} æ•°æ®å¤±è´¥: {e}")
                        continue

                if data_frames:
                    market_data = pl.concat(data_frames)
                else:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•è‚¡ç¥¨æ•°æ®")
                    return pl.DataFrame()
            else:
                # åŠ è½½æ‰€æœ‰å¸‚åœºæ•°æ®
                market_data = self.parquet_manager.load_parquet("market_data_all")

            # åº”ç”¨æ—¥æœŸè¿‡æ»¤
            if market_data is not None and not market_data.is_empty():
                if start_date:
                    market_data = market_data.filter(pl.col("date") >= start_date)
                if end_date:
                    market_data = market_data.filter(pl.col("date") <= end_date)

                # ç¼“å­˜ç»“æœ
                self._cache_data(cache_key, market_data)
                logger.info(f"ğŸ“Š æˆåŠŸåŠ è½½å¸‚åœºæ•°æ®: {len(market_data)} è¡Œ")
                return market_data
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å¸‚åœºæ•°æ®")
                return pl.DataFrame()

        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return pl.DataFrame()

    def save_market_data(self, data: pl.DataFrame, table_name: str = "market_data") -> bool:
        """ä¿å­˜å¸‚åœºæ•°æ®

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            table_name: è¡¨å

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä¿å­˜åˆ°Parquet
            self.parquet_manager.save_parquet(data, table_name)

            # æ›´æ–°æŸ¥è¯¢å¼•æ“
            self.query_engine.load_parquet_table(table_name, f"data/{table_name}.parquet")

            logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜å¸‚åœºæ•°æ®: {table_name}")
            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return False

    def load_instruments(self) -> pl.DataFrame:
        """åŠ è½½è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        try:
            cache_key = "instruments"
            if self._is_cache_valid(cache_key):
                return self._data_cache[cache_key]

            instruments = self.parquet_manager.load_parquet("instruments")
            if instruments is None:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯")
                return pl.DataFrame()

            self._cache_data(cache_key, instruments)
            logger.info(f"ğŸ“‹ æˆåŠŸåŠ è½½è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯: {len(instruments)} åªè‚¡ç¥¨")
            return instruments

        except Exception as e:
            logger.error(f"âŒ åŠ è½½è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return pl.DataFrame()

    def load_factors(self,
                    factor_names: Optional[List[str]] = None,
                    symbols: Optional[List[str]] = None) -> pl.DataFrame:
        """åŠ è½½å› å­æ•°æ®

        Args:
            factor_names: å› å­åç§°åˆ—è¡¨
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            å› å­æ•°æ®DataFrame
        """
        try:
            cache_key = f"factors_{factor_names}_{symbols}"
            if self._is_cache_valid(cache_key):
                return self._data_cache[cache_key]

            # ä»æŸ¥è¯¢å¼•æ“è·å–å› å­æ•°æ®
            if factor_names:
                factor_list = ','.join([f"'{f}'" for f in factor_names])
                query = f"SELECT * FROM factors WHERE factor_name IN ({factor_list})"
            else:
                query = "SELECT * FROM factors"

            if symbols:
                symbols_str = ','.join([f"'{s}'" for s in symbols])
                query += f" AND symbol IN ({symbols_str})"

            factors = self.query_engine.execute_query(query)

            if factors is not None and not factors.is_empty():
                self._cache_data(cache_key, factors)
                logger.info(f"ğŸ“Š æˆåŠŸåŠ è½½å› å­æ•°æ®: {len(factors)} è¡Œ")
                return factors
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å› å­æ•°æ®")
                return pl.DataFrame()

        except Exception as e:
            logger.error(f"âŒ åŠ è½½å› å­æ•°æ®å¤±è´¥: {e}")
            return pl.DataFrame()

    def execute_query(self, query: str, **params) -> pl.DataFrame:
        """æ‰§è¡ŒSQLæŸ¥è¯¢

        Args:
            query: SQLæŸ¥è¯¢è¯­å¥
            **params: æŸ¥è¯¢å‚æ•°

        Returns:
            æŸ¥è¯¢ç»“æœDataFrame
        """
        try:
            result = self.query_engine.execute_query(query, **params)
            if result is not None:
                logger.info(f"ğŸ” æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ: {len(result)} è¡Œç»“æœ")
                return result
            else:
                logger.warning("âš ï¸ æŸ¥è¯¢æ— ç»“æœ")
                return pl.DataFrame()

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return pl.DataFrame()

    def get_technical_indicators(self,
                               symbols: Optional[List[str]] = None,
                               indicators: Optional[List[str]] = None) -> pl.DataFrame:
        """è·å–æŠ€æœ¯æŒ‡æ ‡æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            indicators: æŒ‡æ ‡åç§°åˆ—è¡¨

        Returns:
            æŠ€æœ¯æŒ‡æ ‡æ•°æ®DataFrame
        """
        try:
            # ä½¿ç”¨ç¼–æ’å±‚çš„æŒ‡æ ‡è®¡ç®—åŠŸèƒ½
            if symbols:
                # åŠ è½½è‚¡ç¥¨æ•°æ®
                market_data = self.load_market_data(symbols)

                if not market_data.is_empty():
                    # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                    indicators_data = pipeline_manager.indicator_calculator.calculate_indicators(
                        market_data, indicators
                    )

                    if isinstance(indicators_data, dict):
                        # å¦‚æœè¿”å›å­—å…¸ï¼Œåˆå¹¶æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®
                        all_indicators = []
                        for symbol_data in indicators_data.values():
                            if symbol_data is not None and not symbol_data.is_empty():
                                all_indicators.append(symbol_data)

                        if all_indicators:
                            return pl.concat(all_indicators)
                        else:
                            return pl.DataFrame()
                    else:
                        return indicators_data or pl.DataFrame()
                else:
                    return pl.DataFrame()
            else:
                # ä»å­˜å‚¨å±‚åŠ è½½é¢„è®¡ç®—çš„æŒ‡æ ‡æ•°æ®
                return self.parquet_manager.load_parquet("technical_indicators") or pl.DataFrame()

        except Exception as e:
            logger.error(f"âŒ è·å–æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            return pl.DataFrame()

    def get_stock_scores(self,
                        symbols: Optional[List[str]] = None,
                        score_type: str = "technical") -> pl.DataFrame:
        """è·å–è‚¡ç¥¨è¯„åˆ†

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            score_type: è¯„åˆ†ç±»å‹

        Returns:
            è‚¡ç¥¨è¯„åˆ†DataFrame
        """
        try:
            if symbols:
                # å®æ—¶è®¡ç®—è¯„åˆ†
                indicators_data = self.get_technical_indicators(symbols)
                if not indicators_data.is_empty():
                    scores = pipeline_manager.score_calculator.calculate_technical_score(indicators_data)
                    ranked_scores = pipeline_manager.score_calculator.rank_stocks(scores)
                    return ranked_scores
                else:
                    return pl.DataFrame()
            else:
                # ä»å­˜å‚¨å±‚åŠ è½½é¢„è®¡ç®—çš„è¯„åˆ†
                return self.parquet_manager.load_parquet("stock_scores") or pl.DataFrame()

        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨è¯„åˆ†å¤±è´¥: {e}")
            return pl.DataFrame()

    def update_data_cache(self):
        """æ›´æ–°æ•°æ®ç¼“å­˜"""
        try:
            # æ¸…é™¤è¿‡æœŸç¼“å­˜
            current_time = datetime.now()
            expired_keys = [
                key for key, expiry in self._cache_expiry.items()
                if current_time > expiry
            ]

            for key in expired_keys:
                if key in self._data_cache:
                    del self._data_cache[key]
                del self._cache_expiry[key]

            logger.info(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆï¼Œæ¸…é™¤ {len(expired_keys)} ä¸ªè¿‡æœŸæ¡ç›®")

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ•°æ®ç¼“å­˜å¤±è´¥: {e}")

    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key in self._cache_expiry:
            return datetime.now() < self._cache_expiry[cache_key]
        return False

    def _cache_data(self, cache_key: str, data: pl.DataFrame, expiry_minutes: int = 30):
        """ç¼“å­˜æ•°æ®

        Args:
            cache_key: ç¼“å­˜é”®
            data: è¦ç¼“å­˜çš„æ•°æ®
            expiry_minutes: è¿‡æœŸæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        """
        self._data_cache[cache_key] = data
        self._cache_expiry[cache_key] = datetime.now() + timedelta(minutes=expiry_minutes)

    def get_data_quality_report(self) -> Dict[str, Any]:
        """è·å–æ•°æ®è´¨é‡æŠ¥å‘Š"""
        try:
            report = {
                "total_records": 0,
                "data_sources": [],
                "last_update": None,
                "quality_score": 0.0
            }

            # æ£€æŸ¥å„ä¸ªæ•°æ®æº
            data_sources = ["market_data", "instruments", "factors", "technical_indicators"]
            total_records = 0

            for source in data_sources:
                try:
                    df = self.parquet_manager.load_parquet(source)
                    if df is not None:
                        record_count = len(df)
                        total_records += record_count
                        report["data_sources"].append({
                            "name": source,
                            "records": record_count,
                            "columns": len(df.columns)
                        })
                except Exception as e:
                    logger.warning(f"âš ï¸ æ£€æŸ¥æ•°æ®æº {source} å¤±è´¥: {e}")

            report["total_records"] = total_records

            # è®¡ç®—è´¨é‡è¯„åˆ†
            if total_records > 0:
                report["quality_score"] = min(100.0, total_records / 10000 * 100)

            return report

        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®è´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")
            return {}


# åˆ›å»ºå…¨å±€å®ä¾‹
data_model = DataModel()
