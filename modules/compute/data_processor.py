"""
æ•°æ®å¤„ç†å™¨ (Data Processor)
åŸºäºPolarsçš„é«˜æ€§èƒ½æ•°æ®æµå¤„ç†

èŒè´£ï¼š
- æ•°æ®åŠ è½½å’Œä¿å­˜
- æ•°æ®éªŒè¯å’Œæ¸…ç†
- æ•°æ®è½¬æ¢å’Œé¢„å¤„ç†
- æ•°æ®æµç®¡ç†
"""

import polars as pl
import numpy as np
from typing import Dict, Any, Optional, Union, List, Tuple
from pathlib import Path
from datetime import datetime, timedelta
import logging

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

logger = logging.getLogger(__name__)


class DataProcessor:
    """Polarsæ•°æ®å¤„ç†å™¨ - ä¸“æ³¨æ•°æ®æµå¤„ç†"""

    def __init__(self):
        self._config: Dict[str, Any] = {}
        self.logger = logging.getLogger(__name__)

    def load_data(self, file_path: Union[str, Path]) -> pl.DataFrame:
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        file_path = Path(file_path)

        if file_path.suffix == '.parquet':
            return pl.read_parquet(file_path)
        elif file_path.suffix == '.csv':
            return pl.read_csv(file_path)
        elif file_path.suffix == '.json':
            return pl.read_json(file_path)
        else:
            raise ValueError(f"Unsupported file format: {file_path.suffix}")

    def save_data(self, df: pl.DataFrame, file_path: Union[str, Path], **kwargs):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        file_path = Path(file_path)

        if file_path.suffix == '.parquet':
            df.write_parquet(file_path, **kwargs)
        elif file_path.suffix == '.csv':
            df.write_csv(file_path, **kwargs)
        elif file_path.suffix == '.json':
            df.write_json(file_path, **kwargs)
        else:
            raise ValueError(f"Unsupported file format: {file_path.suffix}")

    def calculate_missing_fields(self, df: pl.DataFrame) -> pl.DataFrame:
        """è®¡ç®—ç¼ºå¤±çš„å­—æ®µï¼ˆvwap, returnsç­‰ï¼‰

        Args:
            df: åŒ…å«OHLCVæ•°æ®çš„DataFrame

        Returns:
            pl.DataFrame: åŒ…å«è®¡ç®—åå­—æ®µçš„DataFrame
        """
        if df.is_empty():
            return df

        result_df = df.clone()

        # è®¡ç®—VWAP (æˆäº¤é‡åŠ æƒå¹³å‡ä»·)
        if 'vwap' not in result_df.columns or result_df.select(pl.col('vwap').is_null().sum()).item() > 0:
            logger.info("ğŸ”„ è®¡ç®—VWAP (æˆäº¤é‡åŠ æƒå¹³å‡ä»·)")
            if 'amount' in result_df.columns and 'volume' in result_df.columns:
                # VWAP = amount / volume
                result_df = result_df.with_columns([
                    (pl.col('amount') / pl.col('volume')).alias('vwap')
                ])
            else:
                logger.warning("âš ï¸ æ— æ³•è®¡ç®—VWAPï¼šç¼ºå°‘amountæˆ–volumeå­—æ®µ")

        # è®¡ç®—Returns (æ”¶ç›Šç‡)
        if 'returns' not in result_df.columns or result_df.select(pl.col('returns').is_null().sum()).item() > 0:
            logger.info("ğŸ”„ è®¡ç®—Returns (æ”¶ç›Šç‡)")
            if 'close' in result_df.columns:
                # Returns = (close - close_prev) / close_prev
                result_df = result_df.with_columns([
                    ((pl.col('close') - pl.col('close').shift(1)) / pl.col('close').shift(1)).alias('returns')
                ])
            else:
                logger.warning("âš ï¸ æ— æ³•è®¡ç®—Returnsï¼šç¼ºå°‘closeå­—æ®µ")

        # è®¡ç®—Volume Ratio (é‡æ¯”)
        if 'volume_ratio' not in result_df.columns or result_df.select(pl.col('volume_ratio').is_null().sum()).item() > 0:
            logger.info("ğŸ”„ è®¡ç®—Volume Ratio (é‡æ¯”)")
            if 'volume' in result_df.columns:
                # é‡æ¯” = å½“æ—¥æˆäº¤é‡ / è¿‡å»5æ—¥å¹³å‡æˆäº¤é‡
                result_df = result_df.with_columns([
                    (pl.col('volume') / pl.col('volume').rolling_mean(window_size=5)).alias('volume_ratio')
                ])
            else:
                logger.warning("âš ï¸ æ— æ³•è®¡ç®—Volume Ratioï¼šç¼ºå°‘volumeå­—æ®µ")

        # è®¡ç®—Price Change (æ¶¨è·Œå¹…)
        if 'price_change' not in result_df.columns:
            logger.info("ğŸ”„ è®¡ç®—Price Change (æ¶¨è·Œå¹…)")
            if 'close' in result_df.columns:
                result_df = result_df.with_columns([
                    (pl.col('close') - pl.col('close').shift(1)).alias('price_change')
                ])

        logger.info("âœ… ç¼ºå¤±å­—æ®µè®¡ç®—å®Œæˆ")
        return result_df

    def validate_data(self, df: pl.DataFrame, required_columns: Optional[List[str]] = None) -> Tuple[bool, List[str]]:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§

        Args:
            df: è¦éªŒè¯çš„DataFrame
            required_columns: å¿…éœ€çš„åˆ—ååˆ—è¡¨

        Returns:
            Tuple[bool, List[str]]: (æ˜¯å¦æœ‰æ•ˆ, ç¼ºå¤±çš„åˆ—)
        """
        if df.is_empty():
            return False, ["DataFrame is empty"]

        missing_columns = []
        if required_columns:
            missing_columns = [col for col in required_columns if col not in df.columns]

        return len(missing_columns) == 0, missing_columns

    def clean_data(self, df: pl.DataFrame, remove_nulls: bool = True, remove_duplicates: bool = True) -> pl.DataFrame:
        """æ•°æ®æ¸…ç†

        Args:
            df: è¦æ¸…ç†çš„DataFrame
            remove_nulls: æ˜¯å¦ç§»é™¤ç©ºå€¼è¡Œ
            remove_duplicates: æ˜¯å¦ç§»é™¤é‡å¤è¡Œ

        Returns:
            pl.DataFrame: æ¸…ç†åçš„DataFrame
        """
        if df.is_empty():
            return df

        result_df = df.clone()

        # ç§»é™¤ç©ºå€¼
        if remove_nulls:
            # åªç§»é™¤å®Œå…¨ä¸ºç©ºçš„è¡Œ
            result_df = result_df.drop_nulls()

        # ç§»é™¤é‡å¤è¡Œ
        if remove_duplicates and 'date' in result_df.columns:
            result_df = result_df.unique(subset=['date'], keep='first')

        return result_df

    def transform_data(self, df: pl.DataFrame, transformations: Dict[str, Any]) -> pl.DataFrame:
        """æ•°æ®è½¬æ¢

        Args:
            df: è¦è½¬æ¢çš„DataFrame
            transformations: è½¬æ¢é…ç½®å­—å…¸

        Returns:
            pl.DataFrame: è½¬æ¢åçš„DataFrame
        """
        if df.is_empty():
            return df

        result_df = df.clone()

        # æ—¥æœŸæ ¼å¼è½¬æ¢
        if 'date' in transformations:
            date_config = transformations['date']
            if date_config.get('to_datetime', False):
                result_df = result_df.with_columns([
                    pl.col('date').str.strptime(pl.Date, "%Y-%m-%d").alias('date')
                ])

        # æ•°å€¼ç±»å‹è½¬æ¢
        if 'numeric_columns' in transformations:
            numeric_cols = transformations['numeric_columns']
            for col in numeric_cols:
                if col in result_df.columns:
                    result_df = result_df.with_columns([
                        pl.col(col).cast(pl.Float64).alias(col)
                    ])

        return result_df

    def get_data_info(self, df: pl.DataFrame) -> Dict[str, Any]:
        """è·å–æ•°æ®åŸºæœ¬ä¿¡æ¯"""
        if df.is_empty():
            return {"error": "DataFrame is empty"}

        info = {
            "shape": df.shape,
            "columns": df.columns,
            "dtypes": {col: str(df[col].dtype) for col in df.columns},
            "null_counts": {col: df[col].null_count() for col in df.columns},
            "memory_usage": df.estimated_size()
        }

        return info

    def batch_process_files(self, file_paths: List[Union[str, Path]],
                          output_dir: Union[str, Path],
                          transformations: Optional[Dict[str, Any]] = None) -> List[str]:
        """æ‰¹é‡å¤„ç†æ•°æ®æ–‡ä»¶

        Args:
            file_paths: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            transformations: è½¬æ¢é…ç½®

        Returns:
            List[str]: å¤„ç†åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        processed_files = []

        for file_path in file_paths:
            try:
                # åŠ è½½æ•°æ®
                df = self.load_data(file_path)

                # éªŒè¯æ•°æ®
                is_valid, missing_cols = self.validate_data(df)
                if not is_valid:
                    self.logger.warning(f"è·³è¿‡æ–‡ä»¶ {file_path}: ç¼ºå°‘åˆ— {missing_cols}")
                    continue

                # æ¸…ç†æ•°æ®
                df = self.clean_data(df)

                # è½¬æ¢æ•°æ®
                if transformations:
                    df = self.transform_data(df, transformations)

                # ä¿å­˜å¤„ç†åçš„æ•°æ®
                input_path = Path(file_path)
                output_path = output_dir / f"processed_{input_path.name}"
                self.save_data(df, output_path)

                processed_files.append(str(output_path))
                self.logger.info(f"å¤„ç†å®Œæˆ: {file_path} -> {output_path}")

            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                continue

        return processed_files

    def convert_from_pandas(self, pdf: 'pd.DataFrame') -> pl.DataFrame:
        """ä»pandas DataFrameè½¬æ¢ä¸ºPolars DataFrame"""
        if not PANDAS_AVAILABLE:
            raise ImportError("pandas is not available")
        return pl.from_pandas(pdf)

    def convert_to_pandas(self, df: pl.DataFrame) -> 'pd.DataFrame':
        """ä»Polars DataFrameè½¬æ¢ä¸ºpandas DataFrame"""
        if not PANDAS_AVAILABLE:
            raise ImportError("pandas is not available")
        return df.to_pandas()

    def optimize_dataframe(self, df: pl.DataFrame) -> pl.DataFrame:
        """ä¼˜åŒ–DataFrameå†…å­˜ä½¿ç”¨å’Œæ€§èƒ½"""
        if df.is_empty():
            return df

        # é‡æ–°æ’åˆ—åˆ—ä»¥æé«˜è®¿é—®æ€§èƒ½
        # å°†å¸¸ç”¨çš„åˆ—æ”¾åœ¨å‰é¢
        priority_cols = ['order_book_id', 'date', 'close', 'volume']
        other_cols = [col for col in df.columns if col not in priority_cols]

        if all(col in df.columns for col in priority_cols):
            df = df.select(priority_cols + other_cols)

        # ä½¿ç”¨æœ€ä¼˜çš„æ•°æ®ç±»å‹
        optimized_df = df

        # ä¼˜åŒ–æ•°å€¼åˆ—çš„æ•°æ®ç±»å‹
        for col in df.columns:
            if col in ['close', 'high', 'low', 'open', 'volume']:
                dtype = df[col].dtype
                if dtype == pl.Float64:
                    # æ£€æŸ¥æ˜¯å¦å¯ä»¥è½¬æ¢ä¸ºFloat32
                    try:
                        optimized_df = optimized_df.with_columns([
                            pl.col(col).cast(pl.Float32).alias(col)
                        ])
                    except:
                        pass  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸç±»å‹

        # ç§»é™¤é‡å¤çš„ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'date' in optimized_df.columns:
            if 'order_book_id' in optimized_df.columns:
                # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„æ—¥æœŸï¼ˆæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼‰
                duplicate_count = optimized_df.group_by(['order_book_id', 'date']).agg(pl.len()).filter(pl.col('len') > 1).height
                if duplicate_count > 0:
                    logger.warning(f"å‘ç° {duplicate_count} ä¸ªé‡å¤çš„æ—¥æœŸè®°å½•ï¼Œæ­£åœ¨å»é‡")
                    optimized_df = optimized_df.unique(subset=['order_book_id', 'date'], keep='last')
            else:
                # å¦‚æœæ²¡æœ‰è‚¡ç¥¨æ ‡è¯†åˆ—ï¼Œåˆ™æŒ‰æ—¥æœŸå»é‡
                duplicate_count = optimized_df.select(pl.col('date')).is_duplicated().sum()
                if duplicate_count > 0:
                    logger.warning(f"å‘ç° {duplicate_count} ä¸ªé‡å¤çš„æ—¥æœŸè®°å½•ï¼Œæ­£åœ¨å»é‡")
                    optimized_df = optimized_df.unique(subset=['date'], keep='last')

        return optimized_df

    def get_memory_usage(self, df: pl.DataFrame) -> Dict[str, Any]:
        """è·å–DataFrameå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            memory_info = {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'estimated_memory_mb': df.estimated_size() / 1024 / 1024,
                'column_info': {}
            }

            for col in df.columns:
                col_info = {
                    'dtype': str(df[col].dtype),
                    'null_count': df[col].null_count()
                }
                memory_info['column_info'][col] = col_info

            return memory_info
        except Exception as e:
            logger.error(f"è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯å¤±è´¥: {e}")
            return {'error': str(e)}


# å…¨å±€æ•°æ®å¤„ç†å™¨å®ä¾‹
data_processor = DataProcessor()

# =============================================================================
# æ•°æ®è¡¥å…¨åŠŸèƒ½ (Data Completion Features)
# =============================================================================

from typing import Tuple
from dataclasses import dataclass


@dataclass
class CompletionConfig:
    """è¡¥å…¨é…ç½®ç±»"""
    max_fill_days: int = 30  # æœ€å¤§è¡¥å…¨å¤©æ•°
    fill_method: str = "forward"  # è¡¥å…¨æ–¹æ³•: forward, backward, linear
    min_data_points: int = 3  # æœ€å°‘æ•°æ®ç‚¹è¦æ±‚
    quality_threshold: float = 0.8  # è´¨é‡é˜ˆå€¼
    enable_validation: bool = True  # å¯ç”¨éªŒè¯


@dataclass
class CompletionResult:
    """è¡¥å…¨ç»“æœç±»"""
    original_count: int
    completed_count: int
    quality_score: float
    missing_dates: List[str]
    filled_dates: List[str]
    success: bool
    message: str


class DataCompletionManager:
    """æ•°æ®è¡¥å…¨ç®¡ç†å™¨"""

    def __init__(self, config: Optional[CompletionConfig] = None):
        self.config = config or CompletionConfig()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

    def complete_market_data(
        self,
        data: pl.DataFrame,
        symbol: str,
        date_column: str = "date",
        value_columns: Optional[List[str]] = None
    ) -> Tuple[pl.DataFrame, CompletionResult]:
        """
        è¡¥å…¨å¸‚åœºæ•°æ®

        Args:
            data: åŸå§‹æ•°æ®DataFrame
            symbol: è‚¡ç¥¨ä»£ç 
            date_column: æ—¥æœŸåˆ—å
            value_columns: éœ€è¦è¡¥å…¨çš„æ•°å€¼åˆ—

        Returns:
            Tuple[è¡¥å…¨åçš„æ•°æ®, è¡¥å…¨ç»“æœ]
        """
        try:
            # æ•°æ®é¢„å¤„ç†
            processed_data = self._preprocess_data(data, date_column)

            # è¯†åˆ«ç¼ºå¤±æ—¥æœŸ
            missing_dates = self._identify_missing_dates(
                processed_data, date_column, symbol
            )

            if not missing_dates:
                # æ— ç¼ºå¤±æ•°æ®ï¼Œç›´æ¥è¿”å›
                result = CompletionResult(
                    original_count=len(processed_data),
                    completed_count=len(processed_data),
                    quality_score=1.0,
                    missing_dates=[],
                    filled_dates=[],
                    success=True,
                    message="æ•°æ®å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨"
                )
                return processed_data, result

            # æ‰§è¡Œè¡¥å…¨
            completed_data = self._perform_completion(
                processed_data, missing_dates, date_column, value_columns
            )

            # è´¨é‡è¯„ä¼°
            quality_score = self._assess_completion_quality(
                processed_data, completed_data, missing_dates
            )

            # ç”Ÿæˆç»“æœ
            result = CompletionResult(
                original_count=len(processed_data),
                completed_count=len(completed_data),
                quality_score=quality_score,
                missing_dates=[d.strftime("%Y-%m-%d") for d in missing_dates],
                filled_dates=[d.strftime("%Y-%m-%d") for d in missing_dates],
                success=quality_score >= self.config.quality_threshold,
                message=f"è¡¥å…¨å®Œæˆï¼Œè´¨é‡è¯„åˆ†: {quality_score:.2f}"
            )

            self.logger.info(f"æ•°æ®è¡¥å…¨å®Œæˆ: {symbol}, è´¨é‡è¯„åˆ†: {quality_score:.2f}")
            return completed_data, result

        except Exception as e:
            self.logger.error(f"æ•°æ®è¡¥å…¨å¤±è´¥: {symbol}, é”™è¯¯: {str(e)}")
            result = CompletionResult(
                original_count=len(data),
                completed_count=0,
                quality_score=0.0,
                missing_dates=[],
                filled_dates=[],
                success=False,
                message=f"è¡¥å…¨å¤±è´¥: {str(e)}"
            )
            return data, result

    def _preprocess_data(self, data: pl.DataFrame, date_column: str) -> pl.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼æ­£ç¡®
        if date_column in data.columns:
            data = data.with_columns(
                pl.col(date_column).cast(pl.Date).alias(date_column)
            )

        # æŒ‰æ—¥æœŸæ’åº
        data = data.sort(date_column)

        return data

    def _identify_missing_dates(
        self,
        data: pl.DataFrame,
        date_column: str,
        symbol: str
    ) -> List[datetime]:
        """è¯†åˆ«ç¼ºå¤±çš„æ—¥æœŸ"""
        if len(data) < 2:
            return []

        # è·å–æ—¥æœŸèŒƒå›´
        dates = data.select(date_column).to_series().to_list()
        min_date = min(dates)
        max_date = max(dates)

        # ç”Ÿæˆå®Œæ•´æ—¥æœŸåºåˆ—ï¼ˆåªè€ƒè™‘å·¥ä½œæ—¥ï¼‰
        complete_dates = self._generate_trading_dates(min_date, max_date)

        # æ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸ
        existing_dates = set(dates)
        missing_dates = [
            date for date in complete_dates
            if date not in existing_dates
        ]

        # é™åˆ¶è¡¥å…¨èŒƒå›´
        from datetime import timedelta
        max_fill_date = min_date + timedelta(days=self.config.max_fill_days)
        missing_dates = [
            date for date in missing_dates
            if date >= min_date and date <= max_fill_date
        ]

        self.logger.debug(f"è¯†åˆ«åˆ°ç¼ºå¤±æ—¥æœŸ: {symbol}, æ•°é‡: {len(missing_dates)}")
        return missing_dates

    def _generate_trading_dates(self, start_date: datetime, end_date: datetime) -> List[datetime]:
        """ç”Ÿæˆäº¤æ˜“æ—¥åºåˆ—ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä»äº¤æ˜“æ—¥å†è·å–ï¼‰"""
        from datetime import timedelta
        dates = []
        current = start_date
        while current <= end_date:
            # ç®€åŒ–ï¼šæ’é™¤å‘¨æœ«ï¼Œå®é™…åº”è¯¥ç”¨çœŸå®çš„äº¤æ˜“æ—¥å†
            if current.weekday() < 5:  # å‘¨ä¸€åˆ°å‘¨äº”
                dates.append(current)
            current += timedelta(days=1)
        return dates

    def _perform_completion(
        self,
        data: pl.DataFrame,
        missing_dates: List[datetime],
        date_column: str,
        value_columns: Optional[List[str]] = None
    ) -> pl.DataFrame:
        """æ‰§è¡Œæ•°æ®è¡¥å…¨"""
        if not missing_dates:
            return data

        # ç¡®å®šéœ€è¦è¡¥å…¨çš„åˆ—
        if value_columns is None:
            # è‡ªåŠ¨è¯†åˆ«æ•°å€¼åˆ—
            value_columns = self._identify_value_columns(data)

        # æ ¹æ®è¡¥å…¨æ–¹æ³•æ‰§è¡Œè¡¥å…¨
        if self.config.fill_method == "forward":
            completed_data = self._forward_fill_completion(
                data, missing_dates, date_column, value_columns
            )
        elif self.config.fill_method == "backward":
            completed_data = self._backward_fill_completion(
                data, missing_dates, date_column, value_columns
            )
        elif self.config.fill_method == "linear":
            completed_data = self._linear_interpolation_completion(
                data, missing_dates, date_column, value_columns
            )
        else:
            # é»˜è®¤å‰å‘å¡«å……
            completed_data = self._forward_fill_completion(
                data, missing_dates, date_column, value_columns
            )

        return completed_data

    def _identify_value_columns(self, data: pl.DataFrame) -> List[str]:
        """è¯†åˆ«æ•°å€¼åˆ—"""
        value_columns = []
        exclude_columns = {"date", "order_book_id", "symbol", "exchange"}

        for col in data.columns:
            if col not in exclude_columns:
                dtype = data.schema[col]
                if dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]:
                    value_columns.append(col)

        return value_columns

    def _forward_fill_completion(
        self,
        data: pl.DataFrame,
        missing_dates: List[datetime],
        date_column: str,
        value_columns: List[str]
    ) -> pl.DataFrame:
        """å‰å‘å¡«å……è¡¥å…¨"""
        # åˆ›å»ºç¼ºå¤±æ—¥æœŸçš„è¡Œ
        missing_rows = []
        for missing_date in missing_dates:
            # æ‰¾åˆ°æœ€è¿‘çš„å‰ä¸€ä¸ªäº¤æ˜“æ—¥æ•°æ®
            prev_data = data.filter(pl.col(date_column) < missing_date)
            if len(prev_data) > 0:
                last_row = prev_data.tail(1)
                new_row = last_row.with_columns(
                    pl.lit(missing_date).alias(date_column)
                )
                missing_rows.append(new_row)
            else:
                # å¦‚æœæ²¡æœ‰å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œè·³è¿‡è¿™ä¸ªç¼ºå¤±æ—¥æœŸ
                continue

        if missing_rows:
            # åˆå¹¶ç¼ºå¤±è¡Œåˆ°åŸæ•°æ®
            missing_df = pl.concat(missing_rows)
            completed_data = pl.concat([data, missing_df])
            # é‡æ–°æ’åº
            completed_data = completed_data.sort(date_column)
        else:
            completed_data = data

        return completed_data

    def _backward_fill_completion(
        self,
        data: pl.DataFrame,
        missing_dates: List[datetime],
        date_column: str,
        value_columns: List[str]
    ) -> pl.DataFrame:
        """åå‘å¡«å……è¡¥å…¨"""
        # åˆ›å»ºç¼ºå¤±æ—¥æœŸçš„è¡Œ
        missing_rows = []
        for missing_date in missing_dates:
            # æ‰¾åˆ°æœ€è¿‘çš„åä¸€ä¸ªäº¤æ˜“æ—¥æ•°æ®
            next_data = data.filter(pl.col(date_column) > missing_date)
            if len(next_data) > 0:
                first_row = next_data.head(1)
                new_row = first_row.with_columns(
                    pl.lit(missing_date).alias(date_column)
                )
                missing_rows.append(new_row)
            else:
                # å¦‚æœæ²¡æœ‰åä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œè·³è¿‡è¿™ä¸ªç¼ºå¤±æ—¥æœŸ
                continue

        if missing_rows:
            # åˆå¹¶ç¼ºå¤±è¡Œåˆ°åŸæ•°æ®
            missing_df = pl.concat(missing_rows)
            completed_data = pl.concat([data, missing_df])
            # é‡æ–°æ’åº
            completed_data = completed_data.sort(date_column)
        else:
            completed_data = data

        return completed_data

    def _linear_interpolation_completion(
        self,
        data: pl.DataFrame,
        missing_dates: List[datetime],
        date_column: str,
        value_columns: List[str]
    ) -> pl.DataFrame:
        """çº¿æ€§æ’å€¼è¡¥å…¨"""
        # å¯¹äºçº¿æ€§æ’å€¼ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ç¼ºå¤±æ—¥æœŸå‰åçš„æ•°æ®ç‚¹
        completed_data = data.clone()

        for missing_date in missing_dates:
            # æ‰¾åˆ°å‰ä¸€ä¸ªå’Œåä¸€ä¸ªæ•°æ®ç‚¹
            prev_data = data.filter(pl.col(date_column) < missing_date)
            next_data = data.filter(pl.col(date_column) > missing_date)

            if len(prev_data) > 0 and len(next_data) > 0:
                prev_row = prev_data.tail(1)
                next_row = next_data.head(1)

                # åˆ›å»ºæ’å€¼è¡Œ
                new_row_data = {date_column: missing_date}

                # å¯¹æ¯ä¸ªæ•°å€¼åˆ—è¿›è¡Œçº¿æ€§æ’å€¼
                for col in value_columns:
                    if col in prev_row.columns and col in next_row.columns:
                        prev_val = prev_row.select(col).item()
                        next_val = next_row.select(col).item()

                        if prev_val is not None and next_val is not None:
                            # ç®€å•çº¿æ€§æ’å€¼ï¼ˆè¿™é‡Œå¯ä»¥æ”¹è¿›ä¸ºåŸºäºæ—¥æœŸçš„æ’å€¼ï¼‰
                            interpolated_val = (prev_val + next_val) / 2
                            new_row_data[col] = interpolated_val
                        else:
                            # å¦‚æœä»»ä¸€å€¼ä¸ºNoneï¼Œä½¿ç”¨å‰å‘å¡«å……
                            new_row_data[col] = prev_val if prev_val is not None else next_val

                # æ·»åŠ å…¶ä»–éæ•°å€¼åˆ—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                for col in data.columns:
                    if col not in new_row_data and col in prev_row.columns:
                        new_row_data[col] = prev_row.select(col).item()

                # åˆ›å»ºæ–°è¡Œå¹¶æ·»åŠ åˆ°æ•°æ®ä¸­
                new_row_df = pl.DataFrame([new_row_data])
                completed_data = pl.concat([completed_data, new_row_df])

        # é‡æ–°æ’åº
        completed_data = completed_data.sort(date_column)

        return completed_data

    def _assess_completion_quality(
        self,
        original_data: pl.DataFrame,
        completed_data: pl.DataFrame,
        missing_dates: List[datetime]
    ) -> float:
        """è¯„ä¼°è¡¥å…¨è´¨é‡"""
        if len(completed_data) == 0:
            return 0.0

        # åŸºç¡€è´¨é‡æŒ‡æ ‡
        total_records = len(completed_data)

        # è®¡ç®—ç©ºå€¼ç‡
        null_counts = []
        for col in completed_data.columns:
            if col != "date":  # æ’é™¤æ—¥æœŸåˆ—
                null_count = completed_data.select(pl.col(col).is_null().sum()).item()
                null_counts.append(null_count)

        total_nulls = sum(null_counts)
        total_cells = total_records * (len(completed_data.columns) - 1)  # æ’é™¤æ—¥æœŸåˆ—
        null_ratio = total_nulls / total_cells if total_cells > 0 else 0.0

        # è®¡ç®—è¡¥å…¨æˆåŠŸç‡
        expected_additional_records = len(missing_dates)
        actual_additional_records = len(completed_data) - len(original_data)
        completion_rate = actual_additional_records / expected_additional_records if expected_additional_records > 0 else 1.0

        # ç»¼åˆè´¨é‡è¯„åˆ†ï¼š(1 - ç©ºå€¼ç‡) * è¡¥å…¨æˆåŠŸç‡
        quality_score = (1.0 - null_ratio) * completion_rate

        # é™åˆ¶åœ¨0-1èŒƒå›´å†…
        quality_score = max(0.0, min(1.0, quality_score))

        return quality_score


# å…¨å±€è¡¥å…¨ç®¡ç†å™¨å®ä¾‹
completion_manager = DataCompletionManager()


def complete_market_data(
    data: pl.DataFrame,
    symbol: str,
    config: Optional[CompletionConfig] = None
) -> Tuple[pl.DataFrame, CompletionResult]:
    """
    è¡¥å…¨å¸‚åœºæ•°æ®çš„ä¸»å…¥å£å‡½æ•°

    Args:
        data: åŸå§‹æ•°æ®DataFrame
        symbol: è‚¡ç¥¨ä»£ç 
        config: è¡¥å…¨é…ç½®ï¼ˆå¯é€‰ï¼‰

    Returns:
        Tuple[è¡¥å…¨åçš„æ•°æ®, è¡¥å…¨ç»“æœ]
    """
    global completion_manager

    if config:
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
        manager = DataCompletionManager(config)
        return manager.complete_market_data(data, symbol)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        return completion_manager.complete_market_data(data, symbol)


def get_completion_config() -> CompletionConfig:
    """è·å–é»˜è®¤è¡¥å…¨é…ç½®"""
    return CompletionConfig()


def update_completion_config(config: CompletionConfig) -> None:
    """æ›´æ–°å…¨å±€è¡¥å…¨é…ç½®"""
    global completion_manager
    completion_manager.config = config
