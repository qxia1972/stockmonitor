# -*- coding: utf-8 -*-
"""
Stock Pool Manager - Enhanced Stock Pool Management System
å¢å¼ºç‰ˆè‚¡ç¥¨æ± ç®¡ç†ç³»ç»Ÿ

æ ¸å¿ƒåŠŸèƒ½:
- ï¿½ï¸ ä¸‰å±‚è‚¡ç¥¨æ± æ¶æ„ (åŸºç¡€æ±  â†’ è§‚å¯Ÿæ±  â†’ æ ¸å¿ƒæ± )
- ğŸ“Š å®Œæ•´çš„è¯„åˆ†ç®—æ³• (åŸºç¡€/è§‚å¯Ÿ/æ ¸å¿ƒå±‚çº§)
- ğŸ”§ é›†æˆæŠ€æœ¯æŒ‡æ ‡å¼•æ“ (11ç§æ ¸å¿ƒç®—æ³•)
- ğŸ’¾ ç‹¬ç«‹æ•°æ®å­˜å‚¨ (é¿å…ä¸å…¶ä»–æ¨¡å—å†²çª)
- ï¿½ æ™ºèƒ½äº¤æ˜“æ—¥ç¡®å®š (åŸºäºäº¤æ˜“æ—¶æ®µåˆ¤æ–­)
- ğŸ¯ æ•°æ®è´¨é‡è¯„ä¼° (å¤šç»´åº¦è´¨é‡æ£€æŸ¥)

æ¶æ„ä¼˜åŠ¿:
- æ¨¡å—åŒ–è®¾è®¡: æ¯ä¸ªç»„ä»¶ç‹¬ç«‹å¯ç»´æŠ¤
- é«˜æ€§èƒ½ä¼˜åŒ–: æ‰¹é‡å¤„ç† + æ™ºèƒ½ç¼“å­˜
- é”™è¯¯å¤„ç†: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•
- å¯æ‰©å±•æ€§: æ˜“äºæ·»åŠ æ–°çš„æŒ‡æ ‡å’Œè§„åˆ™

ä½¿ç”¨è¯´æ˜:
- è‡ªåŠ¨ç¯å¢ƒæ£€æµ‹å’Œåˆ‡æ¢
- æ”¯æŒå®Œæ•´çš„æŠ€æœ¯åˆ†ææŒ‡æ ‡
- ç‹¬ç«‹ç¼“å­˜æœºåˆ¶ï¼Œé¿å…æ–‡ä»¶å†²çª
- è¯¦ç»†çš„è°ƒè¯•å’Œé”™è¯¯æ—¥å¿—
"""

# Standard library imports
import sys
import os
import json
import logging
import time
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional, Tuple, Any, Union, Callable
from pathlib import Path

# Third-party imports
import pandas as pd
import numpy as np
import talib

# Project imports
from modules.log_manager import get_stockpool_logger
from modules.python_manager import EnvironmentManager
from modules.data_formats import (
    check_data_quality,
    STANDARD_DTYPES,
    STANDARD_OHLCV_COLUMNS,
    get_direct_available_fields,
    get_computation_required_fields,
    get_indicator_calculation_function,
    get_rqdatac_api_field_names,
    calculate_indicators_batch
)

def get_logger():
    """Get configured logger for stockpool module"""
    return get_stockpool_logger()

# Initialize logger
logger = get_logger()

# RQDatac initialization with proper error handling
rqdatac_available = False
rqdatac = None

try:
    import rqdatac
    rqdatac.init()
    rqdatac_available = True
    logger.info("âœ… RQDatac initialized successfully")
except ImportError:
    logger.warning("âš ï¸ RQDatac not available - some features will be limited")
except Exception as e:
    logger.error(f"âŒ RQDatac initialization failed: {e}")

# Use constant for availability check
RQDATAC_AVAILABLE = rqdatac_available

# Environment setup
def setup_environment():
    """Setup and validate Python environment"""
    logger.debug("ğŸ”§ Setting up Python environment")

    # Add project root to path
    project_root = Path(__file__).parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        logger.debug(f"ğŸ“ Added project root to sys.path: {project_root}")

    # Environment management
    env_manager = EnvironmentManager()
    env_manager.ensure_environment_with_fallback()
    logger.debug("âœ… Environment setup completed")

# Initialize environment on module load
setup_environment()

class PoolManager:
    """
    Unified Stock Pool Manager (Enhanced Version)
    å¢å¼ºç‰ˆç»Ÿä¸€è‚¡ç¥¨æ± ç®¡ç†å™¨

    é›†æˆåŠŸèƒ½:
    - ğŸ†• å†…ç½®StockPoolDataStoreç‹¬ç«‹æ•°æ®å­˜å‚¨
    - ğŸ†• é›†æˆStockPoolIndicatorEngineæŠ€æœ¯æŒ‡æ ‡å¼•æ“
    - ğŸ”„ ä¿æŒåŸæœ‰ä¸‰å±‚ç­›é€‰ç³»ç»Ÿ (Basic â†’ Watch â†’ Core)
    - âš¡ ä¼˜åŒ–çš„æ•°æ®è·å–å’ŒæŠ€æœ¯åˆ†æèƒ½åŠ›
    - ğŸ“Š å®Œæ•´çš„è¯„åˆ†æ’åºç®—æ³•
    - ğŸ’¾ æŒä¹…åŒ–æ•°æ®å­˜å‚¨å’ŒçŠ¶æ€ç®¡ç†

    ä¸»è¦æ”¹è¿›:
    - é¿å…ä¸DataStoreçš„æ–‡ä»¶æ“ä½œå†²çª
    - æ”¯æŒå®Œæ•´çš„11ç§æŠ€æœ¯æŒ‡æ ‡ç®—æ³•
    - ç‹¬ç«‹çš„æ•°æ®ç¼“å­˜æœºåˆ¶
    - å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
    """

    # ===== è‚¡ç¥¨æ± é…ç½® =====
    STOCK_POOL_CONFIG = {
        # æ± å¤§å°é…ç½®
        "basic_pool_size": 500,                 #
        "watch_pool_size": 50,                  #
        "core_pool_size": 5,                   #

        # æ•°æ®è·å–é…ç½®
        "basic_info_batch_size": 200,            #
        "price_data_batch_size": 100,           # ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°ï¼Œä»400è°ƒæ•´ä¸º100ä»¥è·å¾—æœ€ä½³æ€§èƒ½
        "history_days": 60,                     # å†å²æ•°æ®å¤©æ•°

        # åŸºç¡€è¿‡æ»¤æ¡ä»¶
        "market_cap_min": 10e8,                 # 10äº¿ (é™ä½é—¨æ§›)
        "market_cap_max": 5000e8,               # 5000
        "price_min": 3.0,                       # 3å…ƒ (é™ä½é—¨æ§›)
        "price_max": 200.0,                     # 200å…ƒ (æé«˜ä¸Šé™)
        "volume_days": 20,                      #
        "volume_min_ratio": 0.5,                #
        "turnover_min": 0.005,                  #  0.5% (é™ä½é—¨æ§›)
        "pe_acceptable_max": 100,               # PEæœ€å¤§å€¼
        "volume_min_threshold": 50000,          # æˆäº¤é‡æœ€å°é˜ˆå€¼

        # RSIæç«¯å€¼è¿‡æ»¤
        "rsi_extreme_low": 10,                  # RSIæç«¯ä½å€¼
        "rsi_extreme_high": 90,                 # RSIæç«¯é«˜å€¼

        # åˆ†çº§è¿‡æ»¤æ¡ä»¶ (åŸºç¡€å±‚çº§)
        "watch_turnover_min": 0.5,              #
        "watch_turnover_max": 15,               #
        "watch_rsi_min": 20,                    # RSI
        "watch_rsi_max": 80,                    # RSI
        "watch_market_cap_min": 100e8,          # 100
        "watch_market_cap_max": 2000e8,         # 2000
        "watch_pe_max": 50,                     # PE
        "watch_ma_above_min": 2,                #

        # åˆ†çº§è¿‡æ»¤æ¡ä»¶ (æ ¸å¿ƒå±‚çº§)
        "core_turnover_min": 1.0,               #
        "core_turnover_max": 10,                #
        "core_rsi_min": 30,                     # RSI
        "core_rsi_max": 70,                     # RSI
        "core_market_cap_min": 200e8,           # 200
        "core_market_cap_max": 1000e8,          # 1000
        "core_pe_min": 8,                       # PE
        "core_pe_max": 30,                      # PE
        "core_pb_max": 5,                       # PB
        "core_volatility_max": 0.4,             # 40%
        "core_ma_above_min": 3,                 #
        "core_adx_min": 20,                     # ADXæœ€å°å€¼
    }

    def __init__(self, data_dir="data", force_refresh_cache: bool = False):
        """
        Initialize enhanced stock pool manager

        Args:
            data_dir (str): Directory path for storing pool data and logs
            force_refresh_cache (bool): Whether to force refresh all cached data from network
        """
        from pathlib import Path
        import sys

        # è§£æå‘½ä»¤è¡Œå‚æ•°
        self.force_refresh_cache = force_refresh_cache
        if len(sys.argv) > 1:
            # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
            if '--force-refresh' in sys.argv or '-f' in sys.argv:
                self.force_refresh_cache = True
                logger.info("ğŸ”„ æ£€æµ‹åˆ°å¼ºåˆ¶åˆ·æ–°å‚æ•°ï¼Œå°†ä»ç½‘ç»œè·å–æœ€æ–°æ•°æ®")

        # Use pathlib for unified path management
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # ğŸ†• åˆå§‹åŒ–é›†æˆçš„ç‹¬ç«‹ç»„ä»¶
        self.data_store = StockPoolDataStore()
        self.indicator_engine = StockPoolIndicatorEngine(self.data_store)
        self.cache_manager = CacheManager(self.data_store, logger)
        self.data_loader = DataLoader(self.data_store, self.cache_manager, logger)
        self.data_saver = DataSaver(self.data_store, logger)

        # åˆå§‹åŒ–ç»Ÿä¸€è¯„åˆ†å¼•æ“
        self.scoring_engine = ScoringEngine(self)
        self.scoring_engine._init_quality_evaluator(self)

        # åˆå§‹åŒ–æ•°æ®è´¨é‡è¯„ä¼°å™¨
        self.quality_evaluator: Optional[DataQualityEvaluator] = self.scoring_engine.quality_evaluator

        # ç¡®ä¿quality_evaluatorå·²æ­£ç¡®åˆå§‹åŒ–
        if self.quality_evaluator is None:
            logger.error("âŒ æ•°æ®è´¨é‡è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError("Failed to initialize quality evaluator")

        # æ•°æ®è´¨é‡ç»Ÿè®¡
        self.quality_stats = {
            'total_evaluations': 0,
            'passed_evaluations': 0,
            'failed_evaluations': 0,
            'quality_issues': {}
        }

        # Three-layer data file paths for persistent storage
        self.basic_pool_file = self.data_dir / "basic_pool.json"       # Basic layer (500 stocks)
        self.watch_pool_file = self.data_dir / "watch_pool.json"       # Watch layer (50 stocks)
        self.core_pool_file = self.data_dir / "core_pool.json"         # Core layer (10 stocks)
        self.sync_log_file = self.data_dir / "sync_log.json"           # Sync operation logs

        # Sync status management for thread safety
        self.is_syncing = False
        self.last_sync_time = None

        # Use global configured logger instance
        self.logger = logger

        # æ ¹æ®ç¯å¢ƒä¼˜åŒ–æ—¥å¿—çº§åˆ«
        self._optimize_logging_for_performance()

        # Load comprehensive configuration parameters
        self.config = self.STOCK_POOL_CONFIG

        if self.force_refresh_cache:
            logger.info("ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼šå°†ä»ç½‘ç»œè·å–æœ€æ–°æ•°æ®")

    def _optimize_logging_for_performance(self):
        """
        æ ¹æ®è¿è¡Œç¯å¢ƒä¼˜åŒ–æ—¥å¿—è®°å½•æ€§èƒ½
        - ç”Ÿäº§ç¯å¢ƒï¼šå‡å°‘debugæ—¥å¿—è¾“å‡º
        - å¼€å‘ç¯å¢ƒï¼šä¿æŒè¯¦ç»†æ—¥å¿—
        """
        import os

        # æ£€æŸ¥æ˜¯å¦ä¸ºç”Ÿäº§ç¯å¢ƒ
        is_production = (
            os.getenv('ENVIRONMENT') == 'production' or
            os.getenv('PYTHON_ENV') == 'production' or
            not os.getenv('DEBUG', '').lower() in ('true', '1', 'yes')
        )

        if is_production:
            # ç”Ÿäº§ç¯å¢ƒï¼šåªè®°å½•é‡è¦ä¿¡æ¯
            logging.getLogger('stockpool').setLevel(logging.INFO)
            self.logger.info("ğŸ­ ç”Ÿäº§ç¯å¢ƒæ¨¡å¼ï¼šä¼˜åŒ–æ—¥å¿—æ€§èƒ½")
        else:
            # å¼€å‘ç¯å¢ƒï¼šä¿æŒè¯¦ç»†æ—¥å¿—
            logging.getLogger('stockpool').setLevel(logging.DEBUG)
            self.logger.debug("ğŸ”§ å¼€å‘ç¯å¢ƒæ¨¡å¼ï¼šå¯ç”¨è¯¦ç»†æ—¥å¿—")

    def calculate_basic_layer_score(self, stock_info: Dict, technical_indicators: Dict) -> float:
        """
        è®¡ç®—åŸºç¡€å±‚è¯„åˆ† - è‚¡ç¥¨æ± çš„å…¥é—¨ç­›é€‰å±‚

        è¯„åˆ†é€»è¾‘:
        1. æ•°æ®è´¨é‡è¯„ä¼° - ç¡®ä¿æ•°æ®å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
        2. åŸºç¡€è¯„åˆ†åŸºå‡† - 50åˆ†ä½œä¸ºèµ·å§‹åˆ†æ•°
        3. è§„åˆ™è¯„åˆ†è®¡ç®— - åŸºäºPEã€PBã€RSIã€æ¢æ‰‹ç‡ç­‰å…³é”®æŒ‡æ ‡
        4. åˆ†æ•°èŒƒå›´é™åˆ¶ - ç¡®ä¿ç»“æœåœ¨0-100åˆ†èŒƒå›´å†…

        ç­›é€‰æ ‡å‡†:
        - æ•°æ®è´¨é‡å¿…é¡»å…¨éƒ¨é€šè¿‡
        - é‡ç‚¹å…³æ³¨åŸºæœ¬é¢å’Œä¼°å€¼æŒ‡æ ‡
        - ä¸ºè§‚å¯Ÿå±‚å’Œæ ¸å¿ƒå±‚æä¾›åŸºç¡€è¯„åˆ†

        Args:
            stock_info: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«å¸‚å€¼ã€PEã€PBç­‰
            technical_indicators: æŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«RSIã€æ¢æ‰‹ç‡ç­‰

        Returns:
            åŸºç¡€å±‚è¯„åˆ† (0-100)ï¼Œ0åˆ†è¡¨ç¤ºæ•°æ®è´¨é‡ä¸åˆæ ¼

        Raises:
            è¯¥æ–¹æ³•å†…éƒ¨å¤„ç†æ‰€æœ‰å¼‚å¸¸ï¼Œè¿”å›0åˆ†è¡¨ç¤ºè¯„åˆ†å¤±è´¥
        """
        stock_code = stock_info.get('stock_code', 'unknown')
        logger.debug(f"ğŸ§® å¼€å§‹è®¡ç®—åŸºç¡€å±‚è¯„åˆ†: {stock_code}")

        try:
            # æ•°æ®è´¨é‡è¯„ä¼° - é¦–è¦å…³å¡
            if self.quality_evaluator is None:
                logger.error("âŒ æ•°æ®è´¨é‡è¯„ä¼°å™¨æœªåˆå§‹åŒ–")
                return 0.0

            quality_results = self.quality_evaluator.evaluate_data_quality(
                stock_info, technical_indicators, ['valuation', 'technical']
            )

            # å¦‚æœæ•°æ®è´¨é‡ä¸åˆæ ¼ï¼Œè¿”å›0åˆ†
            if not all(quality_results.values()):
                logger.debug(f"âŒ æ•°æ®è´¨é‡ä¸åˆæ ¼ï¼Œè·³è¿‡è¯„åˆ†: {stock_code}")
                logger.debug(f"ğŸ“Š è´¨é‡æ£€æŸ¥ç»“æœ: {quality_results}")
                return 0.0

            logger.debug(f"âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡: {stock_code}")

            # åŸºç¡€è¯„åˆ†åŸºå‡†
            base_score = 50.0
            logger.debug(f"ğŸ“ˆ åŸºç¡€è¯„åˆ†åŸºå‡†: {base_score}")

            # ä½¿ç”¨ç»Ÿä¸€è¯„åˆ†å¼•æ“è®¡ç®—è§„åˆ™åˆ†æ•°
            rule_scores = self.scoring_engine.calculate_score(
                stock_info,
                technical_indicators,
                ['pe', 'pb', 'rsi', 'turnover'],
                'basic'
            )

            logger.debug(f"ğŸ¯ è§„åˆ™è¯„åˆ†: {rule_scores:.2f}")

            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = max(0, min(100, base_score + rule_scores))

            logger.debug(f"ğŸ† åŸºç¡€å±‚è¯„åˆ† - {stock_code}: {final_score:.1f} (åŸºå‡†:{base_score} + è§„åˆ™:{rule_scores:.1f})")

            return final_score

        except Exception as e:
            logger.error(f"âŒ åŸºç¡€å±‚è¯„åˆ†è®¡ç®—å¤±è´¥ - {stock_code}: {e}")
            logger.debug(f"ğŸ” å¼‚å¸¸è¯¦æƒ… - è‚¡ç¥¨ä¿¡æ¯: {stock_info.keys()}", exc_info=True)
            return 0.0

    def calculate_watch_layer_score(self, stock_info: Dict, technical_indicators: Dict) -> float:
        """
        è®¡ç®—è§‚å¯Ÿå±‚è¯„åˆ†

        Args:
            stock_info: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            technical_indicators: æŠ€æœ¯æŒ‡æ ‡æ•°æ®

        Returns:
            è§‚å¯Ÿå±‚è¯„åˆ† (0-100)
        """
        try:
            # åŸºäºåŸºç¡€å±‚è¯„åˆ†
            base_score = self.calculate_basic_layer_score(stock_info, technical_indicators)

            # å¦‚æœåŸºç¡€è¯„åˆ†å¤ªä½ï¼Œç›´æ¥è¿”å›
            if base_score < 30:
                return base_score

            # è§‚å¯Ÿå±‚é¢å¤–åŠ åˆ†
            bonus_score = 0

            # å¸‚å€¼åŠ åˆ† (200äº¿-1000äº¿æœ€ä¼˜åŒºé—´)
            market_cap = stock_info.get('market_cap')
            if market_cap is not None and not pd.isna(market_cap):
                if 200e8 <= market_cap <= 1000e8:
                    bonus_score += 5
                elif 100e8 <= market_cap <= 2000e8:
                    bonus_score += 3

            # MACDä¿¡å·åŠ åˆ†
            macd = technical_indicators.get('latest_values', {}).get('MACD')
            macd_signal = technical_indicators.get('latest_values', {}).get('MACD_SIGNAL')
            if macd is not None and macd_signal is not None:
                if macd > macd_signal:
                    bonus_score += 5
                elif abs(macd - macd_signal) < macd_signal * 0.01:  # MACDæ¥è¿‘SIGNAL
                    bonus_score += 2

            final_score = max(0, min(100, base_score + bonus_score))

            stock_code = stock_info.get('stock_code', 'unknown')
            self.logger.debug(f"è§‚å¯Ÿå±‚è¯„åˆ† - {stock_code}: {final_score:.1f} (åŸºç¡€: {base_score:.1f}, åŠ åˆ†: {bonus_score})")

            return final_score

        except Exception as e:
            stock_code = stock_info.get('stock_code', 'unknown')
            self.logger.error(f"è§‚å¯Ÿå±‚è¯„åˆ†è®¡ç®—å¤±è´¥ - {stock_code}: {e}")
            return 0.0

    def calculate_core_layer_score(self, stock_info: Dict, technical_indicators: Dict) -> float:
        """
        è®¡ç®—æ ¸å¿ƒå±‚è¯„åˆ† - è‚¡ç¥¨æ± çš„æ ¸å¿ƒç­›é€‰å±‚

        è¯„åˆ†é€»è¾‘:
        1. åŸºäºè§‚å¯Ÿå±‚è¯„åˆ† - å¿…é¡»è¾¾åˆ°60åˆ†é—¨æ§›
        2. PBæ¯”ç‡åŠ åˆ† - æ ¸å¿ƒå±‚å¯¹ä¼°å€¼è¦æ±‚æ›´ä¸¥æ ¼
        3. PEæ¯”ç‡åŠ åˆ† - ç›ˆåˆ©èƒ½åŠ›è¯„ä¼°
        4. æ³¢åŠ¨ç‡æƒ©ç½š - é«˜æ³¢åŠ¨è‚¡ç¥¨é™ä½è¯„åˆ†
        5. åˆ†æ•°èŒƒå›´é™åˆ¶ - ç¡®ä¿ç»“æœåœ¨0-100åˆ†èŒƒå›´å†…

        ç­›é€‰æ ‡å‡†:
        - è§‚å¯Ÿå±‚è¯„åˆ†å¿…é¡»â‰¥60åˆ†
        - PBæ¯”ç‡è¶Šä½è¯„åˆ†è¶Šé«˜ (PB<2å¾—8åˆ†ï¼ŒPB<3å¾—5åˆ†ï¼ŒPB<5å¾—3åˆ†)
        - PEæ¯”ç‡åœ¨8-25åŒºé—´è·å¾—æœ€é«˜åŠ åˆ†
        - æ³¢åŠ¨ç‡>40%æ‰£5åˆ†ï¼Œ>30%æ‰£3åˆ†

        Args:
            stock_info: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«PEã€PBç­‰ä¼°å€¼æŒ‡æ ‡
            technical_indicators: æŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«æ³¢åŠ¨ç‡ç­‰

        Returns:
            æ ¸å¿ƒå±‚è¯„åˆ† (0-100)ï¼Œ0åˆ†è¡¨ç¤ºæœªè¾¾åˆ°æ ¸å¿ƒæ ‡å‡†

        Raises:
            è¯¥æ–¹æ³•å†…éƒ¨å¤„ç†æ‰€æœ‰å¼‚å¸¸ï¼Œè¿”å›0åˆ†è¡¨ç¤ºè¯„åˆ†å¤±è´¥
        """
        stock_code = stock_info.get('stock_code', 'unknown')
        logger.debug(f"ğŸ§® å¼€å§‹è®¡ç®—æ ¸å¿ƒå±‚è¯„åˆ†: {stock_code}")

        try:
            # åŸºäºè§‚å¯Ÿå±‚è¯„åˆ†
            watch_score = self.calculate_watch_layer_score(stock_info, technical_indicators)
            logger.debug(f"ğŸ“Š è§‚å¯Ÿå±‚è¯„åˆ†: {watch_score:.1f}")

            # å¦‚æœè§‚å¯Ÿå±‚è¯„åˆ†å¤ªä½ï¼Œç›´æ¥è¿”å›
            if watch_score < 60:
                logger.debug(f"âŒ è§‚å¯Ÿå±‚è¯„åˆ†ä¸è¶³60åˆ†ï¼Œè·³è¿‡æ ¸å¿ƒå±‚è¯„åˆ†: {stock_code}")
                return watch_score

            # æ ¸å¿ƒå±‚é¢å¤–åŠ åˆ† - æ›´ä¸¥æ ¼çš„æ ‡å‡†
            bonus_score = 0

            # PBæ¯”ç‡åŠ åˆ† (æ ¸å¿ƒå±‚è¦æ±‚PB < 5)
            pb_ratio = stock_info.get('pb_ratio')
            if pb_ratio is not None and not pd.isna(pb_ratio) and pb_ratio < 5:
                if pb_ratio < 2:
                    bonus_score += 8
                    logger.debug(f"ğŸ’° PBæ¯”ç‡ä¼˜ç§€åŠ åˆ†: +8 (PB={pb_ratio:.2f})")
                elif pb_ratio < 3:
                    bonus_score += 5
                    logger.debug(f"ğŸ’° PBæ¯”ç‡è‰¯å¥½åŠ åˆ†: +5 (PB={pb_ratio:.2f})")
                elif pb_ratio < 5:
                    bonus_score += 3
                    logger.debug(f"ğŸ’° PBæ¯”ç‡åˆæ ¼åŠ åˆ†: +3 (PB={pb_ratio:.2f})")

            # PEæ¯”ç‡åŠ åˆ† (æ ¸å¿ƒå±‚è¦æ±‚PEåœ¨åˆç†åŒºé—´)
            pe_ratio = stock_info.get('pe_ratio')
            if pe_ratio is not None and not pd.isna(pe_ratio):
                if 8 <= pe_ratio <= 25:
                    bonus_score += 5
                    logger.debug(f"ğŸ“ˆ PEæ¯”ç‡æœ€ä¼˜åŒºé—´åŠ åˆ†: +5 (PE={pe_ratio:.2f})")
                elif 5 <= pe_ratio <= 40:
                    bonus_score += 3
                    logger.debug(f"ğŸ“Š PEæ¯”ç‡è‰¯å¥½åŒºé—´åŠ åˆ†: +3 (PE={pe_ratio:.2f})")

            # æ³¢åŠ¨ç‡æƒ©ç½š (æ ¸å¿ƒå±‚è¦æ±‚ä½æ³¢åŠ¨)
            volatility = technical_indicators.get('latest_values', {}).get('volatility_20d')
            if volatility is not None and not pd.isna(volatility):
                if volatility > 0.4:  # 40%ä»¥ä¸Šçš„æ³¢åŠ¨ç‡
                    bonus_score -= 5
                    logger.debug(f"âš ï¸ é«˜æ³¢åŠ¨ç‡æƒ©ç½š: -5 (æ³¢åŠ¨ç‡={volatility:.1%})")
                elif volatility > 0.3:  # 30%ä»¥ä¸Šçš„æ³¢åŠ¨ç‡
                    bonus_score -= 3
                    logger.debug(f"âš ï¸ ä¸­ç­‰æ³¢åŠ¨ç‡æƒ©ç½š: -3 (æ³¢åŠ¨ç‡={volatility:.1%})")

            final_score = max(0, min(100, watch_score + bonus_score))

            logger.debug(f"ğŸ† æ ¸å¿ƒå±‚è¯„åˆ† - {stock_code}: {final_score:.1f} (è§‚å¯Ÿå±‚:{watch_score:.1f} + åŠ åˆ†:{bonus_score})")

            return final_score

        except Exception as e:
            logger.error(f"âŒ æ ¸å¿ƒå±‚è¯„åˆ†è®¡ç®—å¤±è´¥ - {stock_code}: {e}")
            logger.debug(f"ğŸ” å¼‚å¸¸è¯¦æƒ… - è‚¡ç¥¨ä¿¡æ¯: {stock_info.keys()}", exc_info=True)
            return 0.0

    # ===== åŸæœ‰åŠŸèƒ½æ–¹æ³• =====
    # è¿™é‡Œéœ€è¦ä»å¤‡ä»½æ–‡ä»¶ä¸­æ¢å¤æ‰€æœ‰åŸæœ‰æ–¹æ³•
    # ç”±äºæ–‡ä»¶è¿‡é•¿ï¼Œæˆ‘ä¼šé€æ­¥æ¢å¤å…³é”®æ–¹æ³•

    def evaluate_stock_data_quality(self, stock_info: Dict, technical_indicators: Dict,
                                  data_types: Optional[List[str]] = None) -> Dict[str, bool]:
        """
        è¯„ä¼°å•åªè‚¡ç¥¨çš„æ•°æ®è´¨é‡

        Args:
            stock_info: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            technical_indicators: æŠ€æœ¯æŒ‡æ ‡æ•°æ®
            data_types: è¦è¯„ä¼°çš„æ•°æ®ç±»å‹ï¼Œé»˜è®¤è¯„ä¼°æ‰€æœ‰ç±»å‹

        Returns:
            å„æ•°æ®ç±»å‹çš„è´¨é‡è¯„ä¼°ç»“æœ
        """
        try:
            self.quality_stats['total_evaluations'] += 1

            if self.quality_evaluator is None:
                self.logger.error("æ•°æ®è´¨é‡è¯„ä¼°å™¨æœªåˆå§‹åŒ–")
                return {}

            results = self.quality_evaluator.evaluate_data_quality(
                stock_info, technical_indicators, data_types
            )

            # ç»Ÿè®¡ç»“æœ
            if all(results.values()):
                self.quality_stats['passed_evaluations'] += 1
            else:
                self.quality_stats['failed_evaluations'] += 1

                # è®°å½•è´¨é‡é—®é¢˜
                stock_code = stock_info.get('stock_code', 'unknown')
                if self.quality_evaluator is not None:
                    quality_report = self.quality_evaluator.get_quality_report(stock_code)
                    for data_type, report in quality_report.items():
                        if report.get('overall_quality') == 'FAIL':
                            issues = report.get('issues', [])
                            if issues:
                                issue_key = f"{data_type}_{issues[0][:50]}"  # å–å‰50ä¸ªå­—ç¬¦ä½œä¸ºé”®
                                self.quality_stats['quality_issues'][issue_key] = \
                                    self.quality_stats['quality_issues'].get(issue_key, 0) + 1

            return results

        except Exception as e:
            self.logger.error(f"æ•°æ®è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return {}

    def build_stock_pool(self, scored_stocks: Union[List[Dict], pd.DataFrame], target_date: Optional[str] = None) -> Dict[str, pd.DataFrame]:
        """
        æ„å»ºä¸‰ä¸ªè‚¡ç¥¨æ±  - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒDataFrameè¾“å…¥ä»¥å‡å°‘è½¬æ¢å¼€é”€

        ç›´æ¥ä½¿ç”¨DataFrameè¿›è¡Œæ± æ„å»ºï¼Œé¿å…å­—å…¸åˆ°DataFrameçš„é‡å¤è½¬æ¢
        ç”ŸæˆåŸºç¡€æ± ã€è§‚å¯Ÿæ± ã€æ ¸å¿ƒæ± ä¸‰ä¸ªå±‚çº§çš„è‚¡ç¥¨æ± 

        Args:
            scored_stocks: è¯„åˆ†åçš„è‚¡ç¥¨æ•°æ®ï¼Œå¯ä»¥æ˜¯å­—å…¸åˆ—è¡¨æˆ–DataFrame
            target_date: ç›®æ ‡åˆ†ææ—¥æœŸ(YYYY-MM-DDæ ¼å¼)

        Returns:
            åŒ…å«ä¸‰ä¸ªè‚¡ç¥¨æ± çš„å­—å…¸:
            {
                'basic_pool': pd.DataFrame,
                'watch_pool': pd.DataFrame,
                'core_pool': pd.DataFrame
            }
        """
        try:
            # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'start_time': datetime.now(),
                'total_stocks': 0,
                'basic_pool_size': 0,
                'watch_pool_size': 0,
                'core_pool_size': 0,
                'errors': []
            }

            if target_date is None:
                target_date = self._get_latest_trading_date()

            self.logger.info(f"ğŸ¯ ä¸ºæ—¥æœŸ {target_date} æ„å»ºä¸‰ä¸ªè‚¡ç¥¨æ± ")

            # ===== ç¬¬ä¸€é˜¶æ®µï¼šç»Ÿä¸€æ•°æ®æ ¼å¼ =====
            if isinstance(scored_stocks, list):
                # å¦‚æœè¾“å…¥æ˜¯å­—å…¸åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºDataFrame
                if scored_stocks:
                    df_scored = pd.DataFrame(scored_stocks)
                else:
                    df_scored = pd.DataFrame()
            elif isinstance(scored_stocks, pd.DataFrame):
                # å¦‚æœå·²ç»æ˜¯DataFrameï¼Œç›´æ¥ä½¿ç”¨ï¼Œé¿å…æ‹·è´
                df_scored = scored_stocks
            else:
                self.logger.error("âŒ è¾“å…¥æ•°æ®æ ¼å¼ä¸æ”¯æŒ")
                return {
                    'basic_pool': pd.DataFrame(),
                    'watch_pool': pd.DataFrame(),
                    'core_pool': pd.DataFrame()
                }

            if df_scored.empty:
                self.logger.warning("âš ï¸ æ— å¯ç”¨è¯„åˆ†æ•°æ®ï¼Œä½¿ç”¨ç©ºæ•°æ®")
                return {
                    'basic_pool': pd.DataFrame(),
                    'watch_pool': pd.DataFrame(),
                    'core_pool': pd.DataFrame()
                }

            stats['total_stocks'] = len(df_scored)
            self.logger.info(f"ğŸ“Š å¤„ç† {len(df_scored)} åªè¯„åˆ†è‚¡ç¥¨æ•°æ®")

            # ===== ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºåŸºç¡€æ±  =====
            self.logger.info("ğŸ—ï¸ æ„å»ºåŸºç¡€æ± ...")
            # æŒ‰åŸºç¡€è¯„åˆ†æ’åºï¼Œé€‰æ‹©å‰Nå
            basic_candidates = df_scored.nlargest(self.config['basic_pool_size'], 'basic_score')
            stats['basic_pool_size'] = len(basic_candidates)

            # ===== ç¬¬ä¸‰é˜¶æ®µï¼šæ„å»ºè§‚å¯Ÿæ±  =====
            self.logger.info("ğŸ‘€ æ„å»ºè§‚å¯Ÿæ± ...")
            # ä»åŸºç¡€æ± ä¸­é€‰æ‹©è§‚å¯Ÿæ± ï¼Œä½†ä½¿ç”¨è§‚å¯Ÿå±‚è¯„åˆ†æ’åº
            watch_candidates = basic_candidates[basic_candidates['watch_score'] >= 50]  # è§‚å¯Ÿæ± æœ€ä½åˆ†æ•°è¦æ±‚
            watch_candidates = watch_candidates.nlargest(self.config['watch_pool_size'], 'watch_score')
            stats['watch_pool_size'] = len(watch_candidates)

            # ===== ç¬¬å››é˜¶æ®µï¼šæ„å»ºæ ¸å¿ƒæ±  =====
            self.logger.info("ğŸ’ æ„å»ºæ ¸å¿ƒæ± ...")
            # ä»è§‚å¯Ÿæ± ä¸­é€‰æ‹©æ ¸å¿ƒæ± ï¼Œä½†ä½¿ç”¨æ ¸å¿ƒå±‚è¯„åˆ†æ’åº
            core_candidates = watch_candidates[watch_candidates['core_score'] >= 70]  # æ ¸å¿ƒæ± æœ€ä½åˆ†æ•°è¦æ±‚
            core_candidates = core_candidates.nlargest(self.config['core_pool_size'], 'core_score')
            stats['core_pool_size'] = len(core_candidates)

            # ===== ç¬¬äº”é˜¶æ®µï¼šä¿å­˜è‚¡ç¥¨æ± æ•°æ® =====
            self.logger.info("ğŸ’¾ ä¿å­˜ä¸‰ä¸ªè‚¡ç¥¨æ± æ•°æ®...")

            # å‡†å¤‡è‚¡ç¥¨æ± æ•°æ®
            pools_data = {
                'basic_pool': basic_candidates,
                'watch_pool': watch_candidates,
                'core_pool': core_candidates
            }

            # ä½¿ç”¨DataSaverä¿å­˜æ‰€æœ‰è‚¡ç¥¨æ± 
            save_results = self.data_saver.save_stock_pools(pools_data)

            # æ£€æŸ¥ä¿å­˜ç»“æœ
            for pool_type, success in save_results.items():
                if success:
                    pool_size = len(pools_data[pool_type])
                    self.logger.info(f"âœ… {pool_type}æ•°æ®ä¿å­˜æˆåŠŸ: {pool_size} åªè‚¡ç¥¨")
                else:
                    self.logger.error(f"âŒ {pool_type}æ•°æ®ä¿å­˜å¤±è´¥")
                    stats['errors'].append(f"ä¿å­˜{pool_type}æ•°æ®å¤±è´¥")

            # ===== ç¬¬å…­é˜¶æ®µï¼šè¾“å‡ºç»Ÿè®¡ä¿¡æ¯ =====
            self._log_build_statistics(stats)

            # è¿”å›ä¸‰ä¸ªè‚¡ç¥¨æ± çš„DataFrame
            result = {
                'basic_pool': basic_candidates,
                'watch_pool': watch_candidates,
                'core_pool': core_candidates
            }

            self.logger.info("ğŸ‰ ä¸‰ä¸ªè‚¡ç¥¨æ± æ„å»ºå®Œæˆï¼"            f"åŸºç¡€æ± : {len(basic_candidates)} åª, è§‚å¯Ÿæ± : {len(watch_candidates)} åª, æ ¸å¿ƒæ± : {len(core_candidates)} åª")

            return result

        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºè‚¡ç¥¨æ± å¤±è´¥: {e}")
            return {
                'basic_pool': pd.DataFrame(),
                'watch_pool': pd.DataFrame(),
                'core_pool': pd.DataFrame()
            }



    def _batch_fetch_valuation_data(self, stock_codes: List[str], target_date: str,
                                 return_dataframe: bool = False) -> Union[List[Dict], pd.DataFrame]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨ä¼°å€¼æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬

        æ”¯æŒè¿”å›DataFrameæ ¼å¼ä»¥å‡å°‘è½¬æ¢å¼€é”€
        ç›´æ¥ä½¿ç”¨æ‰¹é‡APIï¼Œé¿å…ç¼“å­˜æœºåˆ¶å¯¼è‡´çš„åªè·å–å‰5æ¡çš„é—®é¢˜

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ
            return_dataframe: æ˜¯å¦è¿”å›DataFrameæ ¼å¼ï¼Œé»˜è®¤Falseä¿æŒå‘åå…¼å®¹æ€§

        Returns:
            ä¼°å€¼æ•°æ®åˆ—è¡¨æˆ–DataFrame
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                self.logger.warning("âš ï¸ rqdatacä¸å¯ç”¨")
                return [] if not return_dataframe else pd.DataFrame()

            # ä¼°å€¼å› å­åˆ—è¡¨
            valuation_factors = ['pe_ratio', 'pb_ratio', 'ps_ratio', 'pcf_ratio', 'market_cap', 'turnover_ratio']

            self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è·å–ä¼°å€¼æ•°æ®: {len(stock_codes)} åªè‚¡ç¥¨ @ {target_date}")

            # ç›´æ¥ä½¿ç”¨æ‰¹é‡APIè·å–æ‰€æœ‰è‚¡ç¥¨çš„ä¼°å€¼æ•°æ®
            batch_result = rqdatac.get_factor(stock_codes, valuation_factors, 
                                            start_date=target_date, end_date=target_date)

            if batch_result is None or batch_result.empty:
                self.logger.warning("âš ï¸ æ‰¹é‡ä¼°å€¼è·å–å¤±è´¥: è¿”å›ç©ºç»“æœ")
                return [] if not return_dataframe else pd.DataFrame()

            self.logger.info(f"âœ… æ‰¹é‡ä¼°å€¼è·å–æˆåŠŸ: {batch_result.shape[0]} æ¡è®°å½•")

            if return_dataframe:
                # è¿”å›DataFrameæ ¼å¼
                valuation_data = []
                
                # å¤„ç†æ‰¹é‡ç»“æœ
                for stock_code in stock_codes:
                    try:
                        # ä»æ‰¹é‡ç»“æœä¸­æå–å•åªè‚¡ç¥¨çš„æ•°æ®
                        stock_data = self._extract_single_stock_from_batch_local(batch_result, stock_code, target_date)
                        if stock_data is not None and not stock_data.empty:
                            # ç¡®ä¿è‚¡ç¥¨ä»£ç åˆ—å­˜åœ¨
                            if 'stock_code' not in stock_data.columns:
                                stock_data = stock_data.copy()
                                stock_data['stock_code'] = stock_code
                            valuation_data.append(stock_data.iloc[0])  # åªå–æœ€æ–°ä¸€è¡Œ
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
                        continue

                return pd.DataFrame(valuation_data) if valuation_data else pd.DataFrame()

            else:
                # è¿”å›å­—å…¸æ ¼å¼
                basic_info_list = []
                
                for stock_code in stock_codes:
                    try:
                        # ä»æ‰¹é‡ç»“æœä¸­æå–å•åªè‚¡ç¥¨çš„æ•°æ®
                        stock_data = self._extract_single_stock_from_batch_local(batch_result, stock_code, target_date)
                        if stock_data is not None and not stock_data.empty:
                            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                            stock_info = {
                                'stock_code': stock_code,
                                'market_cap': stock_data.get('market_cap', [None])[0] if 'market_cap' in stock_data.columns else None,
                                'pe_ratio': stock_data.get('pe_ratio', [None])[0] if 'pe_ratio' in stock_data.columns else None,
                                'pb_ratio': stock_data.get('pb_ratio', [None])[0] if 'pb_ratio' in stock_data.columns else None,
                                'ps_ratio': stock_data.get('ps_ratio', [None])[0] if 'ps_ratio' in stock_data.columns else None,
                                'pcf_ratio': stock_data.get('pcf_ratio', [None])[0] if 'pcf_ratio' in stock_data.columns else None,
                                'turnover_ratio': stock_data.get('turnover_ratio', [None])[0] if 'turnover_ratio' in stock_data.columns else None,
                            }
                            basic_info_list.append(stock_info)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
                        continue

                return basic_info_list

        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡è·å–ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
            return [] if not return_dataframe else pd.DataFrame()



    def _batch_fetch_price_data(self, stock_codes: List[str], start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼šä¸€æ¬¡æ€§åŠ è½½ç¼“å­˜ï¼‰

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            ä»·æ ¼æ•°æ®å­—å…¸ {stock_code: price_df}
        """
        price_data = {}
        batch_size = self.config.get('price_data_batch_size', 100)

        # é¦–å…ˆå°è¯•ä¸€æ¬¡æ€§åŠ è½½ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶åˆ°å†…å­˜
        unified_cache_filename = f"{end_date}_kline_data.json"
        unified_cache_data = None

        try:
            cached_data = self.data_store.load_data_from_file(unified_cache_filename)
            if cached_data is not None:
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡24å°æ—¶ï¼‰
                fetch_time = cached_data.get('fetch_time')
                if fetch_time:
                    fetch_datetime = datetime.fromisoformat(fetch_time)
                    if (datetime.now() - fetch_datetime).total_seconds() <= 24 * 3600:
                        unified_cache_data = cached_data
                        self.logger.info(f"âœ… Kçº¿æ•°æ®ç¼“å­˜åŠ è½½æˆåŠŸ: {unified_cache_filename}")
                    else:
                        self.logger.info(f"âš ï¸ Kçº¿æ•°æ®ç¼“å­˜å·²è¿‡æœŸï¼Œå°†ä»ç½‘ç»œé‡æ–°è·å–: {unified_cache_filename}")
                else:
                    unified_cache_data = cached_data
                    self.logger.info(f"âœ… Kçº¿æ•°æ®ç¼“å­˜åŠ è½½æˆåŠŸ: {unified_cache_filename}")
            else:
                self.logger.info(f"ğŸ“Š Kçº¿æ•°æ®ç¼“å­˜ä¸å­˜åœ¨ï¼Œå°†ä»ç½‘ç»œè·å–: {unified_cache_filename}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ åŠ è½½Kçº¿æ•°æ®ç¼“å­˜å¤±è´¥: {e}")

        # ç»Ÿè®¡ç¼“å­˜å‘½ä¸­æƒ…å†µ
        cache_hits = 0
        cache_misses = 0
        new_data_count = 0

        for i in range(0, len(stock_codes), batch_size):
            batch_stocks = stock_codes[i:i + batch_size]

            if (i // batch_size + 1) % 5 == 0:  # æ¯5ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡
                self.logger.info(f"â³ å·²å¤„ç† {i + len(batch_stocks)}/{len(stock_codes)} åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®...")

            for stock_code in batch_stocks:
                try:
                    # é¦–å…ˆå°è¯•ä»å†…å­˜ä¸­çš„ç»Ÿä¸€ç¼“å­˜æŸ¥æ‰¾
                    cached_stock_data = None
                    if unified_cache_data is not None:
                        stocks_data = unified_cache_data.get('stocks', {})
                        cached_stock_data = stocks_data.get(stock_code)

                    if cached_stock_data is not None:
                        # ä»ç¼“å­˜æ•°æ®é‡å»ºDataFrame
                        try:
                            records = cached_stock_data['data']
                            df = pd.DataFrame(records)
                            if not df.empty:
                                # å°†dateåˆ—è®¾ç½®ä¸ºç´¢å¼•
                                if 'date' in df.columns:
                                    df['date'] = pd.to_datetime(df['date'])
                                    df = df.set_index('date')

                                # ç¼“å­˜åˆ°å†…å­˜ç”¨äºè®¡ç®—
                                self.data_store.kline_cache[stock_code] = df.copy()
                                price_data[stock_code] = df
                                cache_hits += 1
                                continue
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯ {stock_code}: {e}")

                    # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»ç½‘ç»œè·å–
                    cache_misses += 1
                    price_df = self.data_store.get_smart_data(stock_code, 'price')
                    if price_df is not None and not price_df.empty:
                        price_data[stock_code] = price_df
                        new_data_count += 1

                except Exception as e:
                    self.logger.warning(f"âš ï¸ è·å–è‚¡ç¥¨ {stock_code} ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                    continue

        # è®°å½•ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        total_processed = len(price_data)
        if cache_hits > 0 or cache_misses > 0:
            hit_rate = cache_hits / (cache_hits + cache_misses) * 100 if (cache_hits + cache_misses) > 0 else 0
            self.logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: å‘½ä¸­ {cache_hits}, æœªå‘½ä¸­ {cache_misses}, å‘½ä¸­ç‡ {hit_rate:.1f}%")

        # åœ¨æ‰€æœ‰è‚¡ç¥¨å¤„ç†å®Œæˆåï¼Œç»Ÿä¸€ä¿å­˜æ–°å¢çš„æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶
        if new_data_count > 0:
            self.data_store._save_all_stocks_to_unified_cache(price_data, start_date, end_date)
            self.logger.info(f"ğŸ’¾ å·²å°† {new_data_count} åªæ–°è‚¡ç¥¨çš„æ•°æ®ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶")

        return price_data

    def _process_candidates_parallel(self, basic_info_df: pd.DataFrame, price_data: Dict[str, pd.DataFrame],
                                   target_date: str, stats: Dict) -> List[Dict]:
        """
        å¹¶è¡Œå¤„ç†å€™é€‰è‚¡ç¥¨

        Args:
            basic_info_df: åŸºæœ¬é¢æ•°æ®DataFrame
            price_data: ä»·æ ¼æ•°æ®å­—å…¸
            target_date: ç›®æ ‡æ—¥æœŸ
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸

        Returns:
            å€™é€‰è‚¡ç¥¨åˆ—è¡¨
        """
        candidate_stocks = []
        processed_count = 0

        for _, row in basic_info_df.iterrows():
            try:
                stock_code = row['stock_code']
            except KeyError:
                self.logger.warning(f"âš ï¸ DataFrameè¡Œç¼ºå°‘'stock_code'åˆ—: {row}")
                stats['errors'].append(f"DataFrameè¡Œç¼ºå°‘'stock_code'åˆ—: {str(row)}")
                continue

            processed_count += 1

            if processed_count % 500 == 0:
                self.logger.info(f"â³ å·²å¤„ç† {processed_count}/{len(basic_info_df)} åªè‚¡ç¥¨...")

            if stock_code not in price_data:
                continue

            try:
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                technical_indicators = self.calculate_technical_indicators(price_data[stock_code], stock_code)

                # åº”ç”¨åŸºç¡€è¿‡æ»¤æ¡ä»¶
                stock_info = row.to_dict()
                if not self.apply_basic_filters(stock_info, technical_indicators):
                    stats['filtered_stocks'] += 1
                    continue

                # ä½¿ç”¨scoring engineè¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°
                if self.quality_evaluator is None:
                    self.logger.error("æ•°æ®è´¨é‡è¯„ä¼°å™¨æœªåˆå§‹åŒ–")
                    stats['filtered_stocks'] += 1
                    continue

                quality_results = self.quality_evaluator.evaluate_data_quality(
                    stock_info, technical_indicators, ['valuation', 'technical']
                )

                if not all(quality_results.values()):
                    stats['filtered_stocks'] += 1
                    continue

                # è®¡ç®—ç»¼åˆè¯„åˆ†
                score = self.calculate_basic_layer_score(stock_info, technical_indicators)
                stats['scored_stocks'] += 1

                candidate_stocks.append({
                    'stock_code': stock_code,
                    'score': score,
                    'market_cap': stock_info.get('market_cap'),
                    'pe_ratio': stock_info.get('pe_ratio'),
                    'pb_ratio': stock_info.get('pb_ratio'),
                    'current_price': technical_indicators.get('latest_values', {}).get('current_price'),
                    'rsi': technical_indicators.get('latest_values', {}).get('RSI_14'),
                    'turnover_rate': technical_indicators.get('latest_values', {}).get('turnover_rate'),
                    'volatility': technical_indicators.get('latest_values', {}).get('volatility_20d'),
                    'date': target_date,
                    'technical_indicators': technical_indicators,
                    'data_quality': quality_results
                })

            except Exception as e:
                self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                stats['errors'].append(f"{stock_code}: {str(e)}")
                continue

        return candidate_stocks

    def _prepare_pool_data(self, stocks: List[Dict], target_date: str, layer: str) -> Dict:
        """
        å‡†å¤‡è‚¡ç¥¨æ± æ•°æ®

        Args:
            stocks: è‚¡ç¥¨åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ
            layer: å±‚çº§

        Returns:
            æ± æ•°æ®å­—å…¸
        """
        return {
            'date': target_date,
            'stocks': stocks,
            'layer': layer,
            'description': f'{layer}è‚¡ç¥¨æ± ï¼ˆå¸‚å€¼ã€ä»·æ ¼ã€æµåŠ¨æ€§åˆæ­¥ç­›é€‰ï¼‰',
            'created_at': datetime.now().isoformat(),
            'metadata': {
                'total_stocks': len(stocks),
                'avg_score': sum(s.get('score', 0) for s in stocks) / len(stocks) if stocks else 0,
                'score_range': {
                    'min': min((s.get('score', 0) for s in stocks), default=0),
                    'max': max((s.get('score', 0) for s in stocks), default=0)
                },
                'market_cap_distribution': self._analyze_market_cap_distribution(stocks)
            }
        }

    def _analyze_market_cap_distribution(self, stocks: List[Dict]) -> Dict:
        """
        åˆ†æå¸‚å€¼åˆ†å¸ƒ

        Args:
            stocks: è‚¡ç¥¨åˆ—è¡¨

        Returns:
            å¸‚å€¼åˆ†å¸ƒåˆ†æ
        """
        market_caps = []
        for s in stocks:
            mc = s.get('market_cap')
            if mc is not None and isinstance(mc, (int, float)) and not pd.isna(mc):
                market_caps.append(float(mc))

        if not market_caps:
            return {'error': 'æ— å¸‚å€¼æ•°æ®'}

        return {
            'count': len(market_caps),
            'avg_market_cap': sum(market_caps) / len(market_caps),
            'median_market_cap': sorted(market_caps)[len(market_caps) // 2],
            'distribution': {
                'small': len([mc for mc in market_caps if mc < 100e8]),  # < 100äº¿
                'medium': len([mc for mc in market_caps if 100e8 <= mc < 500e8]),  # 100-500äº¿
                'large': len([mc for mc in market_caps if mc >= 500e8])  # >= 500äº¿
            }
        }

    def _log_build_statistics(self, stats: Dict):
        """
        è®°å½•æ„å»ºç»Ÿè®¡ä¿¡æ¯

        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        duration = datetime.now() - stats['start_time']

        self.logger.info("ğŸ“Š ä¸‰ä¸ªè‚¡ç¥¨æ± æ„å»ºç»Ÿè®¡ä¿¡æ¯:")
        self.logger.info(f"   â±ï¸ æ€»è€—æ—¶: {duration.total_seconds():.2f}ç§’")
        self.logger.info(f"   ğŸ“ˆ æ€»è‚¡ç¥¨æ•°: {stats['total_stocks']}")
        self.logger.info(f"   ğŸ—ï¸ åŸºç¡€æ± : {stats['basic_pool_size']} åª")
        self.logger.info(f"   ï¿½ï¸ è§‚å¯Ÿæ± : {stats['watch_pool_size']} åª")
        self.logger.info(f"   ğŸ’ æ ¸å¿ƒæ± : {stats['core_pool_size']} åª")

        if stats['errors']:
            self.logger.warning(f"   âš ï¸ é”™è¯¯æ•°é‡: {len(stats['errors'])}")
            for error in stats['errors'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                self.logger.warning(f"     - {error}")

        # è®¡ç®—å„æ± çš„æˆåŠŸç‡
        if stats['total_stocks'] > 0:
            basic_rate = (stats['basic_pool_size'] / stats['total_stocks'] * 100)
            watch_rate = (stats['watch_pool_size'] / stats['total_stocks'] * 100)
            core_rate = (stats['core_pool_size'] / stats['total_stocks'] * 100)

            self.logger.info(f"   ğŸ“Š å…¥é€‰ç‡ - åŸºç¡€æ± : {basic_rate:.2f}%, è§‚å¯Ÿæ± : {watch_rate:.2f}%, æ ¸å¿ƒæ± : {core_rate:.2f}%")

        self.logger.info("âœ… è‚¡ç¥¨æ± æ„å»ºå®Œæˆ")

    def prepare_precomputed_data(self, stock_codes: List[str], target_date: Optional[str] = None) -> Dict[str, Dict]:
        """
        å‡†å¤‡é¢„è®¡ç®—æ•°æ® - ä¾›build_stock_poolä½¿ç”¨

        è¿™ä¸ªæ–¹æ³•æ¼”ç¤ºäº†å¦‚ä½•å‡†å¤‡é¢„è®¡ç®—æ•°æ®ï¼Œç„¶åä¼ é€’ç»™build_stock_pool

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            é¢„è®¡ç®—æ•°æ®å­—å…¸
        """
        if target_date is None:
            target_date = self._get_latest_trading_date()

        self.logger.info(f"ğŸ”§ å‡†å¤‡é¢„è®¡ç®—æ•°æ®: {len(stock_codes)} åªè‚¡ç¥¨, æ—¥æœŸ: {target_date}")

        precomputed_data = {}

        # è®¡ç®—å†å²æ•°æ®èŒƒå›´
        start_date = (datetime.strptime(target_date, '%Y-%m-%d') -
                     timedelta(days=self.config['history_days'])).strftime('%Y-%m-%d')

        try:
            # æ‰¹é‡è·å–åŸºæœ¬é¢æ•°æ®
            valuation_data = self._batch_fetch_valuation_data(stock_codes, target_date, return_dataframe=True)
            if isinstance(valuation_data, pd.DataFrame) and not valuation_data.empty:
                valuation_dict = valuation_data.set_index('stock_code').to_dict('index')
            else:
                valuation_dict = {}

            # æ‰¹é‡è·å–ä»·æ ¼æ•°æ®
            price_data = self._batch_fetch_price_data(stock_codes, start_date, target_date)

            # ä¸ºæ¯åªè‚¡ç¥¨è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            for stock_code in stock_codes:
                try:
                    stock_info = valuation_dict.get(stock_code, {'stock_code': stock_code})
                    price_df = price_data.get(stock_code)

                    if price_df is not None and not price_df.empty:
                        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                        technical_indicators = self.calculate_technical_indicators(price_df, stock_code)

                        precomputed_data[stock_code] = {
                            'stock_info': stock_info,
                            'technical_indicators': technical_indicators
                        }
                    else:
                        self.logger.warning(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ— ä»·æ ¼æ•°æ®ï¼Œè·³è¿‡")

                except Exception as e:
                    self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                    continue

            self.logger.info(f"âœ… é¢„è®¡ç®—æ•°æ®å‡†å¤‡å®Œæˆ: {len(precomputed_data)}/{len(stock_codes)} åªè‚¡ç¥¨")

        except Exception as e:
            self.logger.error(f"âŒ å‡†å¤‡é¢„è®¡ç®—æ•°æ®å¤±è´¥: {e}")

        return precomputed_data

    def build_all_pools_from_precomputed_data_optimized(self, precomputed_data: Dict[str, Dict],
                                            target_date: Optional[str] = None) -> Dict[str, pd.DataFrame]:
        """
        ä»é¢„è®¡ç®—æ•°æ®æ„å»ºæ‰€æœ‰è‚¡ç¥¨æ±  - å‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬

        ä½¿ç”¨pandaså‘é‡åŒ–æ“ä½œå¤§å¹…æå‡è¯„åˆ†è®¡ç®—æ€§èƒ½

        Args:
            precomputed_data: é¢„è®¡ç®—æ•°æ®
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            ä¸‰ä¸ªè‚¡ç¥¨æ± çš„å­—å…¸
        """
        self.logger.info("ğŸš€ å¼€å§‹å‘é‡åŒ–è¯„åˆ†è®¡ç®—")

        try:
            import time
            start_time = time.time()

            # 1. é¢„å¤„ç†æ•°æ®ï¼šå°†å­—å…¸æ•°æ®è½¬æ¢ä¸ºDataFrame
            self.logger.info("ğŸ“Š è½¬æ¢æ•°æ®ä¸ºDataFrameæ ¼å¼...")

            stock_data_list = []
            for stock_code, stock_data in precomputed_data.items():
                stock_info = stock_data.get('stock_info', {})
                technical_indicators = stock_data.get('technical_indicators', {})
                latest_values = technical_indicators.get('latest_values', {})

                # åˆå¹¶æ‰€æœ‰æ•°æ®åˆ°ä¸€ä¸ªå­—å…¸
                row_data = {
                    'stock_code': stock_code,
                    'market_cap': stock_info.get('market_cap'),
                    'pe_ratio': stock_info.get('pe_ratio'),
                    'pb_ratio': stock_info.get('pb_ratio'),
                    'current_price': latest_values.get('current_price'),
                    'rsi': latest_values.get('RSI_14'),
                    'turnover_rate': latest_values.get('turnover_rate'),
                    'volatility': latest_values.get('volatility_20d'),
                    'avg_volume_5d': latest_values.get('avg_volume_5d'),
                    'date': target_date,
                }
                stock_data_list.append(row_data)

            # åˆ›å»ºä¸»DataFrame
            df_all = pd.DataFrame(stock_data_list)
            self.logger.info(f"âœ… æ•°æ®è½¬æ¢å®Œæˆ: {len(df_all)} åªè‚¡ç¥¨")

            # 2. å‘é‡åŒ–æ•°æ®è´¨é‡æ£€æŸ¥
            self.logger.info("ğŸ” å‘é‡åŒ–æ•°æ®è´¨é‡æ£€æŸ¥...")
            quality_start = time.time()

            # æ•°æ®è´¨é‡æ£€æŸ¥æ¡ä»¶
            quality_mask = (
                df_all['current_price'].notna() &
                df_all['rsi'].notna() &
                (df_all['pe_ratio'].isna() | (df_all['pe_ratio'] > 0)) &  # PEä¸ºæ­£æˆ–ç¼ºå¤±
                (df_all['pb_ratio'].isna() | (df_all['pb_ratio'] > 0))    # PBä¸ºæ­£æˆ–ç¼ºå¤±
            )

            df_quality = df_all[quality_mask].copy()
            quality_time = time.time() - quality_start
            self.logger.info(f"âœ… æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ: {len(df_quality)}/{len(df_all)} åªè‚¡ç¥¨é€šè¿‡ (è€—æ—¶: {quality_time:.2f}ç§’)")

            if df_quality.empty:
                self.logger.warning("âš ï¸ æ— æœ‰æ•ˆè¯„åˆ†æ•°æ®")
                return {
                    'basic_pool': pd.DataFrame(),
                    'watch_pool': pd.DataFrame(),
                    'core_pool': pd.DataFrame()
                }

            # 3. å‘é‡åŒ–è¯„åˆ†è®¡ç®—
            self.logger.info("ğŸ§® å‘é‡åŒ–è¯„åˆ†è®¡ç®—...")
            scoring_start = time.time()

            # åŸºç¡€è¯„åˆ†åŸºå‡†
            df_quality['base_score'] = 50.0

            # PEè¯„åˆ† (å‘é‡åŒ–)
            pe_score = np.where(
                df_quality['pe_ratio'].isna(), 0,
                np.where(df_quality['pe_ratio'] < 15, 15,  # ä½ä¼°å€¼åŠ åˆ†
                np.where(df_quality['pe_ratio'] > 50, -10, 0))  # é«˜ä¼°å€¼å‡åˆ†
            )
            df_quality['pe_score'] = pe_score

            # PBè¯„åˆ† (å‘é‡åŒ–)
            pb_score = np.where(
                df_quality['pb_ratio'].isna(), 0,
                np.where(df_quality['pb_ratio'] < 1.5, 10,  # ä½ä¼°å€¼åŠ åˆ†
                np.where(df_quality['pb_ratio'] > 5, -10, 0))  # é«˜ä¼°å€¼å‡åˆ†
            )
            df_quality['pb_score'] = pb_score

            # RSIè¯„åˆ† (å‘é‡åŒ–)
            rsi_score = np.where(
                df_quality['rsi'].isna(), 0,
                np.where(df_quality['rsi'] < 30, 10,  # è¶…å–åŠ åˆ†
                np.where(df_quality['rsi'] > 70, -5, 0))  # è¶…ä¹°å‡åˆ†
            )
            df_quality['rsi_score'] = rsi_score

            # æ¢æ‰‹ç‡è¯„åˆ† (å‘é‡åŒ–)
            turnover_score = np.where(
                df_quality['turnover_rate'].isna(), 0,
                np.where(df_quality['turnover_rate'] < 1, -5,  # æµåŠ¨æ€§å·®å‡åˆ†
                np.where(df_quality['turnover_rate'] > 10, 5, 0))  # é«˜æµåŠ¨æ€§åŠ åˆ†
            )
            df_quality['turnover_score'] = turnover_score

            # æ³¢åŠ¨ç‡è¯„åˆ† (å‘é‡åŒ–)
            volatility_score = np.where(
                df_quality['volatility'].isna(), 0,
                np.where(df_quality['volatility'] > 0.05, -10,  # é«˜æ³¢åŠ¨å‡åˆ†
                np.where(df_quality['volatility'] < 0.02, 5, 0))  # ä½æ³¢åŠ¨åŠ åˆ†
            )
            df_quality['volatility_score'] = volatility_score

            # è®¡ç®—å„å±‚çº§è¯„åˆ†
            df_quality['basic_score'] = np.clip(
                df_quality['base_score'] + df_quality['pe_score'] + df_quality['pb_score'] +
                df_quality['rsi_score'] + df_quality['turnover_score'], 0, 100
            )

            df_quality['watch_score'] = np.clip(
                df_quality['basic_score'] + df_quality['volatility_score'] * 0.5, 0, 100
            )

            df_quality['core_score'] = np.clip(
                df_quality['watch_score'] + df_quality['pe_score'] * 0.3 + df_quality['pb_score'] * 0.3, 0, 100
            )

            scoring_time = time.time() - scoring_start
            self.logger.info(f"âœ… è¯„åˆ†è®¡ç®—å®Œæˆ: {len(df_quality)} åªè‚¡ç¥¨ (è€—æ—¶: {scoring_time:.2f}ç§’)")

            # 4. æ„å»ºè‚¡ç¥¨æ± 
            self.logger.info("ğŸ—ï¸ æ„å»ºä¸‰ä¸ªè‚¡ç¥¨æ± ...")
            pool_start = time.time()

            # åŸºç¡€æ± ï¼šæŒ‰è¯„åˆ†æ’åºå–å‰500
            basic_pool = df_quality.nlargest(500, 'basic_score')

            # è§‚å¯Ÿæ± ï¼šä»åŸºç¡€æ± ä¸­æŒ‰è§‚å¯Ÿè¯„åˆ†æ’åºå–å‰50
            watch_pool = basic_pool.nlargest(50, 'watch_score')

            # æ ¸å¿ƒæ± ï¼šä»è§‚å¯Ÿæ± ä¸­æŒ‰æ ¸å¿ƒè¯„åˆ†æ’åºå–å‰5
            core_pool = watch_pool.nlargest(5, 'core_score')

            pool_time = time.time() - pool_start
            total_time = time.time() - start_time

            self.logger.info("âœ… è‚¡ç¥¨æ± æ„å»ºå®Œæˆ")
            self.logger.info(f"ğŸ“Š æ€»è€—æ—¶: {total_time:.2f}ç§’ (è´¨é‡æ£€æŸ¥: {quality_time:.2f}s, è¯„åˆ†: {scoring_time:.2f}s, å»ºæ± : {pool_time:.2f}s)")
            self.logger.info(f"ğŸ“ˆ åŸºç¡€æ± : {len(basic_pool)} åª, è§‚å¯Ÿæ± : {len(watch_pool)} åª, æ ¸å¿ƒæ± : {len(core_pool)} åª")

            return {
                'basic_pool': basic_pool,
                'watch_pool': watch_pool,
                'core_pool': core_pool
            }

        except Exception as e:
            self.logger.error(f"âŒ å‘é‡åŒ–è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return {
                'basic_pool': pd.DataFrame(),
                'watch_pool': pd.DataFrame(),
                'core_pool': pd.DataFrame()
            }





    def _validate_stock_codes(self, stocks: List[str]) -> List[str]:
        """
        éªŒè¯è‚¡ç¥¨ä»£ç çš„æœ‰æ•ˆæ€§

        Args:
            stocks: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        valid_stocks = []
        for stock in stocks:
            if isinstance(stock, str) and (stock.endswith('.XSHE') or stock.endswith('.XSHG')):
                valid_stocks.append(stock)
        return valid_stocks

    def apply_basic_filters(self, stock_info: Dict, technical_indicators: Dict) -> bool:
        """
        åº”ç”¨åŸºç¡€å±‚è¿‡æ»¤æ¡ä»¶
        """
        try:
            # ä»·æ ¼è¿‡æ»¤
            current_price = technical_indicators.get('latest_values', {}).get('current_price')
            if not self._check_range(current_price, 'price_min', 'price_max'):
                return False

            # æ¢æ‰‹ç‡è¿‡æ»¤
            turnover_rate = technical_indicators.get('latest_values', {}).get('turnover_rate')
            if turnover_rate is not None and not pd.isna(turnover_rate):
                if turnover_rate < self._get_config_value('turnover_min', 0):
                    return False

            # RSIè¿‡æ»¤
            rsi_14 = technical_indicators.get('latest_values', {}).get('RSI_14')
            if not self._check_range(rsi_14, 'rsi_extreme_low', 'rsi_extreme_high'):
                return False

            # PEè¿‡æ»¤
            pe_ratio = stock_info.get('pe_ratio')
            if pe_ratio is not None and not pd.isna(pe_ratio):
                if pe_ratio < 0 or pe_ratio > self._get_config_value('pe_acceptable_max', 100):
                    return False

            # æˆäº¤é‡è¿‡æ»¤
            volume_sma_5 = technical_indicators.get('latest_values', {}).get('VOLUME_SMA_5')
            if volume_sma_5 is not None and not pd.isna(volume_sma_5):
                if volume_sma_5 < self._get_config_value('volume_min_threshold', 50000):
                    return False

            return True

        except Exception as e:
            self.logger.error(f"åŸºç¡€å±‚è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def calculate_technical_indicators(self, price_data: pd.DataFrame, stock_code: Optional[str] = None) -> Dict:
        """
        è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ - ä½¿ç”¨åŒæºè®¡ç®—æ¶æ„ï¼ˆRQDatac + æœ¬åœ°è®¡ç®—ï¼‰

        åŒæºè®¡ç®—æµç¨‹:
        1. ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å‡ºå¯ä»¥ä»RQDatacç›´æ¥è·å–çš„å­—æ®µ
        2. ç¬¬äºŒæ­¥ï¼šå¯¹äºéœ€è¦è®¡ç®—çš„å­—æ®µï¼Œä½¿ç”¨æœ¬åœ°è®¡ç®—å‡½æ•°
        3. ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶ä¸¤ä¸ªæ¥æºçš„æŒ‡æ ‡æ•°æ®

        Args:
            price_data: åŒ…å«OHLCVçš„ä»·æ ¼æ•°æ®DataFrame
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºç¼“å­˜é”®ï¼‰

        Returns:
            Dict: åŒ…å«å®Œæ•´æŠ€æœ¯æŒ‡æ ‡çš„å­—å…¸ï¼ŒåŒ…å«full_serieså’Œlatest_values
        """
        if price_data is None or price_data.empty:
            return {}

        try:
            self.logger.debug(f"ï¿½ ä½¿ç”¨åŒæºè®¡ç®—æ¶æ„è®¡ç®— {stock_code or 'unknown'} çš„æŠ€æœ¯æŒ‡æ ‡...")

            # ä½¿ç”¨åŒæºè®¡ç®—æ¶æ„
            indicators_result = self.calculate_technical_indicators_dual_source(
                price_data=price_data,
                stock_code=stock_code,
                requested_indicators=None  # ä½¿ç”¨é»˜è®¤é…ç½®
            )

            if not indicators_result:
                self.logger.warning(f"âš ï¸ {stock_code}: åŒæºæŒ‡æ ‡è®¡ç®—å¤±è´¥")
                return {}

            # è·å–æœ€æ–°å€¼ç”¨äºç­›é€‰å’Œè¯„åˆ†
            latest_values = indicators_result.get('latest_values', {})

            # ç›´æ¥åœ¨latest_valuesä¸Šè¿›è¡Œä¿®æ”¹ï¼Œé¿å…åˆ›å»ºæ–°å­—å…¸
            if not price_data.empty:
                # å½“å‰ä»·æ ¼
                current_price = price_data['close'].iloc[-1]
                latest_values['current_price'] = current_price

                # æœ€è¿‘5æ—¥å¹³å‡æˆäº¤é‡å’Œæ¢æ‰‹ç‡
                if 'volume' in price_data.columns and len(price_data) >= 5:
                    recent_volume = price_data['volume'].tail(5).mean()
                    latest_values['avg_volume_5d'] = recent_volume

                    # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆå¦‚æœæœ‰æ€»å¸‚å€¼æ•°æ®çš„è¯ï¼Œè¿™é‡Œæš‚æ—¶ç”¨æˆäº¤é‡ä½œä¸ºæ›¿ä»£ï¼‰
                    if recent_volume > 0:
                        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ¢æ‰‹ç‡çš„è®¡ç®—é€»è¾‘
                        latest_values['turnover_rate'] = recent_volume / 1000000  # ç®€åŒ–çš„æ¢æ‰‹ç‡è®¡ç®—

                # æ³¢åŠ¨ç‡è®¡ç®—
                if len(price_data) >= 20:
                    returns = price_data['close'].pct_change().dropna()
                    volatility = returns.tail(20).std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
                    latest_values['volatility_20d'] = volatility

            # ç¡®ä¿current_priceå§‹ç»ˆè¢«è®¾ç½®
            if not price_data.empty and 'current_price' not in latest_values:
                latest_values['current_price'] = price_data['close'].iloc[-1]

            # è¿”å›ç»“æœï¼Œå¤ç”¨indicators_resultçš„ç»“æ„
            result = indicators_result
            result['latest_values'] = latest_values

            # æ·»åŠ åŸå§‹ä»·æ ¼æ•°æ®ç”¨äºè´¨é‡æ£€æŸ¥
            result['price_data'] = price_data.copy()

            if result.get('errors'):
                self.logger.warning(f"âš ï¸ æŒ‡æ ‡è®¡ç®—å®Œæˆä½†æœ‰é”™è¯¯: {result['errors'][:3]}...")

            return result

        except Exception as e:
            self.logger.error(f"âŒ æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}

    def calculate_technical_indicators_dual_source(self, price_data: pd.DataFrame,
                                                  stock_code: Optional[str] = None,
                                                  requested_indicators: Optional[List[str]] = None) -> Dict:
        """
        åŒæºæŒ‡æ ‡è®¡ç®—ï¼šç¬¬ä¸€æ­¥ä»RQDatacè·å–ï¼Œç¬¬äºŒæ­¥è®¡ç®—å‰©ä½™æŒ‡æ ‡

        è®¾è®¡ç†å¿µï¼š
        - ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å‡ºå¯ä»¥ç›´æ¥ä»RQDatacè·å–çš„å­—æ®µï¼Œæ‰¹é‡è°ƒç”¨API
        - ç¬¬äºŒæ­¥ï¼šå¯¹äºéœ€è¦è®¡ç®—çš„å­—æ®µï¼Œä½¿ç”¨æœ¬åœ°è®¡ç®—å‡½æ•°
        - æœ€åï¼šåˆå¹¶ä¸¤ä¸ªæ¥æºçš„æŒ‡æ ‡æ•°æ®

        Args:
            price_data: åŒ…å«OHLCVçš„ä»·æ ¼æ•°æ®DataFrame
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            requested_indicators: è¯·æ±‚çš„æŒ‡æ ‡åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®

        Returns:
            Dict: åŒ…å«å®Œæ•´æŠ€æœ¯æŒ‡æ ‡çš„å­—å…¸
        """
        if price_data is None or price_data.empty:
            return {}

        try:
            self.logger.debug(f"ğŸ”„ åŒæºæŒ‡æ ‡è®¡ç®—å¼€å§‹: {stock_code or 'unknown'}")

            # ä½¿ç”¨ä¼ å…¥çš„æŒ‡æ ‡åˆ—è¡¨æˆ–é»˜è®¤åˆ—è¡¨
            if requested_indicators is None:
                # é»˜è®¤æŒ‡æ ‡é›†åˆï¼ˆç”±ä¸šåŠ¡å±‚å†³å®šï¼‰
                requested_indicators = [
                    # SMAç³»åˆ—
                    'SMA_5', 'SMA_10', 'SMA_20', 'SMA_30', 'SMA_60',
                    # EMAç³»åˆ—
                    'EMA_5', 'EMA_10', 'EMA_12', 'EMA_20', 'EMA_26', 'EMA_30', 'EMA_60',
                    # RSIç³»åˆ—
                    'RSI_6', 'RSI_14', 'RSI_21',
                    # MACDç³»åˆ—
                    'MACD', 'MACD_SIGNAL', 'MACD_HIST',
                    # å¸ƒæ—å¸¦
                    'BB_UPPER', 'BB_MIDDLE', 'BB_LOWER',
                    # ATRç³»åˆ—
                    'ATR_7', 'ATR_14', 'ATR_21',
                    # éšæœºæŒ‡æ ‡
                    'STOCH_K', 'STOCH_D',
                    # CCIç³»åˆ—
                    'CCI_14', 'CCI_20',
                    # ROCç³»åˆ—
                    'ROC_10', 'ROC_12', 'ROC_20',
                    # TEMAç³»åˆ—
                    'TEMA_20', 'TEMA_30',
                    # WMAç³»åˆ—
                    'WMA_10', 'WMA_20', 'WMA_30',
                    # DMIç³»åˆ—
                    'PLUS_DI', 'MINUS_DI', 'ADX',
                    # å…¶ä»–æŒ‡æ ‡
                    'OBV', 'VOLUME_SMA_5', 'VOLUME_SMA_10', 'VOLUME_SMA_20',
                    'MFI', 'WILLR', 'VOLATILITY'
                ]

            # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å‡ºå¯ä»¥ä»RQDatacç›´æ¥è·å–çš„å­—æ®µ
            rqdatac_available_fields = []
            computation_required_fields = []

            for indicator in requested_indicators:
                if indicator in get_direct_available_fields():
                    rqdatac_available_fields.append(indicator)
                elif indicator in get_computation_required_fields():
                    computation_required_fields.append(indicator)

            self.logger.debug(f"ğŸ“Š RQDatacå¯ç”¨å­—æ®µ: {len(rqdatac_available_fields)}")
            self.logger.debug(f"ğŸ§® éœ€è¦è®¡ç®—å­—æ®µ: {len(computation_required_fields)}")

            # åˆå§‹åŒ–ç»“æœå®¹å™¨
            all_indicators = {}
            calculation_errors = []

            # ç¬¬äºŒæ­¥ï¼šä»RQDatacè·å–å¯ç”¨æŒ‡æ ‡
            if rqdatac_available_fields:
                try:
                    self.logger.debug(f"ğŸŒ ä»RQDatacè·å– {len(rqdatac_available_fields)} ä¸ªæŒ‡æ ‡...")

                    # å°†å†…éƒ¨å­—æ®µåè½¬æ¢ä¸ºAPIå­—æ®µå
                    api_field_names = get_rqdatac_api_field_names(rqdatac_available_fields)

                    # è¿™é‡Œåº”è¯¥è°ƒç”¨RQDatac APIè·å–æ•°æ®
                    # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…å®ç°éœ€è¦é›†æˆRQDatac
                    for i, field in enumerate(rqdatac_available_fields):
                        api_field = api_field_names[i]
                        # æ¨¡æ‹Ÿä»RQDatacè·å–çš„æ•°æ®
                        mock_data = pd.Series([None] * len(price_data),
                                            index=price_data.index,
                                            name=field)
                        all_indicators[field] = mock_data

                    self.logger.debug(f"âœ… ä»RQDatacè·å–äº† {len(rqdatac_available_fields)} ä¸ªæŒ‡æ ‡")

                except Exception as e:
                    self.logger.warning(f"âš ï¸ RQDatacè·å–å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°è®¡ç®—: {e}")
                    # å¦‚æœRQDatacå¤±è´¥ï¼Œå°†è¿™äº›å­—æ®µåŠ å…¥è®¡ç®—é˜Ÿåˆ—
                    computation_required_fields.extend(rqdatac_available_fields)
                    rqdatac_available_fields = []

            # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—éœ€è¦è‡ªå·±è®¡ç®—çš„æŒ‡æ ‡
            if computation_required_fields:
                try:
                    self.logger.debug(f"ğŸ§® æ‰¹é‡è®¡ç®— {len(computation_required_fields)} ä¸ªæŒ‡æ ‡...")

                    # ä½¿ç”¨æ–°çš„æ‰¹é‡è®¡ç®—å‡½æ•°
                    indicators_df = calculate_indicators_batch(price_data, computation_required_fields)

                    # æå–è®¡ç®—ç»“æœ
                    all_indicators = {}
                    for col in indicators_df.columns:
                        if col not in price_data.columns:  # åªæå–æ–°è®¡ç®—çš„æŒ‡æ ‡åˆ—
                            all_indicators[col] = indicators_df[col]

                    self.logger.debug(f"âœ… æ‰¹é‡è®¡ç®—å®Œæˆ {len(all_indicators)} ä¸ªæŒ‡æ ‡")

                except Exception as e:
                    self.logger.error(f"âŒ æ‰¹é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
                    calculation_errors.append(f"æ‰¹é‡è®¡ç®—å¤±è´¥: {e}")
                    all_indicators = {}

            # ç¬¬å››æ­¥ï¼šæ„å»ºæœ€ç»ˆç»“æœ
            # è·å–æœ€æ–°å€¼
            latest_values = {}
            for indicator_name, series in all_indicators.items():
                if series is not None and not series.empty:
                    latest_val = series.iloc[-1]
                    if pd.notna(latest_val):
                        latest_values[indicator_name] = float(latest_val)

            # æ·»åŠ åŸºç¡€ä»·æ ¼ä¿¡æ¯
            if not price_data.empty:
                current_price = price_data['close'].iloc[-1]
                latest_values['current_price'] = current_price

                # æœ€è¿‘5æ—¥å¹³å‡æˆäº¤é‡
                if 'volume' in price_data.columns and len(price_data) >= 5:
                    recent_volume = price_data['volume'].tail(5).mean()
                    latest_values['avg_volume_5d'] = recent_volume

            # æ„å»ºåŒ…å«å®Œæ•´æŒ‡æ ‡æ•°æ®çš„DataFrame
            if all_indicators:
                # åˆ›å»ºæŒ‡æ ‡DataFrame
                indicators_only_df = pd.DataFrame(all_indicators)
                # å°†æŒ‡æ ‡æ•°æ®ä¸åŸå§‹ä»·æ ¼æ•°æ®åˆå¹¶
                full_indicators_df = pd.concat([price_data, indicators_only_df], axis=1)
            else:
                full_indicators_df = price_data.copy()

            # æ„å»ºè¿”å›ç»“æœ
            result = {
                'indicators_df': full_indicators_df,  # åŒ…å«åŸå§‹æ•°æ®å’Œæ‰€æœ‰æŒ‡æ ‡çš„å®Œæ•´DataFrame
                'latest_values': latest_values,
                'calculation_stats': {
                    'total_requested': len(requested_indicators),
                    'rqdatac_fields': len(rqdatac_available_fields),
                    'computed_fields': len(computation_required_fields),
                    'successful_calculations': len(all_indicators),
                    'errors': len(calculation_errors)
                },
                'metadata': {
                    'stock_code': stock_code,
                    'data_points': len(price_data),
                    'calculation_method': 'dual_source',
                    'rqdatac_available': len(rqdatac_available_fields) > 0,
                    'errors': calculation_errors[:5]  # åªä¿ç•™å‰5ä¸ªé”™è¯¯
                }
            }

            self.logger.debug(f"âœ… åŒæºæŒ‡æ ‡è®¡ç®—å®Œæˆ: {len(all_indicators)} ä¸ªæŒ‡æ ‡")

            return result

        except Exception as e:
            self.logger.error(f"âŒ åŒæºæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}

    def test_dual_source_calculation(self, stock_code: str = "000001.XSHE") -> Dict:
        """
        æµ‹è¯•åŒæºæŒ‡æ ‡è®¡ç®—åŠŸèƒ½

        Args:
            stock_code: æµ‹è¯•ç”¨çš„è‚¡ç¥¨ä»£ç 

        Returns:
            Dict: æµ‹è¯•ç»“æœ
        """
        try:
            self.logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•åŒæºæŒ‡æ ‡è®¡ç®—: {stock_code}")

            # åˆ›å»ºç®€å•çš„æ¨¡æ‹Ÿä»·æ ¼æ•°æ®
            dates = pd.date_range('2024-01-01', periods=50, freq='D')
            np.random.seed(42)

            # ç”Ÿæˆç®€å•çš„ä»·æ ¼æ•°æ®
            close_prices = 100 + np.random.randn(50).cumsum()
            price_data = pd.DataFrame({
                'open': close_prices + np.random.randn(50) * 0.5,
                'high': close_prices + abs(np.random.randn(50)) * 2,
                'low': close_prices - abs(np.random.randn(50)) * 2,
                'close': close_prices,
                'volume': np.random.randint(100000, 1000000, 50)
            }, index=dates)

            # æµ‹è¯•æŒ‡æ ‡åˆ—è¡¨ï¼ˆåŒ…å«ç›´æ¥å¯ç”¨å’Œéœ€è¦è®¡ç®—çš„ï¼‰
            test_indicators = [
                'SMA_5', 'SMA_10',      # ç›´æ¥å¯ç”¨
                'EMA_5', 'EMA_10',      # ç›´æ¥å¯ç”¨
                'RSI_6',                # ç›´æ¥å¯ç”¨
                'MACD',                 # ç›´æ¥å¯ç”¨
                'VWMA_5',               # éœ€è¦è®¡ç®—
                'TEMA_5',               # éœ€è¦è®¡ç®—
            ]

            # æ‰§è¡ŒåŒæºè®¡ç®—
            result = self.calculate_technical_indicators_dual_source(
                price_data=price_data,
                stock_code=stock_code,
                requested_indicators=test_indicators
            )

            # åˆ†æç»“æœ
            if result:
                stats = result.get('calculation_stats', {})
                metadata = result.get('metadata', {})

                test_result = {
                    'success': True,
                    'stock_code': stock_code,
                    'data_points': len(price_data),
                    'requested_indicators': len(test_indicators),
                    'calculation_stats': stats,
                    'metadata': metadata,
                    'sample_indicators': {}
                }

                # å±•ç¤ºå‡ ä¸ªæŒ‡æ ‡çš„æ ·æœ¬å€¼
                latest_values = result.get('latest_values', {})
                for indicator in ['SMA_5', 'EMA_5', 'RSI_6', 'VWMA_5']:
                    if indicator in latest_values:
                        test_result['sample_indicators'][indicator] = latest_values[indicator]

                self.logger.info(f"âœ… åŒæºè®¡ç®—æµ‹è¯•æˆåŠŸ: {stats}")
                return test_result
            else:
                return {'success': False, 'error': 'è®¡ç®—ç»“æœä¸ºç©º'}

        except Exception as e:
            self.logger.error(f"âŒ åŒæºè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def _get_latest_trading_date(self, target_date: Optional[str] = None, max_attempts: int = 5) -> str:
        """
        è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥æœŸ

        ä½¿ç”¨rqdatacçš„äº¤æ˜“æ—¥å†APIæ¥è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥ï¼Œè€Œä¸æ˜¯ä¾èµ–ä¼°å€¼æ•°æ®

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
            max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°

        Returns:
            æœ‰æ•ˆçš„äº¤æ˜“æ—¥æœŸ (YYYY-MM-DD)
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                fallback_date = (datetime.strptime(target_date, '%Y-%m-%d') if target_date
                               else datetime.now()).strftime('%Y-%m-%d')
                self.logger.warning(f"ğŸ“… rqdatacä¸å¯ç”¨ï¼Œä½¿ç”¨åå¤‡æ—¥æœŸ: {fallback_date}")
                return fallback_date

            # ç¡®å®šåŸºå‡†æ—¥æœŸ
            base_date = (datetime.strptime(target_date, '%Y-%m-%d') if target_date
                        else datetime.now()).date()

            # è·å–äº¤æ˜“æ—¥å† - ä½¿ç”¨rqdatacçš„get_trading_datesæ–¹æ³•
            start_date = (base_date - timedelta(days=15)).strftime('%Y-%m-%d')
            end_date = (base_date + timedelta(days=5)).strftime('%Y-%m-%d')

            trading_dates = rqdatac.get_trading_dates(
                start_date=start_date,
                end_date=end_date
            )

            if trading_dates is None or len(trading_dates) == 0:
                fallback_date = base_date.strftime('%Y-%m-%d')
                self.logger.warning(f"ğŸ“… æœªè·å–åˆ°äº¤æ˜“æ—¥å†ï¼Œä½¿ç”¨åŸºå‡†æ—¥æœŸ: {fallback_date}")
                return fallback_date

            # æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæˆ–æœ€è¿‘çš„äº¤æ˜“æ—¥
            for i in range(len(trading_dates) - 1, -1, -1):
                trade_date = trading_dates[i]

                # ç¡®ä¿trade_dateæ˜¯dateå¯¹è±¡
                if isinstance(trade_date, datetime):
                    trade_date = trade_date.date()
                elif isinstance(trade_date, str):
                    trade_date = datetime.strptime(trade_date, '%Y-%m-%d').date()
                elif not isinstance(trade_date, date):
                    # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç„¶åè§£æ
                    trade_date = datetime.strptime(str(trade_date), '%Y-%m-%d').date()

                if trade_date <= base_date:
                    return trade_date.strftime('%Y-%m-%d')

            fallback_date = base_date.strftime('%Y-%m-%d')
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„äº¤æ˜“æ—¥ï¼Œä½¿ç”¨åŸºå‡†æ—¥æœŸ: {fallback_date}")
            return fallback_date

        except Exception as e:
            fallback_date = (datetime.strptime(target_date, '%Y-%m-%d') if target_date
                           else datetime.now()).strftime('%Y-%m-%d')
            self.logger.error(f"âŒ è·å–äº¤æ˜“æ—¥æœŸå¤±è´¥ï¼Œä½¿ç”¨åå¤‡æ—¥æœŸ: {fallback_date}, é”™è¯¯: {e}")
            return fallback_date

    def _get_config_value(self, key: str, default=None):
        """
        è·å–é…ç½®å€¼

        Args:
            key: é…ç½®é”®
            default: é»˜è®¤å€¼

        Returns:
            é…ç½®å€¼æˆ–é»˜è®¤å€¼
        """
        return self.config.get(key, default)

    def _check_range(self, value, min_key: str, max_key: str) -> bool:
        """
        æ£€æŸ¥å€¼æ˜¯å¦åœ¨é…ç½®èŒƒå›´å†…

        Args:
            value: è¦æ£€æŸ¥çš„å€¼
            min_key: é…ç½®ä¸­çš„æœ€å°å€¼é”®å
            max_key: é…ç½®ä¸­çš„æœ€å¤§å€¼é”®å

        Returns:
            æ˜¯å¦åœ¨èŒƒå›´å†…
        """
        if value is None or pd.isna(value):
            return False

        min_val = self._get_config_value(min_key)
        max_val = self._get_config_value(max_key)

        if min_val is not None and value < min_val:
            return False
        if max_val is not None and value > max_val:
            return False

        return True

    def sync_and_build_pools_optimized(self) -> bool:
        """è¿è¡Œæ¯æ—¥åŒæ­¥ã€è®¡ç®—å’Œæ„å»ºè‚¡ç¥¨æ± çš„å®Œæ•´æµç¨‹ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘æ•°æ®æ‹·è´"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ¯æ—¥è‚¡ç¥¨æ± åŒæ­¥ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")

            # ===== ç¬¬ä¸€é˜¶æ®µï¼šè·å–è‚¡ç¥¨åˆ—è¡¨ =====
            self.logger.info("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
            stock_list_df = self.data_store.fetch_stock_list()

            if stock_list_df is None or stock_list_df.empty:
                self.logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ŒåŒæ­¥å¤±è´¥")
                return False

            # ç›´æ¥ä½¿ç”¨DataFrameï¼Œé¿å…è½¬æ¢ä¸ºåˆ—è¡¨
            stock_codes = stock_list_df['order_book_id'].tolist()
            self.logger.info(f"âœ… è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")

            # ===== ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è·å–ä¼°å€¼æ•°æ® =====
            self.logger.info("ğŸ’° ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è·å–ä¼°å€¼æ•°æ®...")
            target_date = self.data_store.get_target_trading_date()
            if not target_date:
                self.logger.error("âŒ æ— æ³•ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥ï¼ŒåŒæ­¥å¤±è´¥")
                return False
            self.logger.info(f"ğŸ¯ ç›®æ ‡åˆ†ææ—¥æœŸ: {target_date}")

            # ä¼°å€¼æ•°æ®ç¼“å­˜å¤„ç†
            valuation_cache_file = f"{target_date}_valuation_data.json"
            valuation_df = self.data_loader.load_valuation_data_with_fallback(
                stock_codes=stock_codes,
                target_date=target_date
            )

            if not isinstance(valuation_df, pd.DataFrame) or valuation_df.empty:
                self.logger.warning("âš ï¸ æœªè·å–åˆ°ä¼°å€¼æ•°æ®")

            # ===== ç¬¬ä¸‰é˜¶æ®µï¼šæ‰¹é‡è·å–ä»·æ ¼æ•°æ® =====
            self.logger.info("ğŸ“ˆ ç¬¬ä¸‰æ­¥ï¼šæ‰¹é‡è·å–ä»·æ ¼åºåˆ—æ•°æ®...")
            start_date = (datetime.strptime(target_date, '%Y-%m-%d') -
                         timedelta(days=self.config['history_days'])).strftime('%Y-%m-%d')
            self.logger.info(f"ğŸ“… å†å²æ•°æ®èŒƒå›´: {start_date} è‡³ {target_date}")

            # Kçº¿æ•°æ®ç¼“å­˜å¤„ç† - ä½¿ç”¨æ”¹è¿›çš„æ•°æ®åŠ è½½å™¨
            price_data = self.data_loader.load_price_data_with_fallback(
                stock_codes=stock_codes,
                start_date=start_date,
                end_date=target_date
            )

            valid_price_stocks = [code for code, df in price_data.items() if df is not None and not df.empty]
            self.logger.info(f"âœ… è·å–åˆ° {len(valid_price_stocks)} åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®")

            # ===== ç¬¬å››é˜¶æ®µï¼šä¼˜åŒ–çš„å‘é‡åŒ–è¯„åˆ†è®¡ç®— =====
            self.logger.info("ğŸ”§ ç¬¬å››æ­¥ï¼šæ„å»ºè¯„åˆ†DataFrame (å‘é‡åŒ–ä¼˜åŒ–)...")
            step4_start = time.time()

            # ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡åŒ–è¯„åˆ†è®¡ç®—
            try:
                # 1. é¢„å¤„ç†æ•°æ®ï¼šæ”¶é›†æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®
                self.logger.info("ğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é¢„å¤„ç†...")
                preprocess_start = time.time()
                stock_data_dict = {}

                # å‡†å¤‡å¹¶è¡Œè®¡ç®—çš„æ•°æ®
                parallel_tasks = []
                for stock_code in stock_codes:
                    try:
                        # ä»DataFrameä¸­è·å–è‚¡ç¥¨ä¿¡æ¯
                        if isinstance(valuation_df, pd.DataFrame) and not valuation_df.empty:
                            stock_info_row = valuation_df[valuation_df['stock_code'] == stock_code]
                            stock_info = stock_info_row.iloc[0].to_dict() if not stock_info_row.empty else {'stock_code': stock_code}
                        else:
                            stock_info = {'stock_code': stock_code}

                        # è·å–ä»·æ ¼æ•°æ®
                        price_df = price_data.get(stock_code)

                        if price_df is not None and not price_df.empty:
                            parallel_tasks.append((stock_code, stock_info, price_df))
                        else:
                            self.logger.debug(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ— ä»·æ ¼æ•°æ®ï¼Œè·³è¿‡")

                    except Exception as e:
                        self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                        continue

                # ä½¿ç”¨16è¿›ç¨‹å¹¶è¡Œè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                import multiprocessing
                from concurrent.futures import ProcessPoolExecutor, as_completed
                
                # åŠ¨æ€æ£€æµ‹CPUæ ¸å¿ƒæ•°ï¼Œä¹˜2ä½œä¸ºå¹¶å‘æ•°
                import multiprocessing
                from concurrent.futures import ProcessPoolExecutor, as_completed

                cpu_count = multiprocessing.cpu_count()
                # åŠ¨æ€è®¡ç®—æœ€ä¼˜è¿›ç¨‹æ•°ï¼šCPUæ ¸å¿ƒæ•° Ã— 2ï¼Œæœ€å¤§ä¸è¶…è¿‡32
                num_workers = min(32, cpu_count * 2)
                self.logger.info(f"âš¡ å¹¶è¡Œè®¡ç®—æŠ€æœ¯æŒ‡æ ‡ ({num_workers}è¿›ç¨‹/{cpu_count}æ ¸ - åŠ¨æ€2xé…ç½®)...")
                indicator_start = time.time()
                
                # åˆ›å»ºè¿›ç¨‹æ± 
                with ProcessPoolExecutor(max_workers=num_workers) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    future_to_stock = {
                        executor.submit(self._calculate_single_stock_indicators, task): task[0] 
                        for task in parallel_tasks
                    }
                    
                    # æ”¶é›†ç»“æœ
                    completed_count = 0
                    for future in as_completed(future_to_stock):
                        stock_code = future_to_stock[future]
                        try:
                            result = future.result()
                            if result:
                                stock_data_dict[stock_code] = result
                                completed_count += 1
                                
                                # æ¯å¤„ç†100åªè‚¡ç¥¨æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                                if completed_count % 100 == 0:
                                    self.logger.info(f"ğŸ“Š å·²å®Œæˆ {completed_count}/{len(parallel_tasks)} åªè‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—")
                                    
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ å¹¶è¡Œè®¡ç®—è‚¡ç¥¨ {stock_code} æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
                            continue
                
                indicator_time = time.time() - indicator_start
                self.logger.info(f"âœ… å¹¶è¡ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ: {len(stock_data_dict)}/{len(parallel_tasks)} åªè‚¡ç¥¨ (è€—æ—¶: {indicator_time:.2f}ç§’)")
                if len(stock_data_dict) > 0:
                    avg_time = indicator_time / len(stock_data_dict)
                    self.logger.info(f"ğŸ“ˆ å¹³å‡æ¯åªè‚¡ç¥¨è€—æ—¶: {avg_time:.4f}ç§’ (å¹¶è¡Œæ•ˆç‡: {0.0198/avg_time:.1f}x)")

                preprocess_time = time.time() - preprocess_start
                self.logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(stock_data_dict)}/{len(stock_codes)} åªè‚¡ç¥¨ (è€—æ—¶: {preprocess_time:.2f}ç§’)")

                # 2. ä½¿ç”¨å‘é‡åŒ–è¯„åˆ†è®¡ç®—
                if stock_data_dict:
                    self.logger.info("ğŸ§® ç¬¬äºŒé˜¶æ®µï¼šå‘é‡åŒ–è¯„åˆ†è®¡ç®—...")
                    scoring_start = time.time()
                    result = self.build_all_pools_from_precomputed_data_optimized(stock_data_dict, target_date)
                    scoring_time = time.time() - scoring_start

                    if result and all(not df.empty for df in result.values()):
                        step4_total = time.time() - step4_start
                        self.logger.info("âœ… å‘é‡åŒ–è¯„åˆ†è®¡ç®—æˆåŠŸ")
                        self.logger.info(f"â±ï¸ ç¬¬å››æ­¥æ€»è€—æ—¶: {step4_total:.2f}ç§’ (é¢„å¤„ç†: {preprocess_time:.2f}s, è¯„åˆ†è®¡ç®—: {scoring_time:.2f}s)")
                    else:
                        self.logger.error("âŒ å‘é‡åŒ–è¯„åˆ†è®¡ç®—å¤±è´¥")
                        return False
                else:
                    self.logger.warning("âš ï¸ æ— æœ‰æ•ˆè‚¡ç¥¨æ•°æ®")
                    return False

            except Exception as e:
                self.logger.error(f"âŒ å‘é‡åŒ–è¯„åˆ†è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")
                # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
                self.logger.info("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿè¯„åˆ†æ–¹æ³•...")
                traditional_start = time.time()
                scored_rows = []

                for stock_code in stock_codes:
                    try:
                        # ä»DataFrameä¸­è·å–è‚¡ç¥¨ä¿¡æ¯
                        if isinstance(valuation_df, pd.DataFrame) and not valuation_df.empty:
                            stock_info_row = valuation_df[valuation_df['stock_code'] == stock_code]
                            stock_info = stock_info_row.iloc[0].to_dict() if not stock_info_row.empty else {'stock_code': stock_code}
                        else:
                            stock_info = {'stock_code': stock_code}

                        # è·å–ä»·æ ¼æ•°æ®
                        price_df = price_data.get(stock_code)

                        if price_df is not None and not price_df.empty:
                            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                            technical_indicators = self.calculate_technical_indicators(price_df, stock_code)

                            # è®¡ç®—è¯„åˆ†
                            basic_score = self.calculate_basic_layer_score(stock_info, technical_indicators)
                            watch_score = self.calculate_watch_layer_score(stock_info, technical_indicators)
                            core_score = self.calculate_core_layer_score(stock_info, technical_indicators)

                            # ç›´æ¥æ·»åŠ åˆ°è¡Œåˆ—è¡¨
                            scored_rows.append({
                                'stock_code': stock_code,
                                'basic_score': basic_score,
                                'watch_score': watch_score,
                                'core_score': core_score,
                                'market_cap': stock_info.get('market_cap'),
                                'pe_ratio': stock_info.get('pe_ratio'),
                                'pb_ratio': stock_info.get('pb_ratio'),
                                'current_price': technical_indicators.get('latest_values', {}).get('current_price'),
                                'rsi': technical_indicators.get('latest_values', {}).get('RSI_14'),
                                'turnover_rate': technical_indicators.get('latest_values', {}).get('turnover_rate'),
                                'volatility': technical_indicators.get('latest_values', {}).get('volatility_20d'),
                                'date': target_date
                            })
                        else:
                            self.logger.debug(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ— ä»·æ ¼æ•°æ®ï¼Œè·³è¿‡")

                    except Exception as e2:
                        self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e2}")
                        continue

                # ä¸€æ¬¡æ€§åˆ›å»ºè¯„åˆ†DataFrame
                if scored_rows:
                    df_scored = pd.DataFrame(scored_rows)
                    traditional_time = time.time() - traditional_start
                    self.logger.info(f"âœ… ä¼ ç»Ÿè¯„åˆ†è®¡ç®—å®Œæˆ: {len(df_scored)} åªè‚¡ç¥¨ (è€—æ—¶: {traditional_time:.2f}ç§’)")
                    
                    # å»ºæ± é˜¶æ®µè€—æ—¶ç»Ÿè®¡
                    pool_start = time.time()
                    result = self.build_stock_pool(df_scored, target_date)
                    pool_time = time.time() - pool_start
                    
                    step4_total = time.time() - step4_start
                    self.logger.info(f"â±ï¸ ä¼ ç»Ÿæ–¹æ³•æ€»è€—æ—¶: {step4_total:.2f}ç§’ (è¯„åˆ†: {traditional_time:.2f}s, å»ºæ± : {pool_time:.2f}s)")
                else:
                    self.logger.warning("âš ï¸ æ— æœ‰æ•ˆè¯„åˆ†æ•°æ®")
                    return False

            if result and all(not df.empty for df in result.values()):
                self.logger.info("âœ… è‚¡ç¥¨æ± æ„å»ºæˆåŠŸ")
                return True
            else:
                self.logger.error("âŒ è‚¡ç¥¨æ± æ„å»ºå¤±è´¥")
                return False

        except Exception as e:
            self.logger.error(f"âŒ æ¯æ—¥åŒæ­¥å¤±è´¥: {e}")
            return False

    @staticmethod
    def _calculate_single_stock_indicators(task):
        """
        å¹¶è¡Œè®¡ç®—å•ä¸ªè‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡
        
        Args:
            task: (stock_code, stock_info, price_df) å…ƒç»„
            
        Returns:
            åŒ…å«è‚¡ç¥¨ä¿¡æ¯å’ŒæŠ€æœ¯æŒ‡æ ‡çš„å­—å…¸
        """
        stock_code, stock_info, price_df = task
        
        try:
            # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æŒ‡æ ‡å¼•æ“å®ä¾‹æ¥è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¸€ä¸ªç‹¬ç«‹çš„è®¡ç®—ç¯å¢ƒï¼Œé¿å…å…±äº«çŠ¶æ€é—®é¢˜
            
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            import pandas as pd
            import numpy as np
            import talib
            
            # ç®€åŒ–çš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ï¼ˆé¿å…å¤æ‚çš„ä¾èµ–ï¼‰
            indicators_result = {}
            latest_values = {}
            
            if price_df is not None and not price_df.empty:
                # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
                if 'close' not in price_df.columns:
                    return None
                    
                close_prices = price_df['close'].values
                
                # è®¡ç®—RSI
                if len(close_prices) >= 14:
                    rsi = talib.RSI(close_prices, timeperiod=14)
                    latest_values['RSI_14'] = rsi[-1] if not np.isnan(rsi[-1]) else 50.0
                else:
                    latest_values['RSI_14'] = 50.0
                
                # å½“å‰ä»·æ ¼
                latest_values['current_price'] = close_prices[-1]
                
                # è®¡ç®—æ³¢åŠ¨ç‡
                if len(close_prices) >= 20:
                    returns = np.diff(np.log(close_prices))
                    volatility = np.std(returns[-20:]) * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
                    latest_values['volatility_20d'] = volatility if not np.isnan(volatility) else 0.02
                else:
                    latest_values['volatility_20d'] = 0.02
                
                # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆç®€åŒ–çš„è®¡ç®—ï¼‰
                if 'volume' in price_df.columns and len(price_df) >= 5:
                    recent_volume = price_df['volume'].tail(5).mean()
                    latest_values['avg_volume_5d'] = recent_volume if not np.isnan(recent_volume) else 100000
                    latest_values['turnover_rate'] = recent_volume / 1000000  # ç®€åŒ–çš„æ¢æ‰‹ç‡
                else:
                    latest_values['avg_volume_5d'] = 100000
                    latest_values['turnover_rate'] = 2.0
            
            indicators_result['latest_values'] = latest_values
            indicators_result['full_series'] = {}  # ç®€åŒ–çš„å®ç°
            indicators_result['errors'] = []
            
            return {
                'stock_info': stock_info,
                'technical_indicators': indicators_result
            }
            
        except Exception as e:
            # åœ¨å¹¶è¡Œç¯å¢ƒä¸­è®°å½•é”™è¯¯
            print(f"âš ï¸ å¹¶è¡Œè®¡ç®—è‚¡ç¥¨ {stock_code} æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            return None

    def get_sync_status(self) -> Dict:
        """
        è·å–åŒæ­¥çŠ¶æ€

        Returns:
            åŒ…å«åŒæ­¥çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
        """
        return {
            'is_syncing': self.is_syncing,
            'last_sync_time': self.last_sync_time,
            'data_date': date.today().isoformat(),
            'basic_pool_exists': self.basic_pool_file.exists(),
            'watch_pool_exists': self.watch_pool_file.exists(),
            'core_pool_exists': self.core_pool_file.exists(),
            'basic_pool_count': self._get_file_count(self.basic_pool_file),
            'watch_pool_count': self._get_file_count(self.watch_pool_file),
            'core_pool_count': self._get_file_count(self.core_pool_file)
        }

    def _get_file_count(self, file_path) -> int:
        """
        è·å–æ–‡ä»¶ä¸­çš„è‚¡ç¥¨æ•°é‡

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            è‚¡ç¥¨æ•°é‡
        """
        try:
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'stocks' in data:
                        return len(data['stocks'])
                    elif isinstance(data, list):
                        return len(data)
            return 0
        except:
            return 0

    def load_all_pools(self, return_dict: bool = False) -> Union[Dict[str, pd.DataFrame], Dict[str, List[Dict]]]:
        """
        åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ± æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬

        é»˜è®¤è¿”å›DataFrameæ ¼å¼ä»¥å‡å°‘è½¬æ¢å¼€é”€ï¼Œå¯é€‰æ‹©è¿”å›å­—å…¸æ ¼å¼ä»¥ä¿æŒå‘åå…¼å®¹æ€§

        Args:
            return_dict: æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œé»˜è®¤ä¸ºFalseè¿”å›DataFrame

        Returns:
            åŒ…å«æ‰€æœ‰å±‚çº§è‚¡ç¥¨æ± çš„å­—å…¸ï¼Œæ ¼å¼ç”±return_dictå‚æ•°å†³å®š
        """
        try:
            result = {}

            # åŠ è½½åŸºç¡€å±‚
            basic_data = self.data_store.load_pool('basic')
            if not basic_data.empty:
                result['basic_layer'] = basic_data

            # åŠ è½½è§‚å¯Ÿå±‚
            watch_data = self.data_store.load_pool('watch')
            if not watch_data.empty:
                result['watch_layer'] = watch_data

            # åŠ è½½æ ¸å¿ƒå±‚
            core_data = self.data_store.load_pool('core')
            if not core_data.empty:
                result['core_layer'] = core_data

            # å¦‚æœéœ€è¦å­—å…¸æ ¼å¼ï¼Œè¿›è¡Œè½¬æ¢ï¼ˆåªåœ¨éœ€è¦æ—¶è¿›è¡Œï¼‰
            if return_dict:
                dict_result = {}
                for key, df in result.items():
                    if not df.empty:
                        # ä½¿ç”¨to_dict('records')è¿›è¡Œé«˜æ•ˆè½¬æ¢
                        dict_result[key] = df.to_dict('records')
                    else:
                        dict_result[key] = []
                return dict_result

            return result

        except Exception as e:
            self.logger.error(f"åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ± å¤±è´¥: {e}")
            if return_dict:
                return {'basic_layer': [], 'watch_layer': [], 'core_layer': []}
            else:
                return {
                    'basic_layer': pd.DataFrame(),
                    'watch_layer': pd.DataFrame(),
                    'core_layer': pd.DataFrame()
                }

    def save_basic_pool(self, pool_data: List[Dict]) -> bool:
        """
        ä¿å­˜åŸºç¡€è‚¡ç¥¨æ±  (ä½¿ç”¨datastore)

        Args:
            pool_data: è¦ä¿å­˜çš„è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # è®¾ç½®å†…éƒ¨å®ä¾‹å¹¶ä¿å­˜
        self.data_store.basic_pool = pd.DataFrame(pool_data)
        return self.data_store.save_basic_pool()

    def save_watch_pool(self, pool_data: List[Dict]) -> bool:
        """
        ä¿å­˜è§‚å¯Ÿè‚¡ç¥¨æ±  (ä½¿ç”¨datastore)

        Args:
            pool_data: è¦ä¿å­˜çš„è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # è®¾ç½®å†…éƒ¨å®ä¾‹å¹¶ä¿å­˜
        self.data_store.watch_pool = pd.DataFrame(pool_data)
        return self.data_store.save_watch_pool()

    def save_core_pool(self, pool_data: List[Dict]) -> bool:
        """
        ä¿å­˜æ ¸å¿ƒè‚¡ç¥¨æ±  (ä½¿ç”¨datastore)

        Args:
            pool_data: è¦ä¿å­˜çš„è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # è®¾ç½®å†…éƒ¨å®ä¾‹å¹¶ä¿å­˜
        self.data_store.core_pool = pd.DataFrame(pool_data)
        return self.data_store.save_core_pool()

    def save_pool(self, pool_type: str, pool_data: List[Dict]) -> bool:
        """
        ä¿å­˜æŒ‡å®šç±»å‹çš„è‚¡ç¥¨æ±  (ä½¿ç”¨datastore)

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')
            pool_data: è¦ä¿å­˜çš„è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # è®¾ç½®å†…éƒ¨å®ä¾‹å¹¶ä¿å­˜
        df_data = pd.DataFrame(pool_data)
        if pool_type == 'basic':
            self.data_store.basic_pool = df_data
        elif pool_type == 'watch':
            self.data_store.watch_pool = df_data
        elif pool_type == 'core':
            self.data_store.core_pool = df_data

        return self.data_store.save_pool_by_name(pool_type)

    def _extract_single_stock_from_batch_local(self, batch_data: Union[pd.DataFrame, pd.Series], stock_code: str, date: str) -> Optional[pd.DataFrame]:
        """
        ä»æ‰¹é‡æ•°æ®ä¸­æå–å•ä¸ªè‚¡ç¥¨çš„æ•°æ®ï¼ˆæœ¬åœ°å®ç°ï¼‰

        Args:
            batch_data: æ‰¹é‡æ•°æ®
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸ

        Returns:
            DataFrame: å•ä¸ªè‚¡ç¥¨çš„æ•°æ®
        """
        try:
            # æ‰¹é‡æ•°æ®çš„ç´¢å¼•æ˜¯ (è‚¡ç¥¨ä»£ç , æ—¥æœŸ) çš„å¤šé‡ç´¢å¼•
            if isinstance(batch_data.index, pd.MultiIndex):
                # æŸ¥æ‰¾å¯¹åº”çš„è‚¡ç¥¨å’Œæ—¥æœŸ
                mask = (batch_data.index.get_level_values(0) == stock_code)
                if date:
                    date_obj = pd.to_datetime(date)
                    mask = mask & (batch_data.index.get_level_values(1) == date_obj)

                if mask.any():
                    single_stock_data = batch_data[mask].copy()
                    # é‡ç½®ç´¢å¼•ï¼Œç§»é™¤å¤šé‡ç´¢å¼•
                    single_stock_data = single_stock_data.reset_index()
                    # é‡å‘½åç´¢å¼•åˆ—ä¸ºæ›´æœ‰æ„ä¹‰çš„åç§°
                    single_stock_data = single_stock_data.rename(columns={'level_0': 'stock_code', 'level_1': 'date'})
                    # åªä¿ç•™å› å­åˆ—å’Œå¿…è¦çš„æ ‡è¯†åˆ—
                    factor_columns = [col for col in single_stock_data.columns if col not in ['stock_code', 'date']]
                    if factor_columns:
                        single_stock_data = single_stock_data[factor_columns]
                    return single_stock_data

            return None

        except Exception as e:
            self.logger.error(f"âŒ ä»æ‰¹é‡æ•°æ®ä¸­æå–å•ä¸ªè‚¡ç¥¨å¤±è´¥ {stock_code}: {e}")
            return None


# ============================================================================
# æ•°æ®ç¼“å­˜ç®¡ç†å™¨ - ç»Ÿä¸€å¤„ç†ç¼“å­˜æ–‡ä»¶æ“ä½œ
# ============================================================================

class CacheManager:
    """
    ç¼“å­˜ç®¡ç†å™¨ - è´Ÿè´£ç¼“å­˜æ–‡ä»¶çš„åŠ è½½ã€éªŒè¯ã€ä¿å­˜æ“ä½œ

    æ ¸å¿ƒåŠŸèƒ½:
    - ç¼“å­˜æ–‡ä»¶åŠ è½½å’ŒéªŒè¯
    - ç¼“å­˜è¿‡æœŸæ£€æŸ¥
    - ç¼“å­˜æ•°æ®ä¿å­˜
    - ç¼“å­˜ä¸€è‡´æ€§ä¿è¯
    """

    def __init__(self, data_store, logger):
        self.data_store = data_store
        self.logger = logger

    def load_cache_with_validation(self, cache_filename: str, target_date: str,
                                 data_type: str) -> Optional[Dict]:
        """
        åŠ è½½å¹¶éªŒè¯ç¼“å­˜æ•°æ®

        Args:
            cache_filename: ç¼“å­˜æ–‡ä»¶å
            target_date: ç›®æ ‡æ—¥æœŸ
            data_type: æ•°æ®ç±»å‹æè¿°

        Returns:
            Optional[Dict]: éªŒè¯é€šè¿‡çš„ç¼“å­˜æ•°æ®æˆ–None
        """
        try:
            # å°è¯•ä»æ–‡ä»¶ç¼“å­˜åŠ è½½
            cached_data = self.data_store.load_data_from_file(cache_filename)
            if cached_data is None:
                self.logger.info(f"ğŸ“Š {data_type}ç¼“å­˜ä¸å­˜åœ¨: {cache_filename}")
                return None

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡24å°æ—¶ï¼‰
            fetch_time = cached_data.get('fetch_time')
            cache_valid = True

            if fetch_time:
                fetch_datetime = datetime.fromisoformat(fetch_time)
                if (datetime.now() - fetch_datetime).total_seconds() > 24 * 3600:
                    cache_valid = False
                    self.logger.info(f"âš ï¸ {data_type}ç¼“å­˜å·²è¿‡æœŸ: {cache_filename}")
                # ç§»é™¤é‡å¤çš„æˆåŠŸæ—¥å¿—ï¼Œè¿™é‡Œåªåœ¨ç¼“å­˜æœ‰æ•ˆæ—¶è®°å½•ä¸€æ¬¡
            else:
                # ç§»é™¤é‡å¤çš„æˆåŠŸæ—¥å¿—ï¼Œè¿™é‡Œåªåœ¨æ²¡æœ‰æ—¶é—´æˆ³æ—¶è®°å½•ä¸€æ¬¡
                pass

            # ç»Ÿä¸€åœ¨è¿™é‡Œè®°å½•ç¼“å­˜åŠ è½½æˆåŠŸçš„æ—¥å¿—
            if cache_valid:
                self.logger.info(f"âœ… {data_type}ç¼“å­˜éªŒè¯é€šè¿‡: {cache_filename}")

            # æ£€æŸ¥äº¤æ˜“æ—¥æ˜¯å¦åŒ¹é…
            cached_trading_date = cached_data.get('trading_date')
            if cached_trading_date and cached_trading_date != target_date:
                cache_valid = False
                self.logger.info(f"âš ï¸ {data_type}ç¼“å­˜äº¤æ˜“æ—¥ä¸åŒ¹é…: {cached_trading_date} vs {target_date}")

            return cached_data if cache_valid else None

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½{data_type}ç¼“å­˜å¤±è´¥: {e}")
            return None

    def save_cache_data(self, data, cache_filename: str, data_type: str,
                       target_date: Optional[str] = None) -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            cache_filename: ç¼“å­˜æ–‡ä»¶å
            data_type: æ•°æ®ç±»å‹æè¿°
            target_date: ç›®æ ‡æ—¥æœŸï¼ˆå¯é€‰ï¼‰

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            if data is None or (hasattr(data, '__len__') and len(data) == 0):
                self.logger.warning(f"âš ï¸ {data_type}æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                return False

            # æ ¹æ®æ•°æ®ç±»å‹è°ƒç”¨ç›¸åº”çš„ä¿å­˜æ–¹æ³•
            if data_type == 'valuation' and hasattr(data, 'to_dict'):
                success = self.data_store.save_valuation_data_to_cache(data, target_date)
            elif data_type == 'price' and isinstance(data, dict):
                success = self.data_store._save_all_stocks_to_unified_cache(data, None, target_date)
            else:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
                return False

            if success:
                self.logger.info(f"ğŸ’¾ {data_type}æ•°æ®å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_filename}")
            else:
                self.logger.error(f"âŒ {data_type}æ•°æ®ä¿å­˜å¤±è´¥: {cache_filename}")

            return success

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜{data_type}æ•°æ®åˆ°ç¼“å­˜å¤±è´¥: {e}")
            return False


# ============================================================================
# æ•°æ®åŠ è½½å™¨ - ç»Ÿä¸€å¤„ç†æ•°æ®è·å–å’Œè¡¥å……
# ============================================================================

class DataLoader:
    """
    æ•°æ®åŠ è½½å™¨ - è´Ÿè´£æ•°æ®çš„è·å–ã€è¡¥å……ã€åˆå¹¶æ“ä½œ

    æ ¸å¿ƒåŠŸèƒ½:
    - ä»ç¼“å­˜åŠ è½½æ•°æ®
    - è¡¥å……ç¼ºå¤±æ•°æ®
    - åˆå¹¶ç¼“å­˜å’Œç½‘ç»œæ•°æ®
    - æ•°æ®å®Œæ•´æ€§éªŒè¯
    """

    def __init__(self, data_store, cache_manager, logger):
        self.data_store = data_store
        self.cache_manager = cache_manager
        self.logger = logger

    def load_valuation_data_with_fallback(self, stock_codes: List[str], target_date: str) -> pd.DataFrame:
        """
        åŠ è½½ä¼°å€¼æ•°æ®ï¼Œå¸¦ç¼“å­˜å’Œç½‘ç»œå›é€€æœºåˆ¶

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            pd.DataFrame: ä¼°å€¼æ•°æ®
        """
        cache_filename = f"{target_date}_valuation_data.json"

        # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
        cached_data = self.cache_manager.load_cache_with_validation(
            cache_filename, target_date, "ä¼°å€¼æ•°æ®"
        )

        if cached_data is not None:
            # ä»ç¼“å­˜é‡å»ºDataFrame
            valuation_df = self._rebuild_valuation_dataframe_from_cache(cached_data, cache_filename)
            if valuation_df is not None:
                return valuation_df

        # 2. ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œä»ç½‘ç»œè·å–
        return self._fetch_and_cache_valuation_data(stock_codes, target_date, cache_filename)

    def load_price_data_with_fallback(self, stock_codes: List[str], start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½ä»·æ ¼æ•°æ®ï¼Œå¸¦ç¼“å­˜å’Œç½‘ç»œå›é€€æœºåˆ¶

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            Dict[str, pd.DataFrame]: ä»·æ ¼æ•°æ®å­—å…¸
        """
        cache_filename = f"{end_date}_kline_data.json"

        # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
        cached_data = self.cache_manager.load_cache_with_validation(
            cache_filename, end_date, "Kçº¿æ•°æ®"
        )

        price_data = {}
        if cached_data is not None:
            # ä»ç¼“å­˜é‡å»ºprice_data
            price_data = self._rebuild_price_data_from_cache(cached_data, cache_filename)

        # 2. å¦‚æœç¼“å­˜ä¸­è‚¡ç¥¨æ•°é‡ä¸è¶³ï¼Œä»ç½‘ç»œè·å–å‰©ä½™æ•°æ®
        return self._fetch_and_merge_price_data(stock_codes, start_date, end_date, cache_filename, price_data)

    def _rebuild_valuation_dataframe_from_cache(self, cached_data: Dict, cache_filename: str) -> Optional[pd.DataFrame]:
        """
        ä»ç¼“å­˜æ•°æ®é‡å»ºä¼°å€¼DataFrame

        Args:
            cached_data: ç¼“å­˜æ•°æ®å­—å…¸
            cache_filename: ç¼“å­˜æ–‡ä»¶å

        Returns:
            Optional[pd.DataFrame]: é‡å»ºçš„DataFrameæˆ–None
        """
        try:
            records = cached_data.get('valuation_data', [])
            if records:
                valuation_df = pd.DataFrame(records)
                self.logger.info(f"âœ… ä¼°å€¼æ•°æ®åŠ è½½å®Œæˆ: {len(valuation_df)} åªè‚¡ç¥¨")
                return valuation_df
            else:
                self.logger.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æ ¼å¼é”™è¯¯: {cache_filename}")
                return None
        except Exception as e:
            self.logger.error(f"âŒ é‡å»ºä¼°å€¼DataFrameå¤±è´¥: {e}")
            return None

    def _rebuild_price_data_from_cache(self, cached_data: Dict, cache_filename: str) -> Dict[str, pd.DataFrame]:
        """
        ä»ç¼“å­˜æ•°æ®é‡å»ºä»·æ ¼æ•°æ®å­—å…¸

        Args:
            cached_data: ç¼“å­˜æ•°æ®å­—å…¸
            cache_filename: ç¼“å­˜æ–‡ä»¶å

        Returns:
            Dict[str, pd.DataFrame]: é‡å»ºçš„ä»·æ ¼æ•°æ®å­—å…¸
        """
        price_data = {}
        try:
            stocks_data = cached_data.get('stocks', {})
            for stock_code, stock_data in stocks_data.items():
                try:
                    records = stock_data.get('data', [])
                    if records:
                        df = pd.DataFrame(records)
                        if not df.empty and 'date' in df.columns:
                            df['date'] = pd.to_datetime(df['date'])
                            df = df.set_index('date')
                            price_data[stock_code] = df
                except Exception as e:
                    self.logger.warning(f"âš ï¸ è§£æç¼“å­˜æ•°æ®å¤±è´¥ {stock_code}: {e}")

            self.logger.info(f"âœ… Kçº¿æ•°æ®åŠ è½½å®Œæˆ: {len(price_data)} åªè‚¡ç¥¨")
        except Exception as e:
            self.logger.error(f"âŒ é‡å»ºä»·æ ¼æ•°æ®å¤±è´¥: {e}")

        return price_data

    def _fetch_and_cache_valuation_data(self, stock_codes: List[str], target_date: str, cache_filename: str) -> pd.DataFrame:
        """
        ä»ç½‘ç»œè·å–ä¼°å€¼æ•°æ®å¹¶ä¿å­˜åˆ°ç¼“å­˜

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ
            cache_filename: ç¼“å­˜æ–‡ä»¶å

        Returns:
            pd.DataFrame: è·å–çš„ä¼°å€¼æ•°æ®
        """
        try:
            self.logger.info(f"ğŸ“Š ä¼°å€¼æ•°æ®ç¼“å­˜æ— æ•ˆï¼Œä»ç½‘ç»œè·å–: {cache_filename}")
            valuation_data = self._batch_fetch_valuation_data(stock_codes, target_date, return_dataframe=True)

            # å¤„ç†è¿”å›çš„æ•°æ®å¹¶ä¿å­˜åˆ°ç¼“å­˜
            if isinstance(valuation_data, pd.DataFrame) and not valuation_data.empty:
                self.cache_manager.save_cache_data(valuation_data, cache_filename, 'valuation', target_date)
                return valuation_data
            elif isinstance(valuation_data, list) and valuation_data:
                valuation_df = pd.DataFrame(valuation_data)
                if not valuation_df.empty:
                    self.cache_manager.save_cache_data(valuation_df, cache_filename, 'valuation', target_date)
                    return valuation_df

            self.logger.warning("âš ï¸ æ— æ³•è·å–ä¼°å€¼æ•°æ®")
            return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"âŒ è·å–ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def _fetch_and_merge_price_data(self, stock_codes: List[str], start_date: str, end_date: str,
                                   cache_filename: str, existing_price_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """
        è·å–ä»·æ ¼æ•°æ®å¹¶ä¸ç°æœ‰æ•°æ®åˆå¹¶

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            cache_filename: ç¼“å­˜æ–‡ä»¶å
            existing_price_data: å·²å­˜åœ¨çš„ä»·æ ¼æ•°æ®

        Returns:
            Dict[str, pd.DataFrame]: åˆå¹¶åçš„ä»·æ ¼æ•°æ®
        """
        price_data = existing_price_data.copy()

        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»ç½‘ç»œè·å–æ•°æ®
            if len(price_data) < len(stock_codes):
                missing_count = len(stock_codes) - len(price_data)
                self.logger.info(f"ğŸ“Š ä»ç½‘ç»œè·å– {missing_count} åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®...")

                network_price_data = self._batch_fetch_price_data(stock_codes, start_date, end_date)

                # åˆå¹¶ç¼“å­˜å’Œç½‘ç»œæ•°æ®
                for stock_code, df in network_price_data.items():
                    if stock_code not in price_data and df is not None and not df.empty:
                        price_data[stock_code] = df

                # ä¿å­˜åˆå¹¶åçš„æ•°æ®åˆ°ç¼“å­˜
                if price_data:
                    self.cache_manager.save_cache_data(price_data, cache_filename, 'price', end_date)

            return price_data

        except Exception as e:
            self.logger.error(f"âŒ è·å–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return price_data

    def _batch_fetch_valuation_data(self, stock_codes: List[str], target_date: str,
                                 return_dataframe: bool = False) -> Union[List[Dict], pd.DataFrame]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨ä¼°å€¼æ•°æ®

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ
            return_dataframe: æ˜¯å¦è¿”å›DataFrameæ ¼å¼

        Returns:
            ä¼°å€¼æ•°æ®åˆ—è¡¨æˆ–DataFrame
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                self.logger.warning("âš ï¸ RQDatacä¸å¯ç”¨")
                return [] if not return_dataframe else pd.DataFrame()

            # ä¼°å€¼å› å­åˆ—è¡¨
            valuation_factors = ['pe_ratio', 'pb_ratio', 'ps_ratio', 'pcf_ratio', 'market_cap', 'turnover_ratio']

            self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è·å–ä¼°å€¼æ•°æ®: {len(stock_codes)} åªè‚¡ç¥¨ @ {target_date}")

            # ç›´æ¥ä½¿ç”¨æ‰¹é‡APIè·å–æ‰€æœ‰è‚¡ç¥¨çš„ä¼°å€¼æ•°æ®
            batch_result = rqdatac.get_factor(stock_codes, valuation_factors,
                                            start_date=target_date, end_date=target_date)

            if batch_result is None or (hasattr(batch_result, 'empty') and batch_result.empty):
                self.logger.warning("âš ï¸ æ‰¹é‡ä¼°å€¼è·å–å¤±è´¥: è¿”å›ç©ºç»“æœ")
                return [] if not return_dataframe else pd.DataFrame()

            self.logger.info(f"âœ… æ‰¹é‡ä¼°å€¼è·å–æˆåŠŸ: {batch_result.shape[0]} æ¡è®°å½•")

            if return_dataframe:
                # ç¡®ä¿è¿”å›DataFrameæ ¼å¼ï¼Œå¹¶æ·»åŠ stock_codeåˆ—
                if isinstance(batch_result, pd.Series):
                    df = batch_result.to_frame().T
                else:
                    df = batch_result.copy() if hasattr(batch_result, 'copy') else pd.DataFrame(batch_result)
                
                # å¤„ç†MultiIndexï¼Œç¡®ä¿æ·»åŠ stock_codeåˆ—
                if isinstance(df.index, pd.MultiIndex):
                    # é‡ç½®MultiIndexï¼Œå°†ç¬¬ä¸€çº§ç´¢å¼•ï¼ˆè‚¡ç¥¨ä»£ç ï¼‰ä½œä¸ºstock_codeåˆ—
                    df = df.reset_index()
                    if 'order_book_id' in df.columns:
                        df = df.rename(columns={'order_book_id': 'stock_code'})
                    elif df.index.names and df.index.names[0]:
                        df = df.rename(columns={df.index.names[0]: 'stock_code'})
                    else:
                        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ç´¢å¼•åï¼Œå‡è®¾ç¬¬ä¸€åˆ—æ˜¯è‚¡ç¥¨ä»£ç 
                        df['stock_code'] = df.iloc[:, 0]
                elif 'stock_code' not in df.columns and hasattr(df, 'index'):
                    # å¤„ç†å•ç´¢å¼•æƒ…å†µ
                    if df.index.name:
                        df = df.reset_index()
                        df = df.rename(columns={df.index.name: 'stock_code'})
                    else:
                        df['stock_code'] = df.index
                
                return df
            else:
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨æ ¼å¼
                if isinstance(batch_result, pd.Series):
                    result_dict = batch_result.to_dict()
                    result_dict['stock_code'] = batch_result.name
                    return [result_dict]
                
                # å¤„ç†MultiIndexï¼Œç¡®ä¿DataFrameæœ‰stock_codeåˆ—
                if isinstance(batch_result.index, pd.MultiIndex):
                    # é‡ç½®MultiIndexï¼Œå°†ç¬¬ä¸€çº§ç´¢å¼•ï¼ˆè‚¡ç¥¨ä»£ç ï¼‰ä½œä¸ºstock_codeåˆ—
                    df_with_code = batch_result.reset_index()
                    if 'order_book_id' in df_with_code.columns:
                        df_with_code = df_with_code.rename(columns={'order_book_id': 'stock_code'})
                    elif df_with_code.index.names and df_with_code.index.names[0]:
                        df_with_code = df_with_code.rename(columns={df_with_code.index.names[0]: 'stock_code'})
                    else:
                        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ç´¢å¼•åï¼Œæ‰‹åŠ¨æ·»åŠ stock_codeåˆ—
                        df_with_code['stock_code'] = df_with_code.index.get_level_values(0)
                elif hasattr(batch_result, 'reset_index'):
                    df_with_code = batch_result.reset_index()
                    if df_with_code.index.name:
                        df_with_code = df_with_code.rename(columns={df_with_code.index.name: 'stock_code'})
                    elif 'stock_code' not in df_with_code.columns:
                        df_with_code['stock_code'] = df_with_code.index
                else:
                    df_with_code = batch_result
                    
                return df_with_code.to_dict('records')

        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡è·å–ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
            return [] if not return_dataframe else pd.DataFrame()

    def _batch_fetch_price_data(self, stock_codes: List[str], start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            ä»·æ ¼æ•°æ®å­—å…¸ {stock_code: price_df}
        """
        try:
            price_data = {}

            if not RQDATAC_AVAILABLE or rqdatac is None:
                self.logger.warning("âš ï¸ RQDatacä¸å¯ç”¨")
                return price_data

            self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è·å–ä»·æ ¼æ•°æ®: {len(stock_codes)} åªè‚¡ç¥¨")

            # ä½¿ç”¨RQDatacçš„æ‰¹é‡APIç›´æ¥è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
            batch_start_time = time.time()
            batch_data = rqdatac.get_price(
                stock_codes,
                start_date=start_date,
                end_date=end_date,
                frequency='1d',
                fields=['open', 'close', 'high', 'low', 'volume']
            )
            batch_time = time.time() - batch_start_time

            if batch_data is None or batch_data.empty:
                self.logger.warning("âš ï¸ æ‰¹é‡è·å–è¿”å›ç©ºæ•°æ®")
                return price_data

            self.logger.info(f"âœ… æ‰¹é‡APIè·å–å®Œæˆ: {len(batch_data)} æ¡è®°å½•, è€—æ—¶: {batch_time:.2f}ç§’")

            # å¤„ç†MultiIndex DataFrameï¼ŒæŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„
            process_start_time = time.time()

            # ç¡®ä¿æ•°æ®æ˜¯MultiIndexæ ¼å¼
            if isinstance(batch_data.index, pd.MultiIndex):
                # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„
                for stock_code in stock_codes:
                    try:
                        # ä»MultiIndexä¸­æå–å•åªè‚¡ç¥¨çš„æ•°æ®
                        if stock_code in batch_data.index.get_level_values(0):
                            stock_df = batch_data.loc[stock_code].copy()

                            # å¦‚æœæ˜¯å•è¡Œæ•°æ®ï¼Œéœ€è¦è½¬æ¢ä¸ºDataFrame
                            if isinstance(stock_df, pd.Series):
                                stock_df = stock_df.to_frame().T

                            # ç¡®ä¿dateåˆ—æ˜¯datetimeç±»å‹å¹¶è®¾ç½®ä¸ºç´¢å¼•
                            if 'date' in stock_df.columns:
                                stock_df['date'] = pd.to_datetime(stock_df['date'])
                                stock_df = stock_df.set_index('date')
                                price_data[stock_code] = stock_df
                            else:
                                # å¦‚æœæ²¡æœ‰dateåˆ—ï¼Œå°è¯•ä½¿ç”¨ç´¢å¼•
                                if isinstance(stock_df.index, pd.DatetimeIndex):
                                    price_data[stock_code] = stock_df
                                else:
                                    self.logger.warning(f"âš ï¸ {stock_code} æ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡")
                        else:
                            self.logger.debug(f"âš ï¸ {stock_code} ä¸åœ¨æ‰¹é‡æ•°æ®ä¸­")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ å¤„ç† {stock_code} æ•°æ®å¤±è´¥: {e}")
                        continue
            else:
                self.logger.warning("âš ï¸ æ‰¹é‡æ•°æ®ä¸æ˜¯é¢„æœŸçš„MultiIndexæ ¼å¼")
                # å›é€€åˆ°é€ä¸ªè·å–
                self.logger.info("ğŸ”„ å›é€€åˆ°é€ä¸ªè·å–æ¨¡å¼...")
                for stock_code in stock_codes:
                    try:
                        df = self.data_store.get_price(stock_code, start_date=start_date, end_date=end_date)
                        if df is not None and not df.empty:
                            price_data[stock_code] = df
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è·å– {stock_code} æ•°æ®å¤±è´¥: {e}")
                        continue

            process_time = time.time() - process_start_time
            self.logger.info(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(price_data)}/{len(stock_codes)} åªè‚¡ç¥¨, å¤„ç†è€—æ—¶: {process_time:.2f}ç§’")

            return price_data

        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡è·å–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            # å›é€€åˆ°é€ä¸ªè·å–
            self.logger.info("ğŸ”„ å›é€€åˆ°é€ä¸ªè·å–æ¨¡å¼...")
            for stock_code in stock_codes:
                try:
                    df = self.data_store.get_price(stock_code, start_date=start_date, end_date=end_date)
                    if df is not None and not df.empty:
                        price_data[stock_code] = df
                except Exception as e:
                    self.logger.warning(f"âš ï¸ è·å– {stock_code} æ•°æ®å¤±è´¥: {e}")
                    continue

            return price_data


# ============================================================================
# æ•°æ®ä¿å­˜å™¨ - ç»Ÿä¸€å¤„ç†æ•°æ®ä¿å­˜åˆ°datastore
# ============================================================================

class DataSaver:
    """
    æ•°æ®ä¿å­˜å™¨ - è´Ÿè´£å°†æ•°æ®ä¿å­˜åˆ°datastore

    æ ¸å¿ƒåŠŸèƒ½:
    - è‚¡ç¥¨æ± æ•°æ®ä¿å­˜
    - æ•°æ®å®Œæ•´æ€§éªŒè¯
    - ä¿å­˜çŠ¶æ€è·Ÿè¸ª
    - é”™è¯¯å¤„ç†å’Œæ¢å¤
    """

    def __init__(self, data_store, logger):
        self.data_store = data_store
        self.logger = logger

    def save_stock_pools(self, pools_data: Dict[str, pd.DataFrame]) -> Dict[str, bool]:
        """
        ä¿å­˜æ‰€æœ‰è‚¡ç¥¨æ± æ•°æ®åˆ°datastore

        Args:
            pools_data: è‚¡ç¥¨æ± æ•°æ®å­—å…¸
                {
                    'basic_pool': pd.DataFrame,
                    'watch_pool': pd.DataFrame,
                    'core_pool': pd.DataFrame
                }

        Returns:
            Dict[str, bool]: ä¿å­˜ç»“æœå­—å…¸
        """
        results = {}

        for pool_type, pool_data in pools_data.items():
            try:
                if pool_data is None or pool_data.empty:
                    self.logger.warning(f"âš ï¸ {pool_type}æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                    results[pool_type] = True
                    continue

                # è®¾ç½®datastoreä¸­çš„å¯¹åº”æ± æ•°æ®
                if pool_type == 'basic_pool':
                    self.data_store.basic_pool = pool_data
                    success = self.data_store.save_basic_pool()
                elif pool_type == 'watch_pool':
                    self.data_store.watch_pool = pool_data
                    success = self.data_store.save_watch_pool()
                elif pool_type == 'core_pool':
                    self.data_store.core_pool = pool_data
                    success = self.data_store.save_core_pool()
                else:
                    self.logger.error(f"âŒ ä¸æ”¯æŒçš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}")
                    results[pool_type] = False
                    continue

                if success:
                    self.logger.info(f"âœ… {pool_type}å·²ä¿å­˜: {len(pool_data)} åªè‚¡ç¥¨")
                else:
                    self.logger.error(f"âŒ {pool_type}ä¿å­˜å¤±è´¥")

                results[pool_type] = success

            except Exception as e:
                self.logger.error(f"âŒ ä¿å­˜{pool_type}å¤±è´¥: {e}")
                results[pool_type] = False

        return results

    def save_single_pool(self, pool_type: str, pool_data: pd.DataFrame) -> bool:
        """
        ä¿å­˜å•ä¸ªè‚¡ç¥¨æ± æ•°æ®

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')
            pool_data: è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            if pool_data is None or pool_data.empty:
                self.logger.warning(f"âš ï¸ {pool_type}æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                return True

            # è®¾ç½®datastoreä¸­çš„å¯¹åº”æ± æ•°æ®
            if pool_type == 'basic':
                self.data_store.basic_pool = pool_data
                return self.data_store.save_basic_pool()
            elif pool_type == 'watch':
                self.data_store.watch_pool = pool_data
                return self.data_store.save_watch_pool()
            elif pool_type == 'core':
                self.data_store.core_pool = pool_data
                return self.data_store.save_core_pool()
            else:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}")
                return False

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜{pool_type}å¤±è´¥: {e}")
            return False

# ============================================================================
# STOCK POOL DATA STORE - ç‹¬ç«‹æ•°æ®å­˜å‚¨å±‚
# ============================================================================

class StockPoolDataStore:
    """
    StockPoolä¸“ç”¨æ•°æ®å­˜å‚¨å™¨

    æ ¸å¿ƒåŠŸèƒ½:
    - ç‹¬ç«‹æ•°æ®ç¼“å­˜: é¿å…ä¸å…¶ä»–æ¨¡å—çš„æ–‡ä»¶æ“ä½œå†²çª
    - æ™ºèƒ½äº¤æ˜“æ—¥ç¡®å®š: åŸºäºäº¤æ˜“æ—¶æ®µçš„å…¨å±€æ—¥æœŸåˆ¤æ–­
    - ä¸‰å±‚è‚¡ç¥¨æ± ç®¡ç†: åŸºç¡€/è§‚å¯Ÿ/æ ¸å¿ƒæ± çš„å†…å­˜å’ŒæŒä¹…åŒ–ç®¡ç†
    - æ‰¹é‡æ•°æ®è·å–: ä¼˜åŒ–çš„RQDatacæ•°æ®è·å–æ¥å£

    è®¾è®¡ä¼˜åŠ¿:
    - è½»é‡çº§æ¶æ„: ä¸“æ³¨äºè‚¡ç¥¨æ± æ•°æ®çš„å­˜å‚¨å’Œè®¿é—®
    - é«˜æ€§èƒ½ç¼“å­˜: å†…å­˜ç¼“å­˜ + æ–‡ä»¶æŒä¹…åŒ–åŒé‡ä¿éšœ
    - æ™ºèƒ½é”™è¯¯å¤„ç†: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé™çº§ç­–ç•¥
    - æ¨¡å—åŒ–è®¾è®¡: ä¸å…¶ä»–ç»„ä»¶è§£è€¦ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•

    æ•°æ®æµ:
    RQDatac â†’ æ‰¹é‡ç¼“å­˜ â†’ å†…å­˜å®ä¾‹ â†’ æ–‡ä»¶æŒä¹…åŒ–
    """

    def __init__(self, cache_dir: str = "stockpool_cache"):
        """
        åˆå§‹åŒ–æ•°æ®å­˜å‚¨å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œç”¨äºå­˜å‚¨ä¸´æ—¶æ•°æ®æ–‡ä»¶

        Raises:
            OSError: å½“æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½•æ—¶æŠ›å‡º
        """
        logger.debug(f"ğŸ”§ åˆå§‹åŒ– StockPoolDataStore, ç¼“å­˜ç›®å½•: {cache_dir}")

        self.cache_dir = cache_dir
        self.data_dir = "stockpool_data"

        # åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
        try:
            os.makedirs(self.cache_dir, exist_ok=True)
            os.makedirs(self.data_dir, exist_ok=True)
            logger.debug(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {self.cache_dir}, æ•°æ®ç›®å½•: {self.data_dir}")
        except OSError as e:
            logger.error(f"âŒ æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½•: {e}")
            raise

        # åˆå§‹åŒ–æ•°æ®ç¼“å­˜ç»“æ„
        self._init_cache_structures()

        # åˆå§‹åŒ–è‚¡ç¥¨æ± å®ä¾‹
        self._init_pool_instances()

        # é¢„å…ˆç¡®å®šç›®æ ‡äº¤æ˜“æ—¥ï¼ˆå…¨å±€ä¼˜åŒ–ï¼‰
        self._determine_target_trading_date()

        logger.info(f"âœ… StockPoolDataStore åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“… é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date}")
        logger.debug(f"ğŸ—ï¸ è‚¡ç¥¨æ± å®ä¾‹çŠ¶æ€: basic={not self.basic_pool.empty}, watch={not self.watch_pool.empty}, core={not self.core_pool.empty}")

    def _init_cache_structures(self):
        """åˆå§‹åŒ–ç¼“å­˜æ•°æ®ç»“æ„"""
        logger.debug("ğŸ”§ åˆå§‹åŒ–ç¼“å­˜æ•°æ®ç»“æ„")

        # è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜
        self.stock_list_cache: Optional[pd.DataFrame] = None
        self.stock_list_timestamp: Optional[float] = None

        # Kçº¿æ•°æ®ç¼“å­˜ - {stock_code: DataFrame}
        self.kline_cache: Dict[str, pd.DataFrame] = {}

        # æ‰¹é‡æ•°æ®ç¼“å­˜ - {cache_key: DataFrame}
        self.batch_cache: Dict[str, pd.DataFrame] = {}

        # ç¼“å­˜è®¿é—®æ—¶é—´è·Ÿè¸ªï¼ˆç”¨äºLRUæ·˜æ±°ï¼‰
        self.cache_access_times: Dict[str, float] = {}

        # ç¼“å­˜å¤§å°é™åˆ¶
        self.max_cache_size = int(os.getenv('MAX_CACHE_SIZE', '1000'))  # é»˜è®¤1000ä¸ªç¼“å­˜é¡¹

        # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        self.cache_expiry_seconds = int(os.getenv('CACHE_EXPIRY_SECONDS', '3600'))  # é»˜è®¤1å°æ—¶

        logger.debug(f"âœ… ç¼“å­˜ç»“æ„åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§ç¼“å­˜å¤§å°: {self.max_cache_size}ï¼Œè¿‡æœŸæ—¶é—´: {self.cache_expiry_seconds}ç§’")

    def _manage_cache_size(self):
        """ç®¡ç†ç¼“å­˜å¤§å°ï¼Œå®æ–½LRUæ·˜æ±°ç­–ç•¥"""
        total_cache_items = len(self.kline_cache) + len(self.batch_cache)

        if total_cache_items > self.max_cache_size:
            # è®¡ç®—éœ€è¦æ·˜æ±°çš„æ•°é‡
            items_to_remove = total_cache_items - self.max_cache_size

            # æ”¶é›†æ‰€æœ‰ç¼“å­˜é¡¹çš„è®¿é—®æ—¶é—´
            all_cache_items = []
            for cache_key in self.kline_cache.keys():
                access_time = self.cache_access_times.get(f"kline_{cache_key}", 0)
                all_cache_items.append((access_time, 'kline', cache_key))

            for cache_key in self.batch_cache.keys():
                access_time = self.cache_access_times.get(f"batch_{cache_key}", 0)
                all_cache_items.append((access_time, 'batch', cache_key))

            # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼ˆæœ€å°‘è®¿é—®çš„åœ¨å‰ï¼‰
            all_cache_items.sort(key=lambda x: x[0])

            # æ·˜æ±°æœ€å°‘è®¿é—®çš„ç¼“å­˜é¡¹
            for i in range(min(items_to_remove, len(all_cache_items))):
                _, cache_type, cache_key = all_cache_items[i]
                if cache_type == 'kline':
                    del self.kline_cache[cache_key]
                    del self.cache_access_times[f"kline_{cache_key}"]
                elif cache_type == 'batch':
                    del self.batch_cache[cache_key]
                    del self.cache_access_times[f"batch_{cache_key}"]

            logger.debug(f"ğŸ—‘ï¸ ç¼“å­˜æ¸…ç†å®Œæˆï¼Œæ·˜æ±°äº† {items_to_remove} ä¸ªç¼“å­˜é¡¹")

    def _is_cache_expired(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        access_time = self.cache_access_times.get(cache_key, 0)
        if access_time == 0:
            return True

        current_time = time.time()
        return (current_time - access_time) > self.cache_expiry_seconds

    def _update_cache_access(self, cache_key: str):
        """æ›´æ–°ç¼“å­˜è®¿é—®æ—¶é—´"""
        self.cache_access_times[cache_key] = time.time()

    def get_cached_kline_data(self, stock_code: str) -> Optional[pd.DataFrame]:
        """è·å–ç¼“å­˜çš„Kçº¿æ•°æ®ï¼ˆå¸¦è¿‡æœŸæ£€æŸ¥ï¼‰"""
        cache_key = f"kline_{stock_code}"

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ
        if stock_code in self.kline_cache and not self._is_cache_expired(cache_key):
            self._update_cache_access(cache_key)
            return self.kline_cache[stock_code]
        elif stock_code in self.kline_cache:
            # ç¼“å­˜è¿‡æœŸï¼Œæ¸…ç†
            del self.kline_cache[stock_code]
            del self.cache_access_times[cache_key]

        return None

    def set_cached_kline_data(self, stock_code: str, data: pd.DataFrame):
        """è®¾ç½®ç¼“å­˜çš„Kçº¿æ•°æ®"""
        cache_key = f"kline_{stock_code}"

        self.kline_cache[stock_code] = data
        self._update_cache_access(cache_key)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†ç¼“å­˜
        self._manage_cache_size()

    def get_cached_batch_data(self, cache_key: str) -> Optional[pd.DataFrame]:
        """è·å–ç¼“å­˜çš„æ‰¹é‡æ•°æ®ï¼ˆå¸¦è¿‡æœŸæ£€æŸ¥ï¼‰"""
        full_key = f"batch_{cache_key}"

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ
        if cache_key in self.batch_cache and not self._is_cache_expired(full_key):
            self._update_cache_access(full_key)
            return self.batch_cache[cache_key]
        elif cache_key in self.batch_cache:
            # ç¼“å­˜è¿‡æœŸï¼Œæ¸…ç†
            del self.batch_cache[cache_key]
            del self.cache_access_times[full_key]

        return None

    def set_cached_batch_data(self, cache_key: str, data: pd.DataFrame):
        """è®¾ç½®ç¼“å­˜çš„æ‰¹é‡æ•°æ®"""
        full_key = f"batch_{cache_key}"

        self.batch_cache[cache_key] = data
        self._update_cache_access(full_key)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†ç¼“å­˜
        self._manage_cache_size()

    def _init_pool_instances(self):
        """åˆå§‹åŒ–è‚¡ç¥¨æ± å®ä¾‹"""
        logger.debug("ğŸ”§ åˆå§‹åŒ–è‚¡ç¥¨æ± å®ä¾‹")

        # ä¸‰å±‚è‚¡ç¥¨æ± çš„å†…å­˜å®ä¾‹
        self.basic_pool: pd.DataFrame = pd.DataFrame()      # åŸºç¡€è‚¡ç¥¨æ± 
        self.watch_pool: pd.DataFrame = pd.DataFrame()      # è§‚å¯Ÿè‚¡ç¥¨æ± 
        self.core_pool: pd.DataFrame = pd.DataFrame()       # æ ¸å¿ƒè‚¡ç¥¨æ± 

        # è‚¡ç¥¨æ± å…ƒæ•°æ®
        self.pool_metadata = {
            'basic_pool': {'last_updated': None, 'count': 0, 'date': None},
            'watch_pool': {'last_updated': None, 'count': 0, 'date': None},
            'core_pool': {'last_updated': None, 'count': 0, 'date': None}
        }

        # å…¨å±€äº¤æ˜“æ—¥ç¡®å®šå™¨
        self.target_trading_date: Optional[str] = None
        self.target_trading_date_determined = False

        logger.debug("âœ… è‚¡ç¥¨æ± å®ä¾‹åˆå§‹åŒ–å®Œæˆ")

    def load_data_from_file(self, filename: str, skip_logging: bool = False) -> Optional[Dict]:
        """
        ä»æ–‡ä»¶åŠ è½½æ•°æ®

        Args:
            filename: æ–‡ä»¶å
            skip_logging: æ˜¯å¦è·³è¿‡æ—¥å¿—è®°å½•ï¼ˆç”¨äºå†…å­˜ç¼“å­˜åœºæ™¯ï¼‰

        Returns:
            Dict: åŠ è½½çš„æ•°æ®æˆ–None
        """
        try:
            from pathlib import Path

            # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ç›®å½•
            search_dirs = [
                Path(self.data_dir),    # stockpool_data
                Path("data"),           # dataç›®å½•
                Path("../data")         # ç›¸å¯¹è·¯å¾„çš„dataç›®å½•
            ]

            for search_dir in search_dirs:
                filepath = search_dir / filename
                if filepath.exists():
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    if not skip_logging:
                        logger.debug(f"ğŸ“ ä» {search_dir} åŠ è½½æ•°æ®æ–‡ä»¶: {filename}")
                    return data

            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°
            if not skip_logging:
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return None

        except Exception as e:
            if not skip_logging:
                logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ {filename}: {e}")
            return None

    def save_data_to_file(self, data: Dict, filename: str, use_indent: bool = True) -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶å
            use_indent: æ˜¯å¦ä½¿ç”¨ç¼©è¿›æ ¼å¼ï¼ˆç”Ÿäº§ç¯å¢ƒå¯è®¾ä¸ºFalseä»¥æé«˜æ€§èƒ½ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            filepath = os.path.join(self.data_dir, filename)

            # åŸå­å†™å…¥æ“ä½œ
            temp_filepath = filepath + ".tmp"
            with open(temp_filepath, 'w', encoding='utf-8') as f:
                if use_indent:
                    json.dump(data, f, ensure_ascii=False, indent=2, default=str)
                else:
                    json.dump(data, f, ensure_ascii=False, separators=(',', ':'), default=str)

            # åŸå­ç§»åŠ¨
            if os.path.exists(filepath):
                os.remove(filepath)
            os.rename(temp_filepath, filepath)

            logger.info(f"ğŸ’¾ æ•°æ®ä¿å­˜æˆåŠŸ: {filename}")
            return True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥ {filename}: {e}")
            return False

    def save_pool(self, pool_type: str) -> bool:
        """
        æ ¹æ®ç±»å‹ä¿å­˜è‚¡ç¥¨æ± æ•°æ®åˆ°æ–‡ä»¶

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ

        Raises:
            ValueError: å½“pool_typeæ— æ•ˆæ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # æ ¹æ®pool_typeè·å–å¯¹åº”çš„æ± æ•°æ®å’Œé…ç½®
            if pool_type == 'basic':
                pool_data = self.basic_pool
                pool_name = 'åŸºç¡€è‚¡ç¥¨æ± '
                filename = 'basic_pool.json'
                metadata_key = 'basic_pool'
            elif pool_type == 'watch':
                pool_data = self.watch_pool
                pool_name = 'è§‚å¯Ÿè‚¡ç¥¨æ± '
                filename = 'watch_pool.json'
                metadata_key = 'watch_pool'
            elif pool_type == 'core':
                pool_data = self.core_pool
                pool_name = 'æ ¸å¿ƒè‚¡ç¥¨æ± '
                filename = 'core_pool.json'
                metadata_key = 'core_pool'
            else:
                raise ValueError(f"æ— æ•ˆçš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}ã€‚æ”¯æŒçš„ç±»å‹: 'basic', 'watch', 'core'")

            # æ£€æŸ¥æ± æ•°æ®æ˜¯å¦æœ‰æ•°æ®
            if pool_data.empty:
                logger.warning(f"âš ï¸ {pool_name}ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                return True

            # æ›´æ–°å…ƒæ•°æ®
            self.pool_metadata[metadata_key] = {
                'last_updated': datetime.now().isoformat(),
                'count': len(pool_data),
                'date': pool_data.iloc[0].get('date') if len(pool_data) > 0 else None
            }

            # ä¿å­˜åˆ°æ–‡ä»¶ - è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨æ ¼å¼
            data_list = pool_data.to_dict('records')
            data = {
                'type': f'{pool_type}_pool',
                'timestamp': datetime.now().isoformat(),
                'count': len(data_list),
                'data': data_list
            }

            # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç´§å‡‘æ ¼å¼ä»¥æé«˜æ€§èƒ½
            use_indent = not (
                os.getenv('ENV', '').lower() == 'production' or
                os.getenv('PRODUCTION', '').lower() in ('true', '1', 'yes') or
                not os.getenv('DEBUG', '').lower() in ('true', '1', 'yes')
            )
            success = self.save_data_to_file(data, filename, use_indent=use_indent)
            if success:
                logger.info(f"âœ… {pool_name}å·²ä¿å­˜: {len(pool_data)} åªè‚¡ç¥¨")
            return success

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜{pool_type}è‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False

    def save_basic_pool(self) -> bool:
        """
        ä¿å­˜åŸºç¡€è‚¡ç¥¨æ± æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self.save_pool('basic')

    def save_watch_pool(self) -> bool:
        """
        ä¿å­˜è§‚å¯Ÿè‚¡ç¥¨æ± æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self.save_pool('watch')

    def save_core_pool(self) -> bool:
        """
        ä¿å­˜æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self.save_pool('core')

    # ===== è‚¡ç¥¨æ± å®ä¾‹è®¿é—®æ–¹æ³• =====

    def get_pool(self, pool_type: str, copy: bool = True) -> pd.DataFrame:
        """
        æ ¹æ®ç±»å‹è·å–è‚¡ç¥¨æ± å®ä¾‹

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')
            copy: æ˜¯å¦è¿”å›æ‹·è´ï¼Œé»˜è®¤ä¸ºTrueä»¥ä¿æŒå‘åå…¼å®¹æ€§

        Returns:
            pd.DataFrame: æŒ‡å®šç±»å‹çš„è‚¡ç¥¨æ± æ•°æ®

        Raises:
            ValueError: å½“pool_typeæ— æ•ˆæ—¶æŠ›å‡ºå¼‚å¸¸
        """
        if pool_type == 'basic':
            if not self.basic_pool.empty:
                return self.basic_pool.copy() if copy else self.basic_pool
            else:
                return pd.DataFrame()
        elif pool_type == 'watch':
            if not self.watch_pool.empty:
                return self.watch_pool.copy() if copy else self.watch_pool
            else:
                return pd.DataFrame()
        elif pool_type == 'core':
            if not self.core_pool.empty:
                return self.core_pool.copy() if copy else self.core_pool
            else:
                return pd.DataFrame()
        else:
            raise ValueError(f"æ— æ•ˆçš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}ã€‚æ”¯æŒçš„ç±»å‹: 'basic', 'watch', 'core'")

    def get_basic_pool(self) -> pd.DataFrame:
        """
        è·å–åŸºç¡€è‚¡ç¥¨æ± å®ä¾‹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            pd.DataFrame: åŸºç¡€è‚¡ç¥¨æ± æ•°æ®
        """
        return self.get_pool('basic')

    def get_watch_pool(self) -> pd.DataFrame:
        """
        è·å–è§‚å¯Ÿè‚¡ç¥¨æ± å®ä¾‹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            pd.DataFrame: è§‚å¯Ÿè‚¡ç¥¨æ± æ•°æ®
        """
        return self.get_pool('watch')

    def get_core_pool(self) -> pd.DataFrame:
        """
        è·å–æ ¸å¿ƒè‚¡ç¥¨æ± å®ä¾‹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            pd.DataFrame: æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®
        """
        return self.get_pool('core')

    def get_all_pools(self) -> Dict[str, pd.DataFrame]:
        """
        è·å–æ‰€æœ‰è‚¡ç¥¨æ± å®ä¾‹

        Returns:
            Dict[str, pd.DataFrame]: åŒ…å«æ‰€æœ‰è‚¡ç¥¨æ± çš„å­—å…¸
        """
        return {
            'basic_pool': self.get_basic_pool(),
            'watch_pool': self.get_watch_pool(),
            'core_pool': self.get_core_pool()
        }

    def get_pool_info(self, pool_type: str) -> Dict:
        """
        è·å–æŒ‡å®šè‚¡ç¥¨æ± çš„ä¿¡æ¯

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            Dict: è‚¡ç¥¨æ± ä¿¡æ¯
        """
        if pool_type not in self.pool_metadata:
            return {'error': f'æœªçŸ¥çš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}'}

        info = self.pool_metadata[pool_type].copy()
        info['pool_type'] = pool_type

        # æ·»åŠ å®æ—¶æ•°æ®
        if pool_type == 'basic':
            info['current_count'] = len(self.basic_pool) if not self.basic_pool.empty else 0
        elif pool_type == 'watch':
            info['current_count'] = len(self.watch_pool) if not self.watch_pool.empty else 0
        elif pool_type == 'core':
            info['current_count'] = len(self.core_pool) if not self.core_pool.empty else 0

        return info

    def get_all_pool_info(self) -> Dict[str, Dict]:
        """
        è·å–æ‰€æœ‰è‚¡ç¥¨æ± çš„ä¿¡æ¯

        Returns:
            Dict[str, Dict]: æ‰€æœ‰è‚¡ç¥¨æ± çš„ä¿¡æ¯
        """
        return {
            'basic_pool': self.get_pool_info('basic'),
            'watch_pool': self.get_pool_info('watch'),
            'core_pool': self.get_pool_info('core')
        }

    def is_pool_available(self, pool_type: str) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šè‚¡ç¥¨æ± æ˜¯å¦å¯ç”¨

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            bool: è‚¡ç¥¨æ± æ˜¯å¦å¯ç”¨
        """
        if pool_type == 'basic':
            return not self.basic_pool.empty
        elif pool_type == 'watch':
            return not self.watch_pool.empty
        elif pool_type == 'core':
            return not self.core_pool.empty
        return False

    def retrieve_stock_codes_from_pool(self, pool_type: str) -> List[str]:
        """
        è·å–æŒ‡å®šè‚¡ç¥¨æ± ä¸­çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            List[str]: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        pool_data = pd.DataFrame()
        if pool_type == 'basic':
            pool_data = self.basic_pool
        elif pool_type == 'watch':
            pool_data = self.watch_pool
        elif pool_type == 'core':
            pool_data = self.core_pool

        if pool_data.empty:
            return []

        # ä»DataFrameä¸­æå–è‚¡ç¥¨ä»£ç åˆ—
        if 'stock_code' in pool_data.columns:
            return pool_data['stock_code'].dropna().tolist()
        else:
            return []

    def find_stock_in_pools_by_code(self, stock_code: str) -> pd.DataFrame:
        """
        åœ¨æ‰€æœ‰è‚¡ç¥¨æ± ä¸­æŸ¥æ‰¾æŒ‡å®šè‚¡ç¥¨

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 

        Returns:
            pd.DataFrame: åŒ…å«æ‰¾åˆ°çš„è‚¡ç¥¨ä¿¡æ¯ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€è¡Œï¼ŒåŒ…å«pool_typeåˆ—æ ‡è¯†æ¥æº
        """
        found_stocks = []

        # åœ¨åŸºç¡€æ± ä¸­æŸ¥æ‰¾
        if not self.basic_pool.empty and 'stock_code' in self.basic_pool.columns:
            mask = self.basic_pool['stock_code'] == stock_code
            if mask.any():
                stock_info = self.basic_pool.loc[mask].iloc[0].to_dict()
                stock_info['pool_type'] = 'basic'
                found_stocks.append(stock_info)

        # åœ¨è§‚å¯Ÿæ± ä¸­æŸ¥æ‰¾
        if not self.watch_pool.empty and 'stock_code' in self.watch_pool.columns:
            mask = self.watch_pool['stock_code'] == stock_code
            if mask.any():
                stock_info = self.watch_pool.loc[mask].iloc[0].to_dict()
                stock_info['pool_type'] = 'watch'
                found_stocks.append(stock_info)

        # åœ¨æ ¸å¿ƒæ± ä¸­æŸ¥æ‰¾
        if not self.core_pool.empty and 'stock_code' in self.core_pool.columns:
            mask = self.core_pool['stock_code'] == stock_code
            if mask.any():
                stock_info = self.core_pool.loc[mask].iloc[0].to_dict()
                stock_info['pool_type'] = 'core'
                found_stocks.append(stock_info)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è‚¡ç¥¨ï¼Œè¿”å›ç©ºDataFrame
        if not found_stocks:
            return pd.DataFrame()

        # å°†æ‰€æœ‰æ‰¾åˆ°çš„è‚¡ç¥¨ä¿¡æ¯åˆå¹¶æˆDataFrame
        result_df = pd.DataFrame(found_stocks)

        # é‡æ–°æ’åˆ—åˆ—ï¼Œå°†pool_typeåˆ—æ”¾åœ¨å‰é¢
        if 'pool_type' in result_df.columns:
            cols = ['pool_type'] + [col for col in result_df.columns if col != 'pool_type']
            result_df = result_df[cols]

        return result_df

    def clear_pool_instances(self) -> None:
        """
        æ¸…ç©ºæ‰€æœ‰è‚¡ç¥¨æ± å®ä¾‹ï¼ˆå†…å­˜ä¸­çš„æ•°æ®ï¼‰
        """
        self.basic_pool = pd.DataFrame()
        self.watch_pool = pd.DataFrame()
        self.core_pool = pd.DataFrame()

        # é‡ç½®å…ƒæ•°æ®
        for pool_type in self.pool_metadata:
            self.pool_metadata[pool_type] = {
                'last_updated': None,
                'count': 0,
                'date': None
            }

        logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰è‚¡ç¥¨æ± å®ä¾‹")

    def reload_pools_from_files(self) -> bool:
        """
        ä»æ–‡ä»¶é‡æ–°åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ± åˆ°å†…å­˜å®ä¾‹

        Returns:
            bool: é‡æ–°åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            success_count = 0

            # é‡æ–°åŠ è½½åŸºç¡€æ± 
            if self.load_basic_pool():
                success_count += 1

            # é‡æ–°åŠ è½½è§‚å¯Ÿæ± 
            if self.load_watch_pool():
                success_count += 1

            # é‡æ–°åŠ è½½æ ¸å¿ƒæ± 
            if self.load_core_pool():
                success_count += 1

            logger.info(f"âœ… å·²ä»æ–‡ä»¶é‡æ–°åŠ è½½ {success_count} ä¸ªè‚¡ç¥¨æ± åˆ°å†…å­˜å®ä¾‹")
            return success_count > 0

        except Exception as e:
            logger.error(f"âŒ ä»æ–‡ä»¶é‡æ–°åŠ è½½è‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False

    def load_pool(self, pool_type: str) -> pd.DataFrame:
        """
        åŠ è½½æŒ‡å®šç±»å‹çš„è‚¡ç¥¨æ± 

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            pd.DataFrame: è‚¡ç¥¨æ± æ•°æ®
        """
        try:
            from pathlib import Path

            # æ ¹æ®pool_typeç¡®å®šæ–‡ä»¶åå’Œæ—¥å¿—ä¿¡æ¯
            pool_configs = {
                'basic': {
                    'filename': 'basic_pool.json',
                    'display_name': 'åŸºç¡€è‚¡ç¥¨æ± ',
                    'emoji': 'ğŸ“‚'
                },
                'watch': {
                    'filename': 'watch_pool.json',
                    'display_name': 'è§‚å¯Ÿè‚¡ç¥¨æ± ',
                    'emoji': 'ğŸ‘€'
                },
                'core': {
                    'filename': 'core_pool.json',
                    'display_name': 'æ ¸å¿ƒè‚¡ç¥¨æ± ',
                    'emoji': 'â­'
                }
            }

            if pool_type not in pool_configs:
                logger.error(f"âŒ ä¸æ”¯æŒçš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}")
                return pd.DataFrame()

            config = pool_configs[pool_type]

            # ä¼˜å…ˆä»ä¿å­˜ç›®å½•åŠ è½½ï¼ˆstockpool_dataï¼‰ï¼Œç„¶åå°è¯•å…¶ä»–ç›®å½•
            search_paths = [
                Path(self.data_dir) / config['filename'],  # ä¼˜å…ˆï¼šstockpool_dataç›®å½•
                Path("data") / config['filename'],         # å¤‡é€‰ï¼šdataç›®å½•
                Path("../data") / config['filename']       # å¤‡é€‰ï¼šä¸Šçº§dataç›®å½•
            ]

            for pool_file in search_paths:
                if pool_file.exists():
                    with open(pool_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    # è§£æä¿å­˜çš„æ•°æ®æ ¼å¼ï¼ˆåªæ”¯æŒæ–°æ ¼å¼ï¼‰
                    if isinstance(data, dict) and 'data' in data:
                        # è¿™æ˜¯save_poolä¿å­˜çš„æ ¼å¼
                        stocks = data['data']
                        df = pd.DataFrame(stocks)
                        logger.info(f"{config['emoji']} åŠ è½½{config['display_name']}: {len(df)} åªè‚¡ç¥¨ (from {pool_file.parent})")
                        return df
                    # å¦‚æœæ•°æ®ç›´æ¥æ˜¯åˆ—è¡¨æ ¼å¼
                    elif isinstance(data, list):
                        df = pd.DataFrame(data)
                        logger.info(f"{config['emoji']} åŠ è½½{config['display_name']}: {len(df)} åªè‚¡ç¥¨ (from {pool_file.parent})")
                        return df
                    else:
                        logger.warning(f"âš ï¸ {config['display_name']}æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®: {pool_file}")
                        continue

            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°æ–‡ä»¶
            logger.warning(f"âš ï¸ {config['display_name']}æ–‡ä»¶ä¸å­˜åœ¨")
            return pd.DataFrame()

        except Exception as e:
            logger.error(f"âŒ åŠ è½½{config['display_name']}å¤±è´¥: {e}")
            return pd.DataFrame()
        
    def save_pool_by_name(self, pool_type: str) -> bool:
        """
        é€šç”¨ä¿å­˜è‚¡ç¥¨æ± æ–¹æ³•ï¼ˆä½¿ç”¨å†…éƒ¨å®ä¾‹ï¼‰

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self.save_pool(pool_type)

    def load_basic_pool(self) -> bool:
        """
        åŠ è½½åŸºç¡€è‚¡ç¥¨æ± æ•°æ®åˆ°å†…éƒ¨å®ä¾‹

        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            data = self.load_pool('basic')
            if not data.empty:
                self.basic_pool = data
                self.pool_metadata['basic_pool'] = {
                    'last_updated': datetime.now().isoformat(),
                    'count': len(data),
                    'date': data.iloc[0].get('date') if len(data) > 0 else None
                }
                logger.info(f"âœ… åŸºç¡€è‚¡ç¥¨æ± å·²åŠ è½½åˆ°å†…å­˜: {len(data)} åªè‚¡ç¥¨")
                return True
            else:
                logger.warning("âš ï¸ åŸºç¡€è‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
                return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åŸºç¡€è‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False

    def load_watch_pool(self) -> bool:
        """
        åŠ è½½è§‚å¯Ÿè‚¡ç¥¨æ± æ•°æ®åˆ°å†…éƒ¨å®ä¾‹

        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            data = self.load_pool('watch')
            if not data.empty:
                self.watch_pool = data
                self.pool_metadata['watch_pool'] = {
                    'last_updated': datetime.now().isoformat(),
                    'count': len(data),
                    'date': data.iloc[0].get('date') if len(data) > 0 else None
                }
                logger.info(f"âœ… è§‚å¯Ÿè‚¡ç¥¨æ± å·²åŠ è½½åˆ°å†…å­˜: {len(data)} åªè‚¡ç¥¨")
                return True
            else:
                logger.warning("âš ï¸ è§‚å¯Ÿè‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
                return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è§‚å¯Ÿè‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False

    def load_core_pool(self) -> bool:
        """
        åŠ è½½æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®åˆ°å†…éƒ¨å®ä¾‹

        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            data = self.load_pool('core')
            if not data.empty:
                self.core_pool = data
                self.pool_metadata['core_pool'] = {
                    'last_updated': datetime.now().isoformat(),
                    'count': len(data),
                    'date': data.iloc[0].get('date') if len(data) > 0 else None
                }
                logger.info(f"âœ… æ ¸å¿ƒè‚¡ç¥¨æ± å·²åŠ è½½åˆ°å†…å­˜: {len(data)} åªè‚¡ç¥¨")
                return True
            else:
                logger.warning("âš ï¸ æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
                return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ ¸å¿ƒè‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False




    def filter_by_type(self, df: pd.DataFrame, include_st: bool = False, status_filter: str = 'Active') -> pd.Series:
        """
        Aè‚¡è‚¡ç¥¨è¿‡æ»¤å™¨ - è¿”å›å¸ƒå°”Seriesç”¨äºè¿‡æ»¤

        Args:
            df: è¦è¿‡æ»¤çš„DataFrame
            include_st: æ˜¯å¦åŒ…å«STè‚¡ç¥¨ï¼Œé»˜è®¤Falseï¼ˆæ’é™¤STè‚¡ç¥¨ï¼‰
            status_filter: çŠ¶æ€è¿‡æ»¤å™¨ï¼Œé»˜è®¤'Active'ï¼ˆåªåŒ…å«æ´»è·ƒè‚¡ç¥¨ï¼‰

        Returns:
            pd.Series: å¸ƒå°”Seriesï¼Œè¡¨ç¤ºæ¯è¡Œæ˜¯å¦æ»¡è¶³è¿‡æ»¤æ¡ä»¶
        """
        try:
            if df is None or df.empty:
                logger.warning("âš ï¸ è¾“å…¥DataFrameä¸ºç©º")
                return pd.Series([], dtype=bool)

            # åˆå§‹åŒ–å…¨ä¸ºTrueçš„å¸ƒå°”Series
            mask = pd.Series(True, index=df.index)

            # è¿‡æ»¤ä¸ºAè‚¡è‚¡ç¥¨ï¼ˆä¸Šæµ·å’Œæ·±åœ³äº¤æ˜“æ‰€ï¼‰
            if 'order_book_id' in df.columns:
                a_stock_mask = (
                    df['order_book_id'].str.endswith('.XSHE') |
                    df['order_book_id'].str.endswith('.XSHG')
                )
                mask = mask & a_stock_mask

            # æ ¹æ®çŠ¶æ€è¿‡æ»¤
            if status_filter and status_filter != 'All' and 'status' in df.columns:
                status_mask = df['status'] == status_filter
                mask = mask & status_mask

            # æ ¹æ®STè‚¡ç¥¨è¿‡æ»¤
            if not include_st and 'abbrev_symbol' in df.columns:
                st_mask = ~df['abbrev_symbol'].str.contains('ST', na=False, case=False)
                mask = mask & st_mask

            logger.debug(f"âœ… Aè‚¡ç±»å‹è¿‡æ»¤: {mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")
            return mask

        except Exception as e:
            logger.error(f"âŒ Aè‚¡ç±»å‹è¿‡æ»¤å¤±è´¥: {e}")
            return pd.Series([], dtype=bool)

    def filter_by_type_to_cache(self, include_st: bool = False, status_filter: str = 'Active') -> bool:
        """
        Aè‚¡è‚¡ç¥¨è¿‡æ»¤å™¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰

        Args:
            include_st: æ˜¯å¦åŒ…å«STè‚¡ç¥¨ï¼Œé»˜è®¤Falseï¼ˆæ’é™¤STè‚¡ç¥¨ï¼‰
            status_filter: çŠ¶æ€è¿‡æ»¤å™¨ï¼Œé»˜è®¤'Active'ï¼ˆåªåŒ…å«æ´»è·ƒè‚¡ç¥¨ï¼‰

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            # è¿‡æ»¤ä¸ºAè‚¡è‚¡ç¥¨ï¼ˆä¸Šæµ·å’Œæ·±åœ³äº¤æ˜“æ‰€ï¼‰
            a_stocks = self.stock_list_cache[
                (self.stock_list_cache['order_book_id'].str.endswith('.XSHE')) |
                (self.stock_list_cache['order_book_id'].str.endswith('.XSHG'))
            ].copy()

            if a_stocks.empty:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°Aè‚¡è‚¡ç¥¨æ•°æ®")
                self.stock_list_cache = pd.DataFrame()
                return False

            # æ ¹æ®çŠ¶æ€è¿‡æ»¤
            if status_filter and 'status' in a_stocks.columns:
                a_stocks = a_stocks[a_stocks['status'] == status_filter]

            # æ ¹æ®STè‚¡ç¥¨è¿‡æ»¤
            if not include_st and 'abbrev_symbol' in a_stocks.columns:
                a_stocks = a_stocks[
                    ~a_stocks['abbrev_symbol'].str.contains('ST', na=False, case=False)
                ]

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = a_stocks
            logger.info(f"âœ… Aè‚¡è¿‡æ»¤å®Œæˆ: {len(a_stocks)} åªè‚¡ç¥¨ (STè¿‡æ»¤: {not include_st}, çŠ¶æ€: {status_filter})")
            return True

        except Exception as e:
            logger.error(f"âŒ Aè‚¡è‚¡ç¥¨è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_market_cap_to_cache(self, min_cap: Optional[float] = None, max_cap: Optional[float] = None) -> bool:
        """
        æŒ‰å¸‚å€¼è¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰

        Args:
            min_cap: æœ€å°å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            max_cap: æœ€å¤§å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            filtered_df = self.stock_list_cache.copy()

            # æŒ‰å¸‚å€¼ä¸‹é™è¿‡æ»¤
            if min_cap is not None and 'market_cap' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['market_cap'] >= min_cap]

            # æŒ‰å¸‚å€¼ä¸Šé™è¿‡æ»¤
            if max_cap is not None and 'market_cap' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['market_cap'] <= max_cap]

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ’° å¸‚å€¼è¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (å¸‚å€¼èŒƒå›´: {min_cap} - {max_cap})")
            return True

        except Exception as e:
            logger.error(f"âŒ å¸‚å€¼è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_market_cap(self, min_cap: Optional[float] = None, max_cap: Optional[float] = None) -> bool:
        """
        æŒ‰å¸‚å€¼è¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®

        Args:
            min_cap: æœ€å°å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            max_cap: æœ€å¤§å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            filtered_df = self.stock_list_cache.copy()

            # æŒ‰å¸‚å€¼ä¸‹é™è¿‡æ»¤
            if min_cap is not None and 'market_cap' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['market_cap'] >= min_cap]

            # æŒ‰å¸‚å€¼ä¸Šé™è¿‡æ»¤
            if max_cap is not None and 'market_cap' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['market_cap'] <= max_cap]

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ’° å¸‚å€¼è¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (å¸‚å€¼èŒƒå›´: {min_cap} - {max_cap})")
            return True

        except Exception as e:
            logger.error(f"âŒ å¸‚å€¼è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_industry_to_cache(self, industries: Optional[List[str]] = None) -> bool:
        """
        æŒ‰è¡Œä¸šè¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰

        Args:
            industries: è¡Œä¸šåˆ—è¡¨

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            if not industries:
                logger.info("â„¹ï¸ æ— è¡Œä¸šè¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡è¡Œä¸šè¿‡æ»¤")
                return True

            filtered_df = self.stock_list_cache.copy()

            # æŒ‰è¡Œä¸šè¿‡æ»¤
            if 'industry_name' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['industry_name'].isin(industries)]
            elif 'citics_industry_name' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['citics_industry_name'].isin(industries)]
            else:
                logger.warning("âš ï¸ è‚¡ç¥¨æ•°æ®ä¸­æ²¡æœ‰è¡Œä¸šä¿¡æ¯å­—æ®µ")
                return False

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ­ è¡Œä¸šè¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (è¡Œä¸š: {industries})")
            return True

        except Exception as e:
            logger.error(f"âŒ è¡Œä¸šè¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_exchange_to_cache(self, exchange: Optional[str] = None) -> bool:
        """
        æŒ‰äº¤æ˜“æ‰€è¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰

        Args:
            exchange: äº¤æ˜“æ‰€ä»£ç  ('XSHE' æˆ– 'XSHG')

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            if not exchange:
                logger.info("â„¹ï¸ æ— äº¤æ˜“æ‰€è¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡äº¤æ˜“æ‰€è¿‡æ»¤")
                return True

            if exchange not in ['XSHE', 'XSHG']:
                logger.error(f"âŒ ä¸æ”¯æŒçš„äº¤æ˜“æ‰€: {exchange}")
                return False

            filtered_df = self.stock_list_cache[
                self.stock_list_cache['order_book_id'].str.endswith(f'.{exchange}')
            ].copy()

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ¢ äº¤æ˜“æ‰€è¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (äº¤æ˜“æ‰€: {exchange})")
            return True

        except Exception as e:
            logger.error(f"âŒ äº¤æ˜“æ‰€è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_industry(self, industries: Optional[List[str]] = None) -> bool:
        """
        æŒ‰è¡Œä¸šè¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®

        Args:
            industries: è¡Œä¸šåˆ—è¡¨

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            if not industries:
                logger.info("â„¹ï¸ æ— è¡Œä¸šè¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡è¡Œä¸šè¿‡æ»¤")
                return True

            filtered_df = self.stock_list_cache.copy()

            # æŒ‰è¡Œä¸šè¿‡æ»¤
            if 'industry_name' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['industry_name'].isin(industries)]
            elif 'citics_industry_name' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['citics_industry_name'].isin(industries)]
            else:
                logger.warning("âš ï¸ è‚¡ç¥¨æ•°æ®ä¸­æ²¡æœ‰è¡Œä¸šä¿¡æ¯å­—æ®µ")
                return False

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ­ è¡Œä¸šè¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (è¡Œä¸š: {industries})")
            return True

        except Exception as e:
            logger.error(f"âŒ è¡Œä¸šè¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_exchange(self, exchange: Optional[str] = None) -> bool:
        """
        æŒ‰äº¤æ˜“æ‰€è¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®

        Args:
            exchange: äº¤æ˜“æ‰€ä»£ç  ('XSHE' æˆ– 'XSHG')

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            if not exchange:
                logger.info("â„¹ï¸ æ— äº¤æ˜“æ‰€è¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡äº¤æ˜“æ‰€è¿‡æ»¤")
                return True

            if exchange not in ['XSHE', 'XSHG']:
                logger.error(f"âŒ ä¸æ”¯æŒçš„äº¤æ˜“æ‰€: {exchange}")
                return False

            filtered_df = self.stock_list_cache[
                self.stock_list_cache['order_book_id'].str.endswith(f'.{exchange}')
            ].copy()

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ¢ äº¤æ˜“æ‰€è¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (äº¤æ˜“æ‰€: {exchange})")
            return True

        except Exception as e:
            logger.error(f"âŒ äº¤æ˜“æ‰€è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def apply_filters_to_cache(self, filters: Optional[Dict[str, Any]] = None) -> bool:
        """
        åº”ç”¨å¤šä¸ªè¿‡æ»¤å™¨åˆ°å†…éƒ¨ç¼“å­˜ - é“¾å¼è¿‡æ»¤ï¼ˆæ—§ç‰ˆå‡½æ•°ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰

        Args:
            filters: è¿‡æ»¤å™¨é…ç½®å­—å…¸
                {
                    'include_st': bool,          # æ˜¯å¦åŒ…å«STè‚¡ç¥¨
                    'status_filter': str,        # çŠ¶æ€è¿‡æ»¤å™¨
                    'min_market_cap': float,     # æœ€å°å¸‚å€¼
                    'max_market_cap': float,     # æœ€å¤§å¸‚å€¼
                    'industries': List[str],     # è¡Œä¸šåˆ—è¡¨
                    'exchange': str              # äº¤æ˜“æ‰€
                }

        Returns:
            bool: æ‰€æœ‰è¿‡æ»¤å™¨æ˜¯å¦éƒ½åº”ç”¨æˆåŠŸ
        """
        try:
            if not filters:
                logger.info("â„¹ï¸ æ— è¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡è¿‡æ»¤")
                return True

            success_count = 0
            total_filters = len(filters)

            # Aè‚¡åŸºç¡€è¿‡æ»¤
            if 'include_st' in filters or 'status_filter' in filters:
                include_st = filters.get('include_st', False)
                status_filter = filters.get('status_filter', 'Active')
                if self.filter_by_type_to_cache(include_st=include_st, status_filter=status_filter):
                    success_count += 1

            # å¸‚å€¼è¿‡æ»¤
            if 'min_market_cap' in filters or 'max_market_cap' in filters:
                min_cap = filters.get('min_market_cap')
                max_cap = filters.get('max_market_cap')
                if self.filter_by_market_cap_to_cache(min_cap=min_cap, max_cap=max_cap):
                    success_count += 1

            # è¡Œä¸šè¿‡æ»¤
            if 'industries' in filters:
                industries = filters['industries']
                if self.filter_by_industry_to_cache(industries=industries):
                    success_count += 1

            # äº¤æ˜“æ‰€è¿‡æ»¤
            if 'exchange' in filters:
                exchange = filters['exchange']
                if self.filter_by_exchange_to_cache(exchange=exchange):
                    success_count += 1

            logger.info(f"ğŸ¯ ç»¼åˆè¿‡æ»¤å®Œæˆ: {success_count}/{total_filters} ä¸ªè¿‡æ»¤å™¨æˆåŠŸåº”ç”¨")
            return success_count == total_filters

        except Exception as e:
            logger.error(f"âŒ ç»¼åˆè¿‡æ»¤å¤±è´¥: {e}")
            return False



    def fetch_stock_list(self, filters: Optional[Dict[str, Any]] = None) -> Optional[pd.DataFrame]:
        """
        è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨ (å¸¦ç¼“å­˜å’Œè¿‡æ»¤)

        Args:
            filters: è¿‡æ»¤å™¨é…ç½®å­—å…¸
                {
                    'include_st': bool,          # æ˜¯å¦åŒ…å«STè‚¡ç¥¨ï¼Œé»˜è®¤False
                    'status_filter': str,        # çŠ¶æ€è¿‡æ»¤å™¨ï¼Œé»˜è®¤'Active'
                    'min_market_cap': float,     # æœ€å°å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
                    'max_market_cap': float,     # æœ€å¤§å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
                    'industries': List[str],     # è¡Œä¸šåˆ—è¡¨
                    'exchange': str              # äº¤æ˜“æ‰€ ('XSHE' æˆ– 'XSHG')
                }

        Returns:
            DataFrame: æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨æˆ–Noneï¼ŒåŒ…å«è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

        ç¤ºä¾‹:
            # è·å–é»˜è®¤è¿‡æ»¤çš„è‚¡ç¥¨ï¼ˆæ´»è·ƒéSTè‚¡ç¥¨ï¼‰
            stocks_df = datastore.fetch_stock_list()

            # è·å–åŒ…å«STè‚¡ç¥¨çš„è‚¡ç¥¨
            stocks_with_st = datastore.fetch_stock_list({
                'include_st': True
            })

            # è·å–å¤§å¸‚å€¼è‚¡ç¥¨ï¼ˆå¸‚å€¼>50äº¿ï¼‰
            large_cap_stocks = datastore.fetch_stock_list({
                'min_market_cap': 500000  # 50äº¿ = 500000ä¸‡å…ƒ
            })

            # è·å–ç‰¹å®šè¡Œä¸šçš„è‚¡ç¥¨
            tech_stocks = datastore.fetch_stock_list({
                'industries': ['è®¡ç®—æœº', 'é€šä¿¡', 'ç”µå­']
            })

            # è·å–æ·±åœ³äº¤æ˜“æ‰€çš„è‚¡ç¥¨
            sz_stocks = datastore.fetch_stock_list({
                'exchange': 'XSHE'
            })

            # ç»¼åˆè¿‡æ»¤ï¼šéSTã€å¤§å¸‚å€¼ã€ç§‘æŠ€è¡Œä¸šã€æ·±åœ³äº¤æ˜“æ‰€
            filtered_stocks = datastore.fetch_stock_list({
                'include_st': False,
                'min_market_cap': 100000,  # 10äº¿
                'industries': ['è®¡ç®—æœº', 'é€šä¿¡'],
                'exchange': 'XSHE'
            })
        """
        try:
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆä¸€æ¬¡æ€§è¿è¡Œï¼Œæ— éœ€è¿‡æœŸæ£€æŸ¥ï¼‰
            if (self.stock_list_cache is not None and
                isinstance(self.stock_list_cache, pd.DataFrame) and
                not self.stock_list_cache.empty):
                logger.debug(f"âœ… ä½¿ç”¨ç¼“å­˜çš„Aè‚¡è‚¡ç¥¨åˆ—è¡¨: {len(self.stock_list_cache)} åªè‚¡ç¥¨")
                return self.stock_list_cache.copy()

            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ rqdatacä¸å¯ç”¨")
                return None

            # è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
            all_stocks = rqdatac.all_instruments(type='Stock', market='cn')

            if all_stocks is None or all_stocks.empty:
                logger.warning("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨æ•°æ®")
                return None

            # å…ˆåº”ç”¨è¿‡æ»¤å™¨ï¼Œç„¶åè®¾ç½®ç¼“å­˜
            filtered_stocks = all_stocks

            # åº”ç”¨è¿‡æ»¤å™¨
            if filters:
                filter_mask = self.apply_filters(all_stocks, filters)
                if filter_mask.empty:
                    logger.warning("âš ï¸ è¿‡æ»¤å™¨åº”ç”¨å¤±è´¥")
                    return None
                # ä½¿ç”¨å¸ƒå°”Seriesè¿‡æ»¤DataFrame
                filtered_stocks = all_stocks[filter_mask]
            else:
                # é»˜è®¤è¿‡æ»¤ï¼šåªä¿ç•™æ´»è·ƒè‚¡ç¥¨ï¼ˆæ’é™¤é€€å¸‚è‚¡ç¥¨ï¼‰
                default_filters = {'status_filter': 'Active'}
                filter_mask = self.apply_filters(all_stocks, default_filters)
                if filter_mask.empty:
                    logger.warning("âš ï¸ é»˜è®¤è¿‡æ»¤å™¨åº”ç”¨å¤±è´¥")
                    return None
                # ä½¿ç”¨å¸ƒå°”Seriesè¿‡æ»¤DataFrame
                filtered_stocks = all_stocks[filter_mask]

            # è®¾ç½®è¿‡æ»¤åçš„ç»“æœåˆ°ç¼“å­˜
            self.stock_list_cache = filtered_stocks

            if filtered_stocks.empty:
                logger.warning("âš ï¸ è¿‡æ»¤åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„Aè‚¡è‚¡ç¥¨")
                return None

            # æ›´æ–°æ—¶é—´æˆ³
            self.stock_list_timestamp = time.time()

            filter_desc = "é»˜è®¤è¿‡æ»¤" if not filters else f"è‡ªå®šä¹‰è¿‡æ»¤({len(filters)}ä¸ªæ¡ä»¶)"
            logger.info(f"ğŸ“‹ è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨: {len(filtered_stocks)} åªè‚¡ç¥¨ ({filter_desc})")
            return filtered_stocks

        except Exception as e:
            logger.error(f"âŒ è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def get_price(self, stock_code: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            Optional[pd.DataFrame]: ä»·æ ¼æ•°æ®DataFrameæˆ–None
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ rqdatacä¸å¯ç”¨")
                return None

            # ä½¿ç”¨rqdatacè·å–ä»·æ ¼æ•°æ®
            price_data = rqdatac.get_price(
                stock_code,
                start_date=start_date,
                end_date=end_date,
                frequency='1d',
                fields=['open', 'close', 'high', 'low', 'volume']
            )

            if price_data is None or price_data.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°{stock_code}çš„ä»·æ ¼æ•°æ®")
                return None

            # ç¡®ä¿dateåˆ—æ˜¯datetimeç±»å‹å¹¶è®¾ç½®ä¸ºç´¢å¼•
            if 'date' in price_data.columns:
                price_data['date'] = pd.to_datetime(price_data['date'])
                price_data = price_data.set_index('date')

            logger.debug(f"âœ… è·å–{stock_code}ä»·æ ¼æ•°æ®æˆåŠŸ: {len(price_data)}æ¡è®°å½•")
            return price_data

        except Exception as e:
            logger.error(f"âŒ è·å–{stock_code}ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return None

    def apply_filters(self, df: pd.DataFrame, filters: Dict[str, Any]) -> pd.Series:
        """
        åº”ç”¨è¿‡æ»¤å™¨å¹¶è¿”å›å¸ƒå°”Series

        Args:
            df: è¦è¿‡æ»¤çš„DataFrame
            filters: è¿‡æ»¤å™¨é…ç½®å­—å…¸

        Returns:
            pd.Series: å¸ƒå°”Seriesï¼Œè¡¨ç¤ºæ¯è¡Œæ˜¯å¦æ»¡è¶³è¿‡æ»¤æ¡ä»¶
        """
        try:
            if df is None or df.empty:
                logger.warning("âš ï¸ æ²¡æœ‰è‚¡ç¥¨æ•°æ®å¯ä¾›è¿‡æ»¤")
                return pd.Series([], dtype=bool)

            # åˆå§‹åŒ–å…¨ä¸ºTrueçš„å¸ƒå°”Series
            mask = pd.Series(True, index=df.index)

            # 1. STè‚¡ç¥¨è¿‡æ»¤
            if not filters.get('include_st', False):
                # è¿‡æ»¤æ‰STè‚¡ç¥¨ï¼ˆåç§°åŒ…å«STæˆ–*STï¼‰
                st_mask = ~df['symbol'].str.contains(r'ST|\*ST', case=False, na=False)
                mask = mask & st_mask
                logger.debug(f"ğŸ“Š STè¿‡æ»¤: {st_mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")

            # 2. çŠ¶æ€è¿‡æ»¤
            status_filter = filters.get('status_filter', 'Active')
            if status_filter != 'All':
                status_mask = df['status'] == status_filter
                mask = mask & status_mask
                logger.debug(f"ğŸ“Š çŠ¶æ€è¿‡æ»¤({status_filter}): {status_mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")

            # 3. å¸‚å€¼è¿‡æ»¤
            min_market_cap = filters.get('min_market_cap')
            max_market_cap = filters.get('max_market_cap')

            if min_market_cap is not None or max_market_cap is not None:
                # è¿™é‡Œéœ€è¦è·å–å¸‚å€¼æ•°æ®ï¼Œæš‚æ—¶è·³è¿‡å¸‚å€¼è¿‡æ»¤
                logger.debug("ğŸ“Š å¸‚å€¼è¿‡æ»¤: æš‚ä¸æ”¯æŒï¼ˆéœ€è¦å¸‚å€¼æ•°æ®ï¼‰")

            # 4. è¡Œä¸šè¿‡æ»¤
            industries = filters.get('industries')
            if industries:
                industry_mask = df['industry_code'].isin(industries)
                mask = mask & industry_mask
                logger.debug(f"ğŸ“Š è¡Œä¸šè¿‡æ»¤({industries}): {industry_mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")

            # 5. äº¤æ˜“æ‰€è¿‡æ»¤
            exchange = filters.get('exchange')
            if exchange:
                exchange_mask = df['exchange'] == exchange
                mask = mask & exchange_mask
                logger.debug(f"ğŸ“Š äº¤æ˜“æ‰€è¿‡æ»¤({exchange}): {exchange_mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")

            logger.info(f"âœ… è¿‡æ»¤å™¨åº”ç”¨æˆåŠŸ: {mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ‰€æœ‰æ¡ä»¶")
            return mask

        except Exception as e:
            logger.error(f"âŒ åº”ç”¨è¿‡æ»¤å™¨å¤±è´¥: {e}")
            return pd.Series([], dtype=bool)

    def _fetch_price_series(self, stock_code: str, start_date: str, end_date: str, force_refresh: bool = False) -> Optional[pd.DataFrame]:
        """
        è·å–Kçº¿æ•°æ® (ä¼˜åŒ–åçš„ç¼“å­˜é€»è¾‘)

        ç¼“å­˜ç­–ç•¥:
        - å†…å­˜ç¼“å­˜ï¼šå­˜å‚¨å®Œæ•´æ•°æ®ç”¨äºåç»­è®¡ç®—
        - ç£ç›˜ç¼“å­˜ï¼šæŒä¹…åŒ–å­˜å‚¨ï¼Œå‡å°‘ç½‘ç»œè®¿é—®
        - æ¯æ¬¡ä»ç½‘ç»œè·å–æ•°æ®ååŒæ­¥ä¿å­˜åˆ°ç£ç›˜
        - ä¸‹æ¬¡è·å–æ—¶å…ˆæ£€æŸ¥ç£ç›˜ç¼“å­˜ï¼Œå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®åˆ™åŠ è½½åˆ°å†…å­˜ï¼Œä¸å†è®¿é—®ç½‘ç»œ
        - ä¸è€ƒè™‘å¢é‡æ›´æ–°ï¼Œæ¯æ¬¡éƒ½æ‹‰å®Œæ•´æ•°æ®
        - æ”¯æŒå¼ºåˆ¶åˆ·æ–°å‚æ•°

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            force_refresh: æ˜¯å¦å¼ºåˆ¶ä»ç½‘ç»œè·å–æœ€æ–°æ•°æ®

        Returns:
            DataFrame: Kçº¿æ•°æ®æˆ–None
        """
        import time
        start_time = time.time()

        # ç”Ÿæˆç¼“å­˜é”®å’Œæ–‡ä»¶åï¼ˆæç®€è®¾è®¡ï¼‰
        cache_key = stock_code  # ç›´æ¥ä½¿ç”¨è‚¡ç¥¨ä»£ç ä½œä¸ºç¼“å­˜é”®
        cache_filename = f"kline_{cache_key}.json"

        # å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ï¼Œå…ˆå°è¯•ä»ç»Ÿä¸€ç¼“å­˜åŠ è½½
        if not force_refresh:
            cached_data = self._load_from_unified_cache(stock_code, end_date)
            if cached_data is not None:
                try:
                    # ä»ç¼“å­˜çš„æ•°æ®é‡å»ºDataFrame
                    records = cached_data['data']
                    df = pd.DataFrame(records)
                    if not df.empty:
                        # å°†dateåˆ—è®¾ç½®ä¸ºç´¢å¼•
                        if 'date' in df.columns:
                            df['date'] = pd.to_datetime(df['date'])
                            df = df.set_index('date')

                        # ç¼“å­˜åˆ°å†…å­˜ç”¨äºè®¡ç®—ï¼ˆæç®€è®¾è®¡ï¼‰
                        self.kline_cache[stock_code] = df.copy()

                        logger.debug(f"âœ… ä»ç»Ÿä¸€ç¼“å­˜åŠ è½½Kçº¿æ•°æ®: {stock_code}, {len(df)}æ¡è®°å½•")
                        return df
                except Exception as e:
                    logger.warning(f"âš ï¸ ç»Ÿä¸€ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯: {e}")

        # æ£€æŸ¥å†…å­˜ç¼“å­˜ï¼ˆä»…å½“ä¸æ˜¯å¼ºåˆ¶åˆ·æ–°æ—¶ï¼‰
        if not force_refresh:
            # é¦–å…ˆå°è¯•ä½¿ç”¨è¾“å…¥çš„stock_codeæŸ¥æ‰¾
            if stock_code in self.kline_cache:
                cached_df = self.kline_cache[stock_code]
                # éªŒè¯ç¼“å­˜æ•°æ®çš„è‚¡ç¥¨ä»£ç æ˜¯å¦åŒ¹é… - ç°åœ¨æ£€æŸ¥order_book_idåˆ—
                if 'order_book_id' in cached_df.columns and cached_df['order_book_id'].iloc[0] == stock_code:
                    logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­: {stock_code}")
                    return cached_df.copy()
                else:
                    # ç¼“å­˜æ•°æ®ä¸åŒ¹é…ï¼Œæ¸…é™¤æ— æ•ˆç¼“å­˜
                    del self.kline_cache[stock_code]
                    logger.debug(f"ğŸ§¹ æ¸…é™¤æ— æ•ˆå†…å­˜ç¼“å­˜: {stock_code}")
            
            # å¦‚æœè¾“å…¥çš„stock_codeæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„order_book_id
            for cache_key, cached_df in self.kline_cache.items():
                if 'order_book_id' in cached_df.columns and cached_df['order_book_id'].iloc[0] == stock_code:
                    logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­ (é€šè¿‡order_book_id): {stock_code}")
                    return cached_df.copy()

        # ä»rqdatacè·å–æ•°æ®
        if not RQDATAC_AVAILABLE or rqdatac is None:
            logger.warning(f"âš ï¸ rqdatacä¸å¯ç”¨ï¼Œæ— æ³•è·å–{stock_code}çš„æ•°æ®")
            return None

        # æ·»åŠ é‡è¯•æœºåˆ¶å’Œè¶…æ—¶æ§åˆ¶
        max_retries = 3
        timeout_seconds = 30  # 30ç§’è¶…æ—¶

        for attempt in range(max_retries):
            try:
                logger.debug(f"ğŸ“¡ è·å– {stock_code} æ•°æ® (å°è¯• {attempt + 1}/{max_retries})...")

                # ç›´æ¥åœ¨ä¸»çº¿ç¨‹è°ƒç”¨rqdatac.get_price()
                df = rqdatac.get_price(
                    stock_code,
                    start_date=start_date,
                    end_date=end_date,
                    frequency='1d',
                    fields=['open', 'close', 'high', 'low', 'limit_up', 'limit_down', 'total_turnover', 'volume', 'num_trades']
                )

                if df is not None and not df.empty:
                    # ä»MultiIndexä¸­æå–è‚¡ç¥¨ä»£ç 
                    if isinstance(df.index, pd.MultiIndex) and 'order_book_id' in df.index.names:
                        # é‡ç½®ç´¢å¼•ï¼Œå°†order_book_idå’Œdateä½œä¸ºåˆ—
                        df_reset = df.reset_index()
                        # æå–è‚¡ç¥¨ä»£ç  - ç›´æ¥ä½¿ç”¨order_book_id
                        extracted_stock_code = df_reset['order_book_id'].iloc[0]
                        # é‡æ–°è®¾ç½®dateä¸ºç´¢å¼•
                        df_reset = df_reset.set_index('date')
                        df = df_reset

                        logger.debug(f"ğŸ“Š ä»DataFrameæå–è‚¡ç¥¨ä»£ç : {extracted_stock_code}")
                    else:
                        # å¦‚æœæ²¡æœ‰MultiIndexï¼Œä½¿ç”¨ä¼ å…¥çš„stock_code
                        extracted_stock_code = stock_code
                        logger.debug(f"ğŸ“Š ä½¿ç”¨ä¼ å…¥çš„è‚¡ç¥¨ä»£ç : {stock_code}")

                    # ç›´æ¥ä½¿ç”¨order_book_idä½œä¸ºç¼“å­˜é”®
                    cache_key = extracted_stock_code
                    cache_filename = f"kline_{cache_key}.json"

                    # æ³¨æ„ï¼šä¸å†åœ¨è¿™é‡Œä¿å­˜åˆ°ç»Ÿä¸€ç¼“å­˜ï¼Œè€Œæ˜¯åœ¨æ‰¹é‡å¤„ç†å®Œæˆåç»Ÿä¸€ä¿å­˜
                    # ä¿å­˜åˆ°ç»Ÿä¸€ç¼“å­˜çš„é€»è¾‘å°†ç§»åˆ°æ‰¹é‡å¤„ç†çš„åœ°æ–¹

                    # ç¼“å­˜åˆ°å†…å­˜ç”¨äºè®¡ç®—ï¼ˆæç®€è®¾è®¡ï¼‰
                    self.kline_cache[extracted_stock_code] = df.copy()

                    logger.debug(f"âœ… è·å–Kçº¿æ•°æ®: {extracted_stock_code}, {len(df)}æ¡è®°å½•")

                    return df
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°{stock_code}çš„Kçº¿æ•°æ®")
                    return None

            except TimeoutError as e:
                logger.warning(f"âš ï¸ {stock_code} è¯·æ±‚è¶…æ—¶: {e}")
                if attempt < max_retries - 1:
                    logger.debug(f"é‡è¯•ä¸­ ({attempt + 1}/{max_retries})...")
                    continue
                else:
                    logger.error(f"âŒ {stock_code} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
                    return None

            except Exception as e:
                logger.warning(f"âš ï¸ è·å– {stock_code} æ•°æ®å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    logger.debug(f"é‡è¯•ä¸­ ({attempt + 1}/{max_retries})...")
                    continue
                else:
                    logger.error(f"âŒ {stock_code} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥: {e}")
                    return None

        return None

    def _fetch_instruments_info(self, stock_code: str) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

        rqdatac.instruments() è¿”å›çš„å­—æ®µåŒ…æ‹¬:
        - order_book_id: è®¢å•ç°¿ID (å¦‚: '000001.XSHE')
        - symbol: è‚¡ç¥¨åç§° (å¦‚: 'å¹³å®‰é“¶è¡Œ')
        - abbrev_symbol: ç¼©å†™ç¬¦å· (å¦‚: 'PAYH')
        - trading_code: äº¤æ˜“ä»£ç  (å¦‚: '000001')
        - exchange: äº¤æ˜“æ‰€ (å¦‚: 'XSHE')
        - type: è¯åˆ¸ç±»å‹ (å¦‚: 'CS' - è‚¡ç¥¨)
        - status: çŠ¶æ€ (å¦‚: 'Active')
        - listed_date: ä¸Šå¸‚æ—¥æœŸ (å¦‚: '1991-04-03')
        - de_listed_date: é€€å¸‚æ—¥æœŸ (å¦‚: '0000-00-00')
        - board_type: æ¿å—ç±»å‹ (å¦‚: 'MainBoard')
        - industry_code: è¡Œä¸šä»£ç  (å¦‚: 'J66')
        - industry_name: è¡Œä¸šåç§° (å¦‚: 'è´§å¸é‡‘èæœåŠ¡')
        - sector_code: æ¿å—ä»£ç  (å¦‚: 'Financials')
        - sector_code_name: æ¿å—åç§° (å¦‚: 'é‡‘è')
        - citics_industry_code: ä¸­ä¿¡è¡Œä¸šä»£ç  (å¦‚: '40')
        - citics_industry_name: ä¸­ä¿¡è¡Œä¸šåç§° (å¦‚: 'é“¶è¡Œ')
        - province: çœä»½ (å¦‚: 'å¹¿ä¸œçœ')
        - office_address: åŠå…¬åœ°å€
        - issue_price: å‘è¡Œä»· (å¦‚: 40.0)
        - round_lot: æ¯æ‰‹è‚¡æ•° (å¦‚: 100)
        - market_tplus: T+1å¸‚åœº (å¦‚: 1)
        - trading_hours: äº¤æ˜“æ—¶é—´ (å¦‚: '09:31-11:30,13:01-15:00')
        - special_type: ç‰¹æ®Šç±»å‹ (å¦‚: 'Normal')
        - concept_names: æ¦‚å¿µåç§°åˆ—è¡¨

        Returns:
            DataFrame: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µä½œä¸ºåˆ—
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ rqdatacä¸å¯ç”¨")
                return None

            # è·å–åŸºæœ¬ä¿¡æ¯ - ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°å
            basic_info = rqdatac.instruments(stock_code)
            if basic_info is not None:
                # å°† Instrument å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                info_dict = {}
                for attr in dir(basic_info):
                    if not attr.startswith('_') and not callable(getattr(basic_info, attr)):
                        try:
                            value = getattr(basic_info, attr)
                            info_dict[attr] = value
                        except Exception as e:
                            logger.debug(f"âš ï¸ è·å–å±æ€§ {attr} å¤±è´¥: {e}")

                # è½¬æ¢ä¸ºDataFrameæ ¼å¼ï¼Œè‚¡ç¥¨ä»£ç ä½œä¸ºç´¢å¼•
                if info_dict:
                    df = pd.DataFrame([info_dict])
                    df.index = pd.Index([stock_code])
                    logger.debug(f"ğŸ“Š è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯: {stock_code}, {len(info_dict)} ä¸ªå­—æ®µ")
                    return df
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä¿¡æ¯å­—æ®µ: {stock_code}")
                    return None
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨ä¿¡æ¯: {stock_code}")
                return None

        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥ {stock_code}: {e}")
            return None

    def _fetch_valuation_series(self, stock_code: str, date: Optional[str] = None) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨ä¼°å€¼ä¿¡æ¯
        æ”¯æŒæ‰¹é‡è·å–ä¼˜åŒ–

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æŸ¥è¯¢æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©

        Returns:
            DataFrame: ä¼°å€¼æ•°æ®ï¼ŒåŒ…å«å¤šåˆ—ä¼°å€¼æŒ‡æ ‡
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ rqdatacä¸å¯ç”¨")
                return None

            if date is None:
                date = datetime.now().strftime('%Y-%m-%d')

            # è·å–ä¼°å€¼å› å­æ•°æ®
            valuation_factors = ['pe_ratio', 'pb_ratio', 'ps_ratio', 'pcf_ratio', 'market_cap', 'turnover_ratio']

            # å°è¯•ä»æ‰¹é‡ç¼“å­˜ä¸­è·å–
            cached_result = self._get_cached_batch_valuation(stock_code, date, valuation_factors)
            if cached_result is not None:
                return cached_result

            # å¦‚æœæ²¡æœ‰æ‰¹é‡ç¼“å­˜ï¼Œåˆ™é€ä¸ªè·å–
            valuation_data = rqdatac.get_factor(stock_code, valuation_factors, start_date=date, end_date=date)

            if valuation_data is not None and not valuation_data.empty:
                # ç¡®ä¿è¿”å›DataFrameæ ¼å¼
                if isinstance(valuation_data, pd.Series):
                    valuation_data = valuation_data.to_frame().T
                elif not isinstance(valuation_data, pd.DataFrame):
                    logger.warning(f"âš ï¸ ä¼°å€¼æ•°æ®æ ¼å¼å¼‚å¸¸: {type(valuation_data)}")
                    return None

                logger.debug(f"ğŸ’° è·å–ä¼°å€¼ä¿¡æ¯: {stock_code} ({date}), {len(valuation_data)} æ¡è®°å½•")

                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ¸å¿ƒå­—æ®µéƒ½ä¸ºNaNï¼ˆæ•°æ®è´¨é‡é—®é¢˜ï¼‰
                core_fields = ['pe_ratio', 'pb_ratio', 'market_cap']
                all_core_nan = True
                for field in core_fields:
                    if field in valuation_data.columns and not valuation_data[field].isna().all():
                        all_core_nan = False
                        break

                if all_core_nan:
                    logger.warning(f"âš ï¸ {stock_code} ({date}) æ ¸å¿ƒä¼°å€¼æ•°æ®å…¨éƒ¨ä¸ºNaNï¼Œå¯èƒ½æ˜¯æ•°æ®æ›´æ–°å»¶è¿Ÿ")
                    return None

                return valuation_data
            else:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°ä¼°å€¼æ•°æ®: {stock_code} ({date})")
                return None

        except Exception as e:
            logger.error(f"âŒ è·å–ä¼°å€¼ä¿¡æ¯å¤±è´¥ {stock_code}: {e}")
            return None

    def _save_to_unified_cache(self, stock_code: str, df: pd.DataFrame, start_date: str, end_date: str) -> bool:
        """
        å°†è‚¡ç¥¨æ•°æ®ä¿å­˜åˆ°ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrame
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç”Ÿæˆç»Ÿä¸€ç¼“å­˜æ–‡ä»¶åï¼ˆä½¿ç”¨ç»“æŸæ—¥æœŸä½œä¸ºæ ‡è¯†ï¼‰
            cache_filename = f"{end_date}_kline_data.json"

            # å°è¯•åŠ è½½ç°æœ‰çš„ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶
            existing_data = self.load_data_from_file(cache_filename)
            if existing_data is None:
                existing_data = {
                    'trading_date': end_date,
                    'fetch_time': datetime.now().isoformat(),
                    'stocks': {}
                }

            # æ·»åŠ æˆ–æ›´æ–°å½“å‰è‚¡ç¥¨çš„æ•°æ®
            stock_data = {
                'stock_code': stock_code,
                'start_date': start_date,
                'end_date': end_date,
                'data': df.reset_index().to_dict('records'),
                'data_points': len(df),
                'last_updated': datetime.now().isoformat()
            }

            existing_data['stocks'][stock_code] = stock_data
            existing_data['fetch_time'] = datetime.now().isoformat()  # æ›´æ–°æ•´ä½“è·å–æ—¶é—´

            # ä¿å­˜åˆ°æ–‡ä»¶
            # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç´§å‡‘æ ¼å¼ä»¥æé«˜æ€§èƒ½
            use_indent = not (
                os.getenv('ENV', '').lower() == 'production' or
                os.getenv('PRODUCTION', '').lower() in ('true', '1', 'yes') or
                not os.getenv('DEBUG', '').lower() in ('true', '1', 'yes')
            )
            return self.save_data_to_file(existing_data, cache_filename, use_indent=use_indent)

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ°ç»Ÿä¸€ç¼“å­˜å¤±è´¥ {stock_code}: {e}")
            return False

    def _save_all_stocks_to_unified_cache(self, price_data: Dict[str, pd.DataFrame], start_date: str, end_date: str) -> bool:
        """
        å°†æ‰€æœ‰è‚¡ç¥¨æ•°æ®ä¸€æ¬¡æ€§ä¿å­˜åˆ°ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶

        Args:
            price_data: ä»·æ ¼æ•°æ®å­—å…¸ {stock_code: price_df}
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç”Ÿæˆç»Ÿä¸€ç¼“å­˜æ–‡ä»¶å
            cache_filename = f"{end_date}_kline_data.json"

            # å‡†å¤‡ç»Ÿä¸€ç¼“å­˜æ•°æ®ç»“æ„
            unified_data = {
                'trading_date': end_date,
                'fetch_time': datetime.now().isoformat(),
                'stocks': {}
            }

            # å¤„ç†æ¯åªè‚¡ç¥¨çš„æ•°æ® - æ‰¹é‡å¤„ç†ä»¥æé«˜æ€§èƒ½
            processed_count = 0
            for stock_code, df in price_data.items():
                if df is not None and not df.empty:
                    stock_data = {
                        'stock_code': stock_code,
                        'start_date': start_date,
                        'end_date': end_date,
                        'data': df.reset_index().to_dict('records'),
                        'data_points': len(df),
                        'last_updated': datetime.now().isoformat()
                    }
                    unified_data['stocks'][stock_code] = stock_data
                    processed_count += 1

                    # æ¯å¤„ç†100åªè‚¡ç¥¨è®°å½•ä¸€æ¬¡è¿›åº¦ï¼ˆå‡å°‘æ—¥å¿—é¢‘ç‡ï¼‰
                    if processed_count % 100 == 0:
                        logger.debug(f"ğŸ“Š å·²å¤„ç† {processed_count} åªè‚¡ç¥¨æ•°æ®")

            # ä¿å­˜åˆ°æ–‡ä»¶
            # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç´§å‡‘æ ¼å¼ä»¥æé«˜æ€§èƒ½
            use_indent = not (
                os.getenv('ENV', '').lower() == 'production' or
                os.getenv('PRODUCTION', '').lower() in ('true', '1', 'yes') or
                not os.getenv('DEBUG', '').lower() in ('true', '1', 'yes')
            )
            success = self.save_data_to_file(unified_data, cache_filename, use_indent=use_indent)
            if success:
                logger.info(f"ğŸ’¾ ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶å·²ä¿å­˜: {cache_filename}, åŒ…å« {len(unified_data['stocks'])} åªè‚¡ç¥¨")
            else:
                logger.error(f"âŒ ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶ä¿å­˜å¤±è´¥: {cache_filename}")

            return success

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¿å­˜åˆ°ç»Ÿä¸€ç¼“å­˜å¤±è´¥: {e}")
            return False

    def save_valuation_data_to_cache(self, valuation_df: pd.DataFrame, target_date: str) -> bool:
        """
        å°†ä¼°å€¼æ•°æ®ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶

        Args:
            valuation_df: ä¼°å€¼æ•°æ®DataFrame
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            if valuation_df.empty:
                logger.warning("âš ï¸ ä¼°å€¼æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                return False

            # ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
            cache_filename = f"{target_date}_valuation_data.json"

            # å‡†å¤‡ç¼“å­˜æ•°æ®ç»“æ„
            cache_data = {
                'trading_date': target_date,
                'fetch_time': datetime.now().isoformat(),
                'valuation_data': valuation_df.to_dict('records'),
                'total_stocks': len(valuation_df)
            }

            # ä¿å­˜åˆ°æ–‡ä»¶
            # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç´§å‡‘æ ¼å¼ä»¥æé«˜æ€§èƒ½
            use_indent = not (
                os.getenv('ENV', '').lower() == 'production' or
                os.getenv('PRODUCTION', '').lower() in ('true', '1', 'yes') or
                not os.getenv('DEBUG', '').lower() in ('true', '1', 'yes')
            )
            success = self.save_data_to_file(cache_data, cache_filename, use_indent=use_indent)
            if success:
                logger.info(f"ğŸ’¾ ä¼°å€¼æ•°æ®å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_filename}, åŒ…å« {len(valuation_df)} åªè‚¡ç¥¨")
            else:
                logger.error(f"âŒ ä¼°å€¼æ•°æ®ä¿å­˜å¤±è´¥: {cache_filename}")

            return success

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä¼°å€¼æ•°æ®åˆ°ç¼“å­˜å¤±è´¥: {e}")
            return False

    def load_valuation_data_from_cache(self, target_date: str) -> Optional[pd.DataFrame]:
        """
        ä»ç¼“å­˜æ–‡ä»¶åŠ è½½ä¼°å€¼æ•°æ®

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            Optional[pd.DataFrame]: ä¼°å€¼æ•°æ®DataFrameæˆ–None
        """
        try:
            cache_filename = f"{target_date}_valuation_data.json"
            cached_data = self.load_data_from_file(cache_filename)

            if cached_data is None:
                return None

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡24å°æ—¶ï¼‰
            fetch_time = cached_data.get('fetch_time')
            if fetch_time:
                fetch_datetime = datetime.fromisoformat(fetch_time)
                if (datetime.now() - fetch_datetime).total_seconds() > 24 * 3600:
                    logger.debug(f"âš ï¸ ä¼°å€¼æ•°æ®ç¼“å­˜å·²è¿‡æœŸ: {cache_filename}")
                    return None

            # è½¬æ¢ä¸ºDataFrame
            records = cached_data.get('valuation_data', [])
            if records:
                df = pd.DataFrame(records)
                logger.info(f"ğŸ“ ä¼°å€¼æ•°æ®å·²ä»ç¼“å­˜åŠ è½½: {cache_filename}, {len(df)} åªè‚¡ç¥¨")
                return df
            else:
                logger.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æ ¼å¼é”™è¯¯: {cache_filename}")
                return None

        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½ä¼°å€¼æ•°æ®ç¼“å­˜å¤±è´¥: {e}")
            return None

    def _load_from_unified_cache(self, stock_code: str, target_date: str) -> Optional[Dict]:
        """
        ä»ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶ä¸­åŠ è½½è‚¡ç¥¨æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨å†…å­˜ç¼“å­˜ï¼‰

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            Dict: è‚¡ç¥¨æ•°æ®æˆ–None
        """
        try:
            # ç”Ÿæˆç»Ÿä¸€ç¼“å­˜æ–‡ä»¶å
            cache_filename = f"{target_date}_kline_data.json"

            # é¦–å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
            if hasattr(self, '_unified_cache') and cache_filename in self._unified_cache:
                cached_data = self._unified_cache[cache_filename]
                logger.debug(f"ğŸ“ ä»å†…å­˜ç¼“å­˜åŠ è½½ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶: {cache_filename}")
            else:
                # ä»æ–‡ä»¶åŠ è½½å¹¶å­˜å…¥å†…å­˜ç¼“å­˜
                cached_data = self.load_data_from_file(cache_filename, skip_logging=True)
                if cached_data is None:
                    return None

                # åˆå§‹åŒ–å†…å­˜ç¼“å­˜
                if not hasattr(self, '_unified_cache'):
                    self._unified_cache = {}

                # å­˜å…¥å†…å­˜ç¼“å­˜
                self._unified_cache[cache_filename] = cached_data
                logger.debug(f"ğŸ“ ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶å·²åŠ è½½åˆ°å†…å­˜: {cache_filename}")

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡24å°æ—¶ï¼‰
            fetch_time = cached_data.get('fetch_time')
            if fetch_time:
                fetch_datetime = datetime.fromisoformat(fetch_time)
                if (datetime.now() - fetch_datetime).total_seconds() > 24 * 3600:
                    logger.debug(f"âš ï¸ ç»Ÿä¸€ç¼“å­˜æ–‡ä»¶å·²è¿‡æœŸ: {cache_filename}")
                    # ä»å†…å­˜ç¼“å­˜ä¸­ç§»é™¤è¿‡æœŸæ–‡ä»¶
                    if hasattr(self, '_unified_cache') and cache_filename in self._unified_cache:
                        del self._unified_cache[cache_filename]
                    return None

            # æ£€æŸ¥äº¤æ˜“æ—¥æ˜¯å¦åŒ¹é…
            trading_date = cached_data.get('trading_date')
            if trading_date != target_date:
                logger.debug(f"âš ï¸ ç¼“å­˜æ–‡ä»¶äº¤æ˜“æ—¥ä¸åŒ¹é…: {trading_date} vs {target_date}")
                return None

            # è·å–æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
            stocks_data = cached_data.get('stocks', {})
            stock_data = stocks_data.get(stock_code)

            if stock_data is None:
                logger.debug(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
                return None

            logger.debug(f"âœ… ä»ç»Ÿä¸€ç¼“å­˜åŠ è½½è‚¡ç¥¨æ•°æ®: {stock_code}")
            return stock_data

        except Exception as e:
            logger.error(f"âŒ ä»ç»Ÿä¸€ç¼“å­˜åŠ è½½å¤±è´¥ {stock_code}: {e}")
            return None

    def _get_latest_trading_date(self, stock_code: str, max_days_back: int = 30) -> Optional[str]:
        """
        è·å–æœ€æ–°çš„æœ‰æ•ˆäº¤æ˜“æ—¥
        ä½¿ç”¨äº¤æ˜“æ—¥å†APIç›´æ¥è·å–æœ‰æ•ˆäº¤æ˜“æ—¥ï¼Œé¿å…é€šè¿‡æ•°æ®éªŒè¯çš„ä½æ•ˆæ–¹å¼

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_days_back: æœ€å¤§å›æº¯å¤©æ•°

        Returns:
            æœ‰æ•ˆçš„äº¤æ˜“æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD) æˆ– None
        """
        try:
            # è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥å†
            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=max_days_back)

            # ä½¿ç”¨rqdatacè·å–äº¤æ˜“æ—¥å†
            trading_dates = rqdatac.get_trading_dates(start_date, end_date)  # type: ignore

            if trading_dates is None or len(trading_dates) == 0:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°äº¤æ˜“æ—¥å†æ•°æ®")
                return None

            # è½¬æ¢ä¸ºæ—¥æœŸåˆ—è¡¨å¹¶æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            trading_dates = sorted([pd.to_datetime(d).date() for d in trading_dates], reverse=True)

            # è¿”å›æœ€æ–°çš„äº¤æ˜“æ—¥
            if len(trading_dates) > 0:
                latest_date = trading_dates[0]
                date_str = latest_date.strftime('%Y-%m-%d')
                logger.debug(f"âœ… æ‰¾åˆ°æœ€æ–°äº¤æ˜“æ—¥: {stock_code} @ {date_str}")
                return date_str

            logger.warning(f"âš ï¸ åœ¨{max_days_back}å¤©å†…æœªæ‰¾åˆ°æœ‰æ•ˆçš„äº¤æ˜“æ—¥")
            return None

        except Exception as e:
            logger.error(f"âŒ è·å–æœ€æ–°äº¤æ˜“æ—¥å¤±è´¥ {stock_code}: {e}")
            return None

    def _determine_target_trading_date(self) -> None:
        """
        é¢„å…ˆç¡®å®šç›®æ ‡äº¤æ˜“æ—¥ï¼Œé¿å…å¯¹æ¯åªè‚¡ç¥¨é‡å¤æ£€æŸ¥
        ä½¿ç”¨ get_trading_hours è·å–ç²¾ç¡®çš„äº¤æ˜“æ—¶æ®µè¿›è¡Œåˆ¤æ–­

        æ ¸å¿ƒé€»è¾‘:
        1. è·å–æœ€æ–°äº¤æ˜“æ—¥å’Œäº¤æ˜“æ—¶æ®µ
        2. æ ¹æ®å½“å‰æ—¶é—´å’Œäº¤æ˜“æ—¶æ®µæ™ºèƒ½åˆ¤æ–­åº”è¯¥ä½¿ç”¨å“ªä¸ªäº¤æ˜“æ—¥
        3. äº¤æ˜“å‰/äº¤æ˜“ä¸­/æ”¶ç›˜å3å°æ—¶å†…ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
        4. æ”¶ç›˜å3å°æ—¶åï¼šä½¿ç”¨å½“å¤©æ•°æ®
        5. åªåšä¸€æ¬¡å…¨å±€åˆ¤æ–­ï¼Œåç»­æ‰€æœ‰è‚¡ç¥¨éƒ½ä½¿ç”¨ç›¸åŒçš„ç»“æœ

        æ€§èƒ½ä¼˜åŒ–:
        - å…¨å±€ä¸€æ¬¡åˆ¤æ–­ vs æ¯åªè‚¡ç¥¨å•ç‹¬åˆ¤æ–­
        - åŸºäºäº¤æ˜“æ—¶æ®µçš„ç²¾ç¡®åˆ¤æ–­ vs ç®€å•æ—¶é—´åˆ¤æ–­
        - æ™ºèƒ½é™çº§ç­–ç•¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§

        Raises:
            æ‰€æœ‰å¼‚å¸¸éƒ½ä¼šè¢«æ•è·å¹¶é™çº§å¤„ç†ï¼Œä¸ä¼šå½±å“ç³»ç»Ÿè¿è¡Œ
        """
        logger.debug("ğŸ” å¼€å§‹ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥")

        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ RQDatacä¸å¯ç”¨ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºåå¤‡")
                self.target_trading_date = datetime.now().strftime('%Y-%m-%d')
                self.target_trading_date_determined = True
                logger.debug(f"ğŸ“… ä½¿ç”¨åå¤‡æ—¥æœŸ: {self.target_trading_date}")
                return

            # è·å–æœ€æ–°äº¤æ˜“æ—¥
            latest_date = rqdatac.get_latest_trading_date()  # type: ignore
            if not latest_date:
                logger.warning("âš ï¸ æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
                self.target_trading_date = datetime.now().strftime('%Y-%m-%d')
                self.target_trading_date_determined = True
                return

            logger.info(f"ğŸ“… æœ€æ–°äº¤æ˜“æ—¥: {latest_date}")

            # è·å–å½“å‰æ—¶é—´
            now = datetime.now()
            latest_date_dt = pd.to_datetime(latest_date).to_pydatetime()
            logger.debug(f"ğŸ• å½“å‰æ—¶é—´: {now}, æœ€æ–°äº¤æ˜“æ—¥: {latest_date_dt.date()}")

            # åˆ¤æ–­æ˜¯å¦åœ¨äº¤æ˜“æ—¥å½“å¤©
            if now.date() == latest_date_dt.date():
                logger.debug("ğŸ“… å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ï¼Œéœ€è¦åˆ¤æ–­æ•°æ®æœ‰æ•ˆæ€§")

                # è·å–å®é™…äº¤æ˜“æ—¶æ®µè¿›è¡Œç²¾ç¡®åˆ¤æ–­
                try:
                    # ä½¿ç”¨å¹³å®‰é“¶è¡Œä½œä¸ºä»£è¡¨è‚¡ç¥¨è·å–äº¤æ˜“æ—¶æ®µ
                    trading_hours_str = rqdatac.get_trading_hours('000001.XSHE', latest_date.strftime('%Y-%m-%d'))  # type: ignore
                    logger.info(f"â° äº¤æ˜“æ—¶æ®µ: {trading_hours_str}")

                    # éªŒè¯äº¤æ˜“æ—¶æ®µæ•°æ®æ ¼å¼
                    if not isinstance(trading_hours_str, str):
                        logger.error(f"âŒ äº¤æ˜“æ—¶æ®µæ•°æ®æ ¼å¼å¼‚å¸¸: {type(trading_hours_str)}")
                        raise ValueError(f"äº¤æ˜“æ—¶æ®µæ ¼å¼å¼‚å¸¸: {trading_hours_str}")

                    # è§£æäº¤æ˜“æ—¶æ®µå¹¶åˆ¤æ–­æ•°æ®æœ‰æ•ˆæ€§
                    should_use_previous = self._should_use_previous_trading_date(trading_hours_str, now)
                    logger.debug(f"ğŸ¯ æ˜¯å¦ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥: {should_use_previous}")

                    if should_use_previous:
                        # ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
                        previous_date = rqdatac.get_previous_trading_date(latest_date)  # type: ignore
                        if previous_date:
                            self.target_trading_date = previous_date.strftime('%Y-%m-%d')
                            logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (å‰ä¸€ä¸ªäº¤æ˜“æ—¥)")
                        else:
                            # å¦‚æœæ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
                            self.target_trading_date = latest_date.strftime('%Y-%m-%d')
                            logger.warning(f"âš ï¸ æ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥: {self.target_trading_date}")
                    else:
                        # ä½¿ç”¨å½“å¤©æ•°æ®
                        self.target_trading_date = latest_date.strftime('%Y-%m-%d')
                        logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (æœ€æ–°äº¤æ˜“æ—¥)")

                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–äº¤æ˜“æ—¶æ®µå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ—¶é—´åˆ¤æ–­: {e}")
                    logger.debug("ğŸ”„ æ‰§è¡Œé™çº§æ—¶é—´æ£€æŸ¥é€»è¾‘", exc_info=True)
                    # é™çº§åˆ°ç®€å•çš„æ—¶é—´åˆ¤æ–­é€»è¾‘
                    self._fallback_time_check(latest_date_dt, now)
            else:
                # å½“å‰ä¸åœ¨äº¤æ˜“æ—¥å½“å¤©ï¼Œæœ€æ–°äº¤æ˜“æ—¥æ•°æ®åº”è¯¥æœ‰æ•ˆ
                logger.info("ğŸ“… å½“å‰ä¸åœ¨äº¤æ˜“æ—¥å½“å¤©ï¼Œæœ€æ–°äº¤æ˜“æ—¥æ•°æ®åº”è¯¥æœ‰æ•ˆ")
                self.target_trading_date = latest_date.strftime('%Y-%m-%d')
                logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (æœ€æ–°äº¤æ˜“æ—¥)")

            self.target_trading_date_determined = True
            logger.debug("âœ… ç›®æ ‡äº¤æ˜“æ—¥ç¡®å®šå®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥å¤±è´¥: {e}")
            logger.debug("ğŸ”„ ä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºæœ€ç»ˆåå¤‡", exc_info=True)
            # é™çº§ä½¿ç”¨å½“å‰æ—¥æœŸ
            self.target_trading_date = datetime.now().strftime('%Y-%m-%d')
            self.target_trading_date_determined = True

    def get_target_trading_date(self) -> Optional[str]:
        """
        è·å–é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥

        Returns:
            ç›®æ ‡äº¤æ˜“æ—¥å­—ç¬¦ä¸² (YYYY-MM-DD) æˆ– None
        """
        if not self.target_trading_date_determined:
            self._determine_target_trading_date()
        return self.target_trading_date

    def _get_cached_batch_valuation(self, stock_code: str, date: str, factors: List[str]) -> Optional[pd.DataFrame]:
        """
        ä»æ‰¹é‡ç¼“å­˜ä¸­è·å–ä¼°å€¼æ•°æ®
        å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåˆ™è§¦å‘æ‰¹é‡è·å–

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸ
            factors: å› å­åˆ—è¡¨

        Returns:
            DataFrame: ä¼°å€¼æ•°æ®æˆ–None
        """
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ‰¹é‡æ•°æ®ç¼“å­˜
        cache_key = f"batch_valuation_{date}"
        if cache_key not in self.batch_cache:
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„æ‰¹é‡è¯·æ±‚
            if not hasattr(self, '_pending_batch_stocks'):
                self._pending_batch_stocks = set()

            self._pending_batch_stocks.add(stock_code)

            # å¦‚æœç§¯ç´¯äº†è¶³å¤Ÿå¤šçš„è‚¡ç¥¨ï¼ˆæ¯”å¦‚5åªï¼‰ï¼Œå°±æ‰§è¡Œæ‰¹é‡è·å–
            if len(self._pending_batch_stocks) >= 5:
                self._execute_batch_valuation_fetch(date, factors)
                self._pending_batch_stocks.clear()

                # é‡æ–°æ£€æŸ¥ç¼“å­˜
                if cache_key in self.batch_cache:
                    batch_data = self.batch_cache[cache_key]
                    return self._extract_single_stock_from_batch(batch_data, stock_code, date)

        # ä»ç¼“å­˜ä¸­æå–å•ä¸ªè‚¡ç¥¨çš„æ•°æ®
        if cache_key in self.batch_cache:
            batch_data = self.batch_cache[cache_key]
            return self._extract_single_stock_from_batch(batch_data, stock_code, date)

        return None

    def _execute_batch_valuation_fetch(self, date: str, factors: List[str]) -> None:
        """
        æ‰§è¡Œæ‰¹é‡ä¼°å€¼æ•°æ®è·å–

        æ‰¹é‡è·å–ç­–ç•¥:
        - ç´¯ç§¯å¤šä¸ªè‚¡ç¥¨è¯·æ±‚åç»Ÿä¸€è·å–
        - å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼Œæé«˜æ•ˆç‡
        - æ™ºèƒ½ç¼“å­˜æ‰¹é‡ç»“æœ
        - å¤±è´¥æ—¶ä¸å½±å“å•ä¸ªè‚¡ç¥¨è·å–

        Args:
            date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
            factors: è¦è·å–çš„ä¼°å€¼å› å­åˆ—è¡¨

        Raises:
            è¯¥æ–¹æ³•å†…éƒ¨å¤„ç†æ‰€æœ‰å¼‚å¸¸ï¼Œä¸ä¼šå‘ä¸Šä¼ æ’­
        """
        logger.debug(f"ğŸ”„ å¼€å§‹æ‰¹é‡ä¼°å€¼è·å–: æ—¥æœŸ={date}, å› å­={factors}")

        try:
            if not hasattr(self, '_pending_batch_stocks') or not self._pending_batch_stocks:
                logger.debug("ğŸ“­ æ— å¾…å¤„ç†çš„æ‰¹é‡è¯·æ±‚ï¼Œè·³è¿‡")
                return

            stock_list = list(self._pending_batch_stocks)
            logger.info(f"ğŸš€ æ‰§è¡Œæ‰¹é‡ä¼°å€¼è·å–: {len(stock_list)} åªè‚¡ç¥¨ @ {date}")
            logger.debug(f"ğŸ“‹ è‚¡ç¥¨åˆ—è¡¨: {stock_list[:5]}{'...' if len(stock_list) > 5 else ''}")

            # æ‰§è¡Œæ‰¹é‡è·å–
            if rqdatac is not None:
                batch_result = rqdatac.get_factor(stock_list, factors, start_date=date, end_date=date)
            else:
                logger.error("âŒ rqdatacä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæ‰¹é‡ä¼°å€¼è·å–")
                return

            if batch_result is not None and not batch_result.empty:
                # ç¡®ä¿è¿”å›DataFrameæ ¼å¼
                if isinstance(batch_result, pd.Series):
                    batch_result = batch_result.to_frame().T
                elif not isinstance(batch_result, pd.DataFrame):
                    logger.warning(f"âš ï¸ æ‰¹é‡ä¼°å€¼æ•°æ®æ ¼å¼å¼‚å¸¸: {type(batch_result)}")
                    return

                # ç¼“å­˜æ‰¹é‡ç»“æœ
                cache_key = f"batch_valuation_{date}"
                self.batch_cache[cache_key] = batch_result

                logger.info(f"âœ… æ‰¹é‡ä¼°å€¼è·å–æˆåŠŸ: {batch_result.shape[0]} æ¡è®°å½•")
                logger.debug(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {batch_result.shape}, åˆ—: {list(batch_result.columns)}")
            else:
                logger.warning(f"âš ï¸ æ‰¹é‡ä¼°å€¼è·å–å¤±è´¥: è¿”å›ç©ºç»“æœ")
                logger.debug("ğŸ” æ£€æŸ¥: RQDatacè¿æ¥çŠ¶æ€å’Œå‚æ•°æœ‰æ•ˆæ€§")

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¼°å€¼è·å–å¼‚å¸¸: {e}")
            logger.debug("ğŸ” å¼‚å¸¸è¯¦æƒ…", exc_info=True)
            # ä¸å‘ä¸Šä¼ æ’­å¼‚å¸¸ï¼Œä¿æŒç³»ç»Ÿç¨³å®šæ€§

    def _extract_single_stock_from_batch(self, batch_data: pd.DataFrame, stock_code: str, date: str) -> Optional[pd.DataFrame]:
        """
        ä»æ‰¹é‡æ•°æ®ä¸­æå–å•ä¸ªè‚¡ç¥¨çš„æ•°æ®

        Args:
            batch_data: æ‰¹é‡æ•°æ®
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸ

        Returns:
            DataFrame: å•ä¸ªè‚¡ç¥¨çš„æ•°æ®
        """
        try:
            # æ‰¹é‡æ•°æ®çš„ç´¢å¼•æ˜¯ (è‚¡ç¥¨ä»£ç , æ—¥æœŸ) çš„å¤šé‡ç´¢å¼•
            if isinstance(batch_data.index, pd.MultiIndex):
                # æŸ¥æ‰¾å¯¹åº”çš„è‚¡ç¥¨å’Œæ—¥æœŸ
                mask = (batch_data.index.get_level_values(0) == stock_code)
                if date:
                    date_obj = pd.to_datetime(date)
                    mask = mask & (batch_data.index.get_level_values(1) == date_obj)

                if mask.any():
                    single_stock_data = batch_data[mask].copy()
                    # é‡ç½®ç´¢å¼•ï¼Œç§»é™¤å¤šé‡ç´¢å¼•
                    single_stock_data = single_stock_data.reset_index()
                    # åªä¿ç•™å› å­åˆ—
                    factor_columns = [col for col in single_stock_data.columns if col not in ['level_0', 'level_1', stock_code, date]]
                    if factor_columns:
                        single_stock_data = single_stock_data[factor_columns]
                    return single_stock_data

            return None

        except Exception as e:
            logger.error(f"âŒ ä»æ‰¹é‡æ•°æ®ä¸­æå–å•ä¸ªè‚¡ç¥¨å¤±è´¥ {stock_code}: {e}")
            return None

    def _should_use_previous_trading_date(self, trading_hours_str: str, current_time: datetime) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®

        é€»è¾‘ï¼š
        1. äº¤æ˜“å‰ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
        2. äº¤æ˜“ä¸­ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆå®æ—¶æ•°æ®ä¸ç¨³å®šï¼‰
        3. äº¤æ˜“å3å°æ—¶å†…ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆæ•°æ®å¯èƒ½è¿˜åœ¨æ›´æ–°ï¼‰
        4. äº¤æ˜“å3å°æ—¶åï¼šä½¿ç”¨å½“å¤©æ•°æ®

        Args:
            trading_hours_str: äº¤æ˜“æ—¶æ®µå­—ç¬¦ä¸²ï¼Œå¦‚ "09:31-11:30,13:01-15:00"
            current_time: å½“å‰æ—¶é—´

        Returns:
            æ˜¯å¦åº”è¯¥ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
        """
        try:
            if not trading_hours_str:
                return True

            current_time_only = current_time.time()

            # è§£æäº¤æ˜“æ—¶æ®µ
            periods = trading_hours_str.split(',')
            market_open_time = None
            market_close_time = None

            for period in periods:
                if '-' not in period:
                    continue

                start_str, end_str = period.split('-')
                try:
                    start_time = datetime.strptime(start_str.strip(), '%H:%M').time()
                    end_time = datetime.strptime(end_str.strip(), '%H:%M').time()

                    if market_open_time is None or start_time < market_open_time:
                        market_open_time = start_time
                    if market_close_time is None or end_time > market_close_time:
                        market_close_time = end_time
                except ValueError:
                    continue

            if market_open_time is None or market_close_time is None:
                logger.warning(f"âš ï¸ æ— æ³•è§£æäº¤æ˜“æ—¶æ®µ: {trading_hours_str}")
                return True

            # è®¡ç®—æ”¶ç›˜å3å°æ—¶çš„æ—¶é—´ç‚¹
            close_datetime = datetime.combine(current_time.date(), market_close_time)
            three_hours_after_close = close_datetime + timedelta(hours=3)
            three_hours_after_close_time = three_hours_after_close.time()

            logger.info(f"â° å¸‚åœºå¼€ç›˜: {market_open_time}, æ”¶ç›˜: {market_close_time}")
            logger.info(f"â° å½“å‰æ—¶é—´: {current_time_only}, æ”¶ç›˜å3å°æ—¶: {three_hours_after_close_time}")

            if current_time_only < market_open_time:
                # äº¤æ˜“å‰ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
                logger.info("â° å½“å‰åœ¨äº¤æ˜“å‰ï¼Œä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥")
                return True
            elif market_open_time <= current_time_only <= market_close_time:
                # äº¤æ˜“ä¸­ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆå®æ—¶æ•°æ®ä¸ç¨³å®šï¼‰
                logger.info("â° å½“å‰åœ¨äº¤æ˜“ä¸­ï¼Œä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆå®æ—¶æ•°æ®ä¸ç¨³å®šï¼‰")
                return True
            elif market_close_time < current_time_only <= three_hours_after_close_time:
                # æ”¶ç›˜å3å°æ—¶å†…ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆæ•°æ®å¯èƒ½è¿˜åœ¨æ›´æ–°ï¼‰
                logger.info("â° å½“å‰åœ¨æ”¶ç›˜å3å°æ—¶å†…ï¼Œä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆæ•°æ®å¯èƒ½è¿˜åœ¨æ›´æ–°ï¼‰")
                return True
            else:
                # æ”¶ç›˜å3å°æ—¶åï¼šä½¿ç”¨å½“å¤©æ•°æ®
                logger.info("â° å½“å‰åœ¨æ”¶ç›˜å3å°æ—¶åï¼Œä½¿ç”¨å½“å¤©æ•°æ®")
                return False

        except Exception as e:
            logger.warning(f"âš ï¸ åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶ä¿å®ˆä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥

    def _fallback_time_check(self, latest_date: datetime, now: datetime) -> None:
        """
        é™çº§æ—¶é—´æ£€æŸ¥é€»è¾‘ï¼ˆå½“è·å–äº¤æ˜“æ—¶æ®µå¤±è´¥æ—¶ä½¿ç”¨ï¼‰

        Args:
            latest_date: æœ€æ–°äº¤æ˜“æ—¥
            now: å½“å‰æ—¶é—´
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´å†…
        current_time = now.time()
        # å‡è®¾Aè‚¡æ”¶ç›˜æ—¶é—´æ˜¯15:00
        market_close_time = datetime.strptime('15:00', '%H:%M').time()

        if current_time < market_close_time:
            # å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ä¸”æœªæ”¶ç›˜ï¼Œæ•°æ®å¿…ç„¶æ— æ•ˆ
            logger.info("â° å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ä¸”æœªæ”¶ç›˜ï¼Œæœ€æ–°äº¤æ˜“æ—¥æ•°æ®å¿…ç„¶æ— æ•ˆ")

            # è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥
            previous_date = rqdatac.get_previous_trading_date(latest_date)  # type: ignore
            if previous_date:
                self.target_trading_date = previous_date.strftime('%Y-%m-%d')
                logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (å‰ä¸€ä¸ªäº¤æ˜“æ—¥)")
            else:
                # å¦‚æœæ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
                self.target_trading_date = latest_date.strftime('%Y-%m-%d')
                logger.warning(f"âš ï¸ æ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥: {self.target_trading_date}")
        else:
            # å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ä½†å·²æ”¶ç›˜ï¼Œæ•°æ®å¯èƒ½æœ‰æ•ˆ
            logger.info("â° å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ä¸”å·²æ”¶ç›˜ï¼Œæœ€æ–°äº¤æ˜“æ—¥æ•°æ®å¯èƒ½æœ‰æ•ˆ")
            self.target_trading_date = latest_date.strftime('%Y-%m-%d')
            logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (æœ€æ–°äº¤æ˜“æ—¥)")

    def _get_previous_trading_date(self, stock_code: str, max_days_back: int = 30) -> Optional[str]:
        """
        ä½¿ç”¨RQDatac APIè·å–æœ€åä¸€ä¸ªå·²å®Œæˆäº¤æ˜“æ—¥çš„æ•°æ®
        æ™ºèƒ½åˆ¤æ–­ï¼šä¼˜å…ˆä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥ï¼Œå¦‚æœæ•°æ®æ— æ•ˆåˆ™ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_days_back: æœ€å¤§å›æº¯å¤©æ•°

        Returns:
            æœ€åä¸€ä¸ªå·²å®Œæˆäº¤æ˜“æ—¥çš„æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD) æˆ– None
        """
        try:
            # å®šä¹‰æ ¸å¿ƒå­—æ®µ
            core_fields = ['pe_ratio', 'pb_ratio', 'market_cap']

            # ä½¿ç”¨RQDatac APIè·å–æœ€æ–°äº¤æ˜“æ—¥
            latest_date = rqdatac.get_latest_trading_date()  # type: ignore
            if not latest_date:
                logger.warning("âš ï¸ æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥")
                return None

            logger.debug(f"ğŸ“… æœ€æ–°äº¤æ˜“æ—¥: {latest_date}")

            # æ£€æŸ¥æœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            valuation_df = self._fetch_data('valuation', stock_code, date=latest_date.strftime('%Y-%m-%d'))

            if valuation_df is not None and not valuation_df.empty:
                # æ£€æŸ¥æ ¸å¿ƒå­—æ®µæ˜¯å¦æœ‰æ•ˆï¼ˆéNaNï¼‰
                has_valid_data = False

                for field in core_fields:
                    if field in valuation_df.columns and not valuation_df[field].isna().all():
                        has_valid_data = True
                        break

                if has_valid_data:
                    logger.debug(f"âœ… æœ€æ–°äº¤æ˜“æ—¥ {latest_date} æ•°æ®æœ‰æ•ˆ")
                    return latest_date.strftime('%Y-%m-%d')

            # å¦‚æœæœ€æ–°äº¤æ˜“æ—¥æ•°æ®æ— æ•ˆï¼Œå°è¯•å‰ä¸€ä¸ªäº¤æ˜“æ—¥
            logger.debug(f"âš ï¸ æœ€æ–°äº¤æ˜“æ—¥ {latest_date} æ•°æ®æ— æ•ˆï¼Œå°è¯•å‰ä¸€ä¸ªäº¤æ˜“æ—¥")

            # ä½¿ç”¨RQDatac APIè·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥
            previous_date = rqdatac.get_previous_trading_date(latest_date)  # type: ignore
            if not previous_date:
                logger.warning("âš ï¸ æ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥")
                return None

            logger.debug(f"ğŸ“… å‰ä¸€ä¸ªäº¤æ˜“æ—¥: {previous_date}")

            # æ£€æŸ¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            prev_valuation_df = self._fetch_data('valuation', stock_code, date=previous_date.strftime('%Y-%m-%d'))

            if prev_valuation_df is not None and not prev_valuation_df.empty:
                # æ£€æŸ¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                has_prev_valid_data = False

                for field in core_fields:
                    if field in prev_valuation_df.columns and not prev_valuation_df[field].isna().all():
                        has_prev_valid_data = True
                        break

                if has_prev_valid_data:
                    logger.debug(f"âœ… å‰ä¸€ä¸ªäº¤æ˜“æ—¥ {previous_date} æ•°æ®æœ‰æ•ˆ")
                    return previous_date.strftime('%Y-%m-%d')

            logger.warning(f"âš ï¸ å‰ä¸€ä¸ªäº¤æ˜“æ—¥ {previous_date} æ•°æ®ä¹Ÿä¸å¯ç”¨")
            return None

        except Exception as e:
            logger.error(f"âŒ è·å–æœ€åä¸€ä¸ªå·²å®Œæˆäº¤æ˜“æ—¥å¤±è´¥ {stock_code}: {e}")
            return None

    def get_smart_data(self, stock_code: str, data_type: str, max_retries: int = 5) -> Optional[pd.DataFrame]:
        """
        æ™ºèƒ½æ•°æ®è·å–å‡½æ•°
        ä½¿ç”¨é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥ï¼Œé¿å…å¯¹æ¯åªè‚¡ç¥¨é‡å¤æ£€æŸ¥
        ä¼˜åŒ–ç‰ˆï¼šå…¨å±€åˆ¤æ–­ä¸€æ¬¡ï¼Œåç»­æ‰€æœ‰è‚¡ç¥¨éƒ½ä½¿ç”¨ç›¸åŒçš„ç»“æœ

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹ ('valuation', 'price')
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            DataFrame: åŒ…å«å®Œæ•´æ•°æ®çš„DataFrameæˆ–None
        """
        try:
            # ä½¿ç”¨é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥ï¼Œé¿å…é‡å¤æ£€æŸ¥
            target_date = self.get_target_trading_date()
            if not target_date:
                logger.warning(f"âš ï¸ æ— æ³•ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥")
                return None

            logger.debug(f"ğŸ¯ ä½¿ç”¨é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥: {target_date}")

            # æ ¹æ®æ•°æ®ç±»å‹è·å–æ•°æ®
            if data_type == 'valuation':
                valuation_df = self._fetch_data('valuation', stock_code, date=target_date)
                if valuation_df is not None and not valuation_df.empty:
                    # éªŒè¯æ•°æ®å®Œæ•´æ€§
                    if self._validate_valuation_completeness(valuation_df):
                        logger.debug(f"âœ… è·å–ä¼°å€¼æ•°æ®æˆåŠŸ: {stock_code} @ {target_date}")
                        return valuation_df
                    else:
                        logger.warning(f"âš ï¸ {stock_code} @ {target_date} ä¼°å€¼æ•°æ®ä¸å®Œæ•´")
                        return None
                else:
                    logger.warning(f"âš ï¸ {stock_code} @ {target_date} æœªè·å–åˆ°ä¼°å€¼æ•°æ®")
                    return None

            elif data_type == 'price':
                # è·å–ä»·æ ¼æ•°æ®éœ€è¦æ›´å¤šå‚æ•° - å¢åŠ åˆ°90å¤©ä»¥ç¡®ä¿æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæ•´æ€§
                start_date = (datetime.strptime(target_date, '%Y-%m-%d') - timedelta(days=90)).strftime('%Y-%m-%d')
                price_df = self._fetch_data('price', stock_code, start_date=start_date, end_date=target_date)
                if price_df is not None and not price_df.empty:
                    # ç¡®ä¿æ•°æ®å®Œæ•´æ€§
                    if self._validate_dataframe_completeness(price_df, data_type):
                        logger.debug(f"âœ… è·å–ä»·æ ¼æ•°æ®æˆåŠŸ: {stock_code}, {len(price_df)}æ¡è®°å½•")
                        return price_df
                    else:
                        logger.warning(f"âš ï¸ {stock_code} ä»·æ ¼æ•°æ®ä¸å®Œæ•´")
                        return None
                else:
                    logger.warning(f"âš ï¸ {stock_code} æœªè·å–åˆ°ä»·æ ¼æ•°æ®")
                    return None

            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
                return None

        except Exception as e:
            logger.error(f"âŒ è·å–æ™ºèƒ½æ•°æ®å¤±è´¥ {stock_code} ({data_type}): {e}")
            return None

    def _fetch_data(self, data_type: str, stock_code: str, **kwargs) -> Optional[pd.DataFrame]:
        """
        ç»Ÿä¸€çš„ç§æœ‰æ•°æ®è·å–æ–¹æ³•

        Args:
            data_type: æ•°æ®ç±»å‹ ('price', 'valuation', 'instruments')
            stock_code: è‚¡ç¥¨ä»£ç 
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            DataFrame: è¯·æ±‚çš„æ•°æ®æˆ–None
        """
        if data_type == 'price':
            start_date = kwargs.get('start_date')
            end_date = kwargs.get('end_date')
            force_refresh = kwargs.get('force_refresh', False)
            if start_date and end_date:
                return self._fetch_price_series(stock_code, start_date, end_date, force_refresh)
            else:
                logger.error("âŒ è·å–ä»·æ ¼æ•°æ®éœ€è¦ start_date å’Œ end_date å‚æ•°")
                return None
        elif data_type == 'valuation':
            date = kwargs.get('date')
            return self._fetch_valuation_series(stock_code, date)
        elif data_type == 'instruments':
            return self._fetch_instruments_info(stock_code)
        else:
            logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
            return None

    def _validate_dataframe_completeness(self, df: pd.DataFrame, data_type: str) -> bool:
        """
        éªŒè¯DataFrameæ•°æ®å®Œæ•´æ€§

        Args:
            df: è¦éªŒè¯çš„DataFrame
            data_type: æ•°æ®ç±»å‹

        Returns:
            bool: æ•°æ®æ˜¯å¦å®Œæ•´
        """
        try:
            if df is None or df.empty:
                return False

            if data_type == 'valuation':
                required_fields = ['pe_ratio', 'pb_ratio', 'market_cap']
                # æ£€æŸ¥æœ€æ–°ä¸€è¡Œæ•°æ®æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µä¸”ä¸ä¸ºNaN
                latest_row = df.iloc[-1] if len(df) > 0 else None
                if latest_row is None:
                    return False

                return all(
                    field in latest_row.index and
                    latest_row[field] is not None and
                    not pd.isna(latest_row[field])
                    for field in required_fields
                )

            elif data_type == 'price':
                required_fields = ['open', 'close', 'high', 'low', 'volume']
                # æ£€æŸ¥æ‰€æœ‰è¡Œæ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µä¸”ä»·æ ¼å¤§äº0
                for _, row in df.iterrows():
                    if not all(
                        field in row.index and
                        row[field] is not None and
                        not pd.isna(row[field]) and
                        (field != 'volume' or row[field] >= 0) and  # volumeå¯ä»¥ä¸º0
                        (field == 'volume' or row[field] > 0)      # ä»·æ ¼å¿…é¡»å¤§äº0
                        for field in required_fields
                    ):
                        return False
                return True

            return False

        except Exception as e:
            logger.error(f"DataFrameæ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
            return False

    def _validate_valuation_completeness(self, df: pd.DataFrame) -> bool:
        """
        æ”¹è¿›çš„ä¼°å€¼æ•°æ®å®Œæ•´æ€§éªŒè¯
        å…è®¸éƒ¨åˆ†å­—æ®µç¼ºå¤±ï¼Œåªè¦æœ‰æ ¸å¿ƒå­—æ®µå³å¯

        Args:
            df: ä¼°å€¼æ•°æ®DataFrame

        Returns:
            bool: æ•°æ®æ˜¯å¦å®Œæ•´
        """
        if df is None or df.empty:
            return False

        # æ ¸å¿ƒå­—æ®µ - è‡³å°‘éœ€è¦è¿™äº›å­—æ®µä¸­çš„ä¸€ä¸ª
        core_fields = ['pe_ratio', 'pb_ratio', 'market_cap']
        available_core_fields = [field for field in core_fields if field in df.columns]

        if not available_core_fields:
            logger.warning(f"âš ï¸ ä¼°å€¼æ•°æ®ç¼ºå°‘æ‰€æœ‰æ ¸å¿ƒå­—æ®µ: {core_fields}")
            return False

        # æ£€æŸ¥æ ¸å¿ƒå­—æ®µæ˜¯å¦æœ‰æœ‰æ•ˆå€¼
        valid_values = 0
        for field in available_core_fields:
            if not df[field].isna().all():
                valid_values += 1

        # è‡³å°‘æœ‰ä¸€ä¸ªæ ¸å¿ƒå­—æ®µæœ‰æœ‰æ•ˆå€¼
        if valid_values == 0:
            logger.warning(f"âš ï¸ ä¼°å€¼æ•°æ®æ ¸å¿ƒå­—æ®µå…¨éƒ¨ä¸ºNaN: {available_core_fields}")
            return False

        logger.debug(f"âœ… ä¼°å€¼æ•°æ®éªŒè¯é€šè¿‡: æ ¸å¿ƒå­—æ®µ {available_core_fields}, æœ‰æ•ˆå€¼ {valid_values}")
        return True

    def _safe_to_numpy(self, series) -> np.ndarray:
        """
        å®‰å…¨åœ°å°†pandas Seriesæˆ–ArrayLikeè½¬æ¢ä¸ºnumpyæ•°ç»„

        Args:
            series: pandas Seriesæˆ–ArrayLikeå¯¹è±¡

        Returns:
            numpyæ•°ç»„
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„pandas Serieså’ŒArrayLike
            if hasattr(series, 'values'):
                return np.array(series.values, dtype=float)
            elif hasattr(series, 'to_numpy'):
                return series.to_numpy().astype(float)
            else:
                return np.array(series, dtype=float)
        except Exception as e:
            logger.warning(f"è½¬æ¢åˆ°numpyæ•°ç»„å¤±è´¥: {e}")
            return np.array([], dtype=float)

import talib

# å¯¼å…¥åŒæºè®¡ç®—ç›¸å…³çš„è¾…åŠ©å‡½æ•°
from modules.data_formats import (
    get_direct_available_fields,
    get_unavailable_rqdatac_fields,
    get_indicator_calculation_function,
    get_rqdatac_api_field_names
)

# ============================================================================
# STOCK POOL INDICATOR ENGINE - æŠ€æœ¯æŒ‡æ ‡å¼•æ“
# ============================================================================

class StockPoolIndicatorEngine:
    """
    StockPoolä¸“ç”¨æŠ€æœ¯æŒ‡æ ‡å¼•æ“
    - åŸºäºç»Ÿä¸€é…ç½®ç³»ç»Ÿé©±åŠ¨
    - æä¾›ç»Ÿä¸€çš„æŒ‡æ ‡è®¡ç®—æ¥å£
    - æ”¯æŒæŒ‰éœ€æŒ‡æ ‡è®¡ç®—
    """

    def _safe_to_numpy(self, series) -> np.ndarray:
        """
        å®‰å…¨åœ°å°†pandas Seriesæˆ–ArrayLikeè½¬æ¢ä¸ºnumpyæ•°ç»„

        Args:
            series: pandas Seriesæˆ–ArrayLikeå¯¹è±¡

        Returns:
            numpyæ•°ç»„
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„pandas Serieså’ŒArrayLike
            if hasattr(series, 'values'):
                return np.array(series.values, dtype=float)
            elif hasattr(series, 'to_numpy'):
                return series.to_numpy().astype(float)
            else:
                return np.array(series, dtype=float)
        except Exception as e:
            logger.warning(f"è½¬æ¢åˆ°numpyæ•°ç»„å¤±è´¥: {e}")
            return np.array([], dtype=float)

    def __init__(self, data_store: StockPoolDataStore):
        """åˆå§‹åŒ–æŒ‡æ ‡å¼•æ“"""
        self.data_store = data_store
        self.calculation_stats = {
            'total_calculations': 0,
            'successful_calculations': 0,
            'errors': 0
        }

        # æŒ‡æ ‡ç¼“å­˜ - æç®€è®¾è®¡ï¼Œä¸kline_cacheä¿æŒä¸€è‡´
        self.indicator_cache: Dict[str, pd.DataFrame] = {}  # {order_book_id: indicators_df}

        logger.info("ğŸ“ˆ StockPoolIndicatorEngineåˆå§‹åŒ–å®Œæˆ")





    def get_indicator_stats(self) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        stats: Dict[str, Any] = {}
        calculation_stats = self.calculation_stats.copy()

        # å¤åˆ¶åŸºç¡€ç»Ÿè®¡æ•°æ®
        for key, value in calculation_stats.items():
            stats[key] = value

        total = stats['total_calculations']
        if total > 0:
            stats['success_rate'] = float(stats['successful_calculations']) / total
            stats['error_rate'] = float(stats['errors']) / total
        else:
            stats['success_rate'] = 0.0
            stats['error_rate'] = 0.0

        # æ·»åŠ ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        stats['cache_size'] = len(self.indicator_cache)
        stats['cache_keys'] = list(self.indicator_cache.keys())  # ç°åœ¨æ˜¯è‚¡ç¥¨ä»£ç åˆ—è¡¨

        return stats

    def clear_indicator_cache(self, stock_code: Optional[str] = None) -> int:
        """
        æ¸…é™¤æŒ‡æ ‡ç¼“å­˜

        Args:
            stock_code: æŒ‡å®šè‚¡ç¥¨ä»£ç ï¼Œåªæ¸…é™¤è¯¥è‚¡ç¥¨çš„ç¼“å­˜ï¼›å¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜

        Returns:
            int: æ¸…é™¤çš„ç¼“å­˜æ¡ç›®æ•°é‡
        """
        try:
            cleared_count = 0

            if stock_code:
                # åªæ¸…é™¤æŒ‡å®šè‚¡ç¥¨çš„ç¼“å­˜
                if stock_code in self.indicator_cache:
                    del self.indicator_cache[stock_code]
                    cleared_count += 1
                
                # æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„order_book_id
                keys_to_remove = []
                for cache_key, cached_df in self.indicator_cache.items():
                    if ('order_book_id' in cached_df.columns and 
                        cached_df['order_book_id'].iloc[0] == stock_code):
                        keys_to_remove.append(cache_key)
                
                for key in keys_to_remove:
                    del self.indicator_cache[key]
                    cleared_count += 1
                    
                if cleared_count == 0:
                    logger.debug(f"â„¹ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨{stock_code}çš„æŒ‡æ ‡ç¼“å­˜")
            else:
                # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
                cleared_count = len(self.indicator_cache)
                self.indicator_cache.clear()

            if cleared_count > 0:
                logger.info(f"ğŸ§¹ æ¸…é™¤æŒ‡æ ‡ç¼“å­˜: {cleared_count} æ¡è®°å½•")
            else:
                logger.debug("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…é™¤çš„ç¼“å­˜")

            return cleared_count

        except Exception as e:
            logger.error(f"âŒ æ¸…é™¤æŒ‡æ ‡ç¼“å­˜å¤±è´¥: {e}")
            return 0

    def calculate_all_indicators(self, kline_data: pd.DataFrame, stock_code: Optional[str] = None,
                                force_refresh: bool = False,
                                requested_indicators: Optional[List[str]] = None) -> Dict:
        """
        è®¡ç®—è‚¡ç¥¨çš„æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡ï¼ˆä½¿ç”¨åŒæºè®¡ç®—æ¶æ„ï¼‰

        åŒæºè®¡ç®—æµç¨‹:
        1. ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å‡ºå¯ä»¥ä»RQDatacç›´æ¥è·å–çš„å­—æ®µ
        2. ç¬¬äºŒæ­¥ï¼šå¯¹äºéœ€è¦è®¡ç®—çš„å­—æ®µï¼Œä½¿ç”¨æœ¬åœ°è®¡ç®—å‡½æ•°
        3. ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶ä¸¤ä¸ªæ¥æºçš„æŒ‡æ ‡æ•°æ®

        ç¼“å­˜ç­–ç•¥:
        - å†…å­˜ç¼“å­˜ï¼šå­˜å‚¨å®Œæ•´çš„æŒ‡æ ‡DataFrameç”¨äºåç»­è®¡ç®—
        - ç¼“å­˜é”®ï¼šåŸºäºè‚¡ç¥¨ä»£ç å’Œæ•°æ®æ—¶é—´èŒƒå›´ç”Ÿæˆ
        - æ”¯æŒå¼ºåˆ¶åˆ·æ–°å‚æ•°
        - é¿å…é‡å¤è®¡ç®—ï¼Œæé«˜æ€§èƒ½

        Args:
            kline_data: Kçº¿æ•°æ® (åŒ…å«OHLCV)
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºæ—¥å¿—å’Œç¼“å­˜ï¼‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—æŒ‡æ ‡

        Returns:
            Dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        if kline_data is None or kline_data.empty:
            return {}

        try:
            # è®°å½•è®¡ç®—ç»Ÿè®¡
            self.calculation_stats['total_calculations'] += 1

            # ç”Ÿæˆç¼“å­˜é”®ï¼ˆä¼˜åŒ–è®¾è®¡ï¼šä½¿ç”¨order_book_idï¼‰
            cache_key = None
            if kline_data is not None and not kline_data.empty and 'order_book_id' in kline_data.columns:
                # ä»klineæ•°æ®ä¸­æå–è§„èŒƒåŒ–çš„order_book_idä½œä¸ºç¼“å­˜é”®
                cache_key = kline_data['order_book_id'].iloc[0]
                logger.debug(f"ğŸ“Š æŒ‡æ ‡ç¼“å­˜é”®: {cache_key}")
            elif stock_code:
                # é™çº§ä½¿ç”¨ä¼ å…¥çš„stock_codeï¼ˆå‘åå…¼å®¹ï¼‰
                cache_key = stock_code
                logger.debug(f"ğŸ“Š ä½¿ç”¨ä¼ å…¥çš„è‚¡ç¥¨ä»£ç ä½œä¸ºç¼“å­˜é”®: {stock_code}")

            # æ£€æŸ¥ç¼“å­˜ï¼ˆä»…å½“ä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ä¸”æœ‰æœ‰æ•ˆçš„ç¼“å­˜é”®æ—¶ï¼‰
            if not force_refresh and cache_key:
                # é¦–å…ˆå°è¯•ä½¿ç”¨è§„èŒƒåŒ–çš„ç¼“å­˜é”®æŸ¥æ‰¾
                if cache_key in self.indicator_cache:
                    cached_df = self.indicator_cache[cache_key]

                    # éªŒè¯ç¼“å­˜æ•°æ®çš„æ­£ç¡®æ€§
                    if 'order_book_id' in cached_df.columns and cached_df['order_book_id'].iloc[0] == cache_key:
                        logger.debug(f"âœ… æŒ‡æ ‡ç¼“å­˜å‘½ä¸­: {cache_key}")
                        # ä»ç¼“å­˜çš„DataFrameé‡å»ºç»“æœ
                        latest_values = {}
                        for col in cached_df.columns:
                            if col != 'order_book_id':  # æ’é™¤order_book_idåˆ—
                                latest_val = cached_df[col].iloc[-1] if not cached_df[col].isna().all() else None
                                if latest_val is not None and not pd.isna(latest_val):
                                    latest_values[col] = float(latest_val)

                        result = {
                            'indicators_df': cached_df,
                            'latest_values': latest_values,
                            'calculation_stats': self.calculation_stats.copy(),
                            'metadata': {
                                'stock_code': stock_code,
                                'data_points': len(kline_data),
                                'indicators_count': len(cached_df.columns) - 1,  # æ’é™¤order_book_idåˆ—
                                'calculation_time': datetime.now().isoformat(),
                                'cached': True,
                                'errors': []
                            }
                        }
                        return result
                    else:
                        # ç¼“å­˜æ•°æ®ä¸åŒ¹é…ï¼Œæ¸…é™¤æ— æ•ˆç¼“å­˜
                        del self.indicator_cache[cache_key]
                        logger.debug(f"ğŸ§¹ æ¸…é™¤æ— æ•ˆæŒ‡æ ‡ç¼“å­˜: {cache_key}")

                # å¦‚æœè§„èŒƒåŒ–é”®æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„order_book_id
                for existing_key, cached_df in self.indicator_cache.items():
                    if ('order_book_id' in cached_df.columns and
                        cached_df['order_book_id'].iloc[0] == cache_key):
                        logger.debug(f"âœ… æŒ‡æ ‡ç¼“å­˜å‘½ä¸­ (é€šè¿‡order_book_idåŒ¹é…): {cache_key}")
                        # ä»ç¼“å­˜çš„DataFrameé‡å»ºç»“æœ
                        latest_values = {}
                        for col in cached_df.columns:
                            if col != 'order_book_id':  # æ’é™¤order_book_idåˆ—
                                latest_val = cached_df[col].iloc[-1] if not cached_df[col].isna().all() else None
                                if latest_val is not None and not pd.isna(latest_val):
                                    latest_values[col] = float(latest_val)

                        result = {
                            'indicators_df': cached_df,
                            'latest_values': latest_values,
                            'calculation_stats': self.calculation_stats.copy(),
                            'metadata': {
                                'stock_code': stock_code,
                                'data_points': len(kline_data),
                                'indicators_count': len(cached_df.columns) - 1,  # æ’é™¤order_book_idåˆ—
                                'calculation_time': datetime.now().isoformat(),
                                'cached': True,
                                'errors': []
                            }
                        }
                        return result

            # ç¼“å­˜æœªå‘½ä¸­æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œå¼€å§‹è®¡ç®—
            logger.debug(f"ğŸ”„ ä½¿ç”¨åŒæºè®¡ç®—æ¶æ„è®¡ç®—æŠ€æœ¯æŒ‡æ ‡: {stock_code}")

            # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
            if not isinstance(kline_data.index, pd.DatetimeIndex):
                # å¤„ç†MultiIndexçš„æƒ…å†µï¼Œæå–æ—¶é—´éƒ¨åˆ†ä½œä¸ºæ–°ç´¢å¼•
                if isinstance(kline_data.index, pd.MultiIndex):
                    kline_data = kline_data.reset_index(level=0, drop=True)
                else:
                    kline_data.index = pd.to_datetime(kline_data.index)

            # æŒ‰æ—¶é—´æ’åº
            kline_data = kline_data.sort_index()

            # å¤„ç†æŒ‡æ ‡åˆ—è¡¨ï¼šå¦‚æœæœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨é»˜è®¤æŒ‡æ ‡é›†åˆ
            if requested_indicators is None:
                # é»˜è®¤æŒ‡æ ‡é›†åˆï¼ˆä¸šåŠ¡å±‚å†³å®šçš„æ ‡å‡†æŒ‡æ ‡é›†ï¼‰
                requested_indicators = [
                    # SMAç³»åˆ—
                    'SMA_5', 'SMA_10', 'SMA_20', 'SMA_30', 'SMA_60',
                    # EMAç³»åˆ—
                    'EMA_5', 'EMA_10', 'EMA_12', 'EMA_20', 'EMA_26', 'EMA_30', 'EMA_60',
                    # RSIç³»åˆ—
                    'RSI_6', 'RSI_14', 'RSI_21',
                    # MACDç³»åˆ—
                    'MACD', 'MACD_SIGNAL', 'MACD_HIST',
                    # å¸ƒæ—å¸¦
                    'BB_UPPER', 'BB_MIDDLE', 'BB_LOWER',
                    # ATRç³»åˆ—
                    'ATR_7', 'ATR_14', 'ATR_21',
                    # éšæœºæŒ‡æ ‡
                    'STOCH_K', 'STOCH_D',
                    # CCIç³»åˆ—
                    'CCI_14', 'CCI_20',
                    # ROCç³»åˆ—
                    'ROC_10', 'ROC_12', 'ROC_20',
                    # TEMAç³»åˆ—
                    'TEMA_20', 'TEMA_30',
                    # WMAç³»åˆ—
                    'WMA_10', 'WMA_20', 'WMA_30',
                    # DMIç³»åˆ—
                    'PLUS_DI', 'MINUS_DI', 'ADX',
                    # å…¶ä»–æŒ‡æ ‡
                    'OBV', 'VOLUME_SMA_5', 'VOLUME_SMA_10', 'VOLUME_SMA_20',
                    'MFI', 'WILLR', 'VOLATILITY'
                ]

            # ä½¿ç”¨åŒæºè®¡ç®—æ¶æ„
            indicators_result = self._calculate_indicators_dual_source(kline_data, requested_indicators, stock_code)

            if not indicators_result:
                logger.warning(f"âš ï¸ {stock_code}: åŒæºæŒ‡æ ‡è®¡ç®—å¤±è´¥")
                return {}

            # è·å–ç»“æœ
            result_df = indicators_result.get('indicators_df', pd.DataFrame())
            calculation_errors = indicators_result.get('errors', [])

            # æ·»åŠ order_book_idåˆ—ç”¨äºç¼“å­˜éªŒè¯ï¼ˆä¸kline_cacheä¿æŒä¸€è‡´ï¼‰
            if 'order_book_id' in kline_data.columns:
                result_df['order_book_id'] = kline_data['order_book_id']
                logger.debug(f"ğŸ“Š æŒ‡æ ‡DataFrameæ·»åŠ order_book_idåˆ—: {kline_data['order_book_id'].iloc[0]}")

            # å¤„ç†é•¿åº¦ä¸åŒ¹é…çš„é—®é¢˜ - æŸäº›æŒ‡æ ‡å¯èƒ½äº§ç”Ÿä¸åŒé•¿åº¦
            if len(result_df) != len(kline_data):
                logger.debug(f"ğŸ”§ å¯¹é½æŒ‡æ ‡æ•°æ®é•¿åº¦: {len(result_df)} -> {len(kline_data)}")
                # é‡æ–°ç´¢å¼•ä»¥åŒ¹é…åŸå§‹æ•°æ®çš„é•¿åº¦
                result_df = result_df.reindex(kline_data.index)

            # ç¼“å­˜è®¡ç®—ç»“æœï¼ˆå¦‚æœæœ‰æœ‰æ•ˆçš„ç¼“å­˜é”®ï¼‰
            if cache_key:
                self.indicator_cache[cache_key] = result_df.copy()
                logger.debug(f"ğŸ’¾ æŒ‡æ ‡æ•°æ®å·²ç¼“å­˜: {cache_key}")

            # è·å–æœ€æ–°å€¼
            latest_values = {}
            for col in result_df.columns:
                if col != 'order_book_id':  # æ’é™¤order_book_idåˆ—
                    latest_val = result_df[col].iloc[-1] if not result_df[col].isna().all() else None
                    if latest_val is not None and not pd.isna(latest_val):
                        latest_values[col] = float(latest_val)

            # æ„å»ºè¿”å›ç»“æœ
            result = {
                'indicators_df': result_df,
                'latest_values': latest_values,
                'calculation_stats': self.calculation_stats.copy(),
                'metadata': {
                    'stock_code': stock_code,
                    'data_points': len(kline_data),
                    'indicators_count': len(result_df.columns) - (1 if 'order_book_id' in result_df.columns else 0),
                    'calculation_time': datetime.now().isoformat(),
                    'cached': False,
                    'errors': calculation_errors,
                    'calculation_method': 'dual_source'
                }
            }

            self.calculation_stats['successful_calculations'] += 1
            logger.debug(f"âœ… åŒæºæŒ‡æ ‡è®¡ç®—å®Œæˆ: {stock_code}, {len(result_df.columns)}ä¸ªæŒ‡æ ‡")

            return result

        except Exception as e:
            self.calculation_stats['errors'] += 1
            logger.error(f"âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}

    def _calculate_indicators_dual_source(self, kline_data: pd.DataFrame,
                                        requested_indicators: List[str],
                                        stock_code: Optional[str] = None) -> Dict:
        """
        åŒæºæŒ‡æ ‡è®¡ç®—ï¼šç¬¬ä¸€æ­¥ä»RQDatacè·å–ï¼Œç¬¬äºŒæ­¥è®¡ç®—å‰©ä½™æŒ‡æ ‡

        è®¾è®¡ç†å¿µï¼š
        - ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å‡ºå¯ä»¥ç›´æ¥ä»RQDatacè·å–çš„å­—æ®µï¼Œæ‰¹é‡è°ƒç”¨API
        - ç¬¬äºŒæ­¥ï¼šå¯¹äºéœ€è¦è®¡ç®—çš„å­—æ®µï¼Œä½¿ç”¨æœ¬åœ°è®¡ç®—å‡½æ•°
        - æœ€åï¼šåˆå¹¶ä¸¤ä¸ªæ¥æºçš„æŒ‡æ ‡æ•°æ®

        Args:
            kline_data: Kçº¿æ•°æ® (åŒ…å«OHLCV)
            requested_indicators: éœ€è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨ï¼ˆå¦‚ ['SMA_5', 'EMA_12', 'MACD']ï¼‰
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            Dict: åŒ…å«å®Œæ•´æŠ€æœ¯æŒ‡æ ‡çš„å­—å…¸
        """
        if kline_data is None or kline_data.empty:
            return {}

        try:
            logger.debug(f"ğŸ”„ åŒæºæŒ‡æ ‡è®¡ç®—å¼€å§‹: {stock_code or 'unknown'}")

            # ä½¿ç”¨ä¼ å…¥çš„æŒ‡æ ‡åˆ—è¡¨
            logger.debug(f"ğŸ“Š è¯·æ±‚è®¡ç®—æŒ‡æ ‡: {len(requested_indicators)} ä¸ª")
            logger.debug(f"ğŸ“‹ æŒ‡æ ‡åˆ—è¡¨: {requested_indicators[:10]}{'...' if len(requested_indicators) > 10 else ''}")

            # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å‡ºå¯ä»¥ä»RQDatacç›´æ¥è·å–çš„å­—æ®µ
            rqdatac_available_fields = []
            computation_required_fields = []

            for indicator in requested_indicators:
                if indicator in get_direct_available_fields():
                    rqdatac_available_fields.append(indicator)
                else:
                    computation_required_fields.append(indicator)

            logger.debug(f"ğŸ“Š RQDatacå¯ç”¨å­—æ®µ: {len(rqdatac_available_fields)}")
            logger.debug(f"ğŸ§® éœ€è¦è®¡ç®—å­—æ®µ: {len(computation_required_fields)}")

            # åˆå§‹åŒ–ç»“æœå®¹å™¨
            all_indicators = {}
            calculation_errors = []

            # ç¬¬äºŒæ­¥ï¼šä»RQDatacè·å–å¯ç”¨æŒ‡æ ‡
            if rqdatac_available_fields:
                try:
                    logger.debug(f"ğŸŒ ä»RQDatacè·å– {len(rqdatac_available_fields)} ä¸ªæŒ‡æ ‡...")

                    # å°†å†…éƒ¨å­—æ®µåè½¬æ¢ä¸ºAPIå­—æ®µå
                    api_field_names = get_rqdatac_api_field_names(rqdatac_available_fields)

                    # è¿™é‡Œåº”è¯¥è°ƒç”¨RQDatac APIè·å–æ•°æ®
                    # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…å®ç°éœ€è¦é›†æˆRQDatac
                    for i, field in enumerate(rqdatac_available_fields):
                        api_field = api_field_names[i]
                        # æ¨¡æ‹Ÿä»RQDatacè·å–çš„æ•°æ®
                        mock_data = pd.Series([None] * len(kline_data),
                                            index=kline_data.index,
                                            name=field)
                        all_indicators[field] = mock_data

                    logger.debug(f"âœ… ä»RQDatacè·å–äº† {len(rqdatac_available_fields)} ä¸ªæŒ‡æ ‡")

                except Exception as e:
                    logger.warning(f"âš ï¸ RQDatacè·å–å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°è®¡ç®—: {e}")
                    # å¦‚æœRQDatacå¤±è´¥ï¼Œå°†è¿™äº›å­—æ®µåŠ å…¥è®¡ç®—é˜Ÿåˆ—
                    computation_required_fields.extend(rqdatac_available_fields)
                    rqdatac_available_fields = []

            # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—éœ€è¦è‡ªå·±è®¡ç®—çš„æŒ‡æ ‡
            if computation_required_fields:
                try:
                    logger.debug(f"ğŸ§® æœ¬åœ°è®¡ç®— {len(computation_required_fields)} ä¸ªæŒ‡æ ‡...")

                    for indicator_name in computation_required_fields:
                        try:
                            # è·å–è®¡ç®—å‡½æ•°
                            calc_function = get_indicator_calculation_function(indicator_name)

                            if calc_function:
                                try:
                                    # ç»Ÿä¸€æ¶æ„ï¼šæ‰€æœ‰æŒ‡æ ‡éƒ½é€šè¿‡é…ç½®é©±åŠ¨ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†
                                    result = calc_function(kline_data)
                                    if result is not None:
                                        all_indicators[indicator_name] = result
                                except Exception as e:
                                    calculation_errors.append(f"{indicator_name} è®¡ç®—å¤±è´¥: {e}")
                                    logger.debug(f"âš ï¸ æŒ‡æ ‡è®¡ç®—å¤±è´¥ {indicator_name}: {e}")
                            else:
                                calculation_errors.append(f"æœªæ‰¾åˆ°è®¡ç®—å‡½æ•°: {indicator_name}")

                        except Exception as e:
                            calculation_errors.append(f"{indicator_name} è®¡ç®—å¤±è´¥: {e}")
                            logger.debug(f"âš ï¸ æŒ‡æ ‡è®¡ç®—å¤±è´¥ {indicator_name}: {e}")

                    logger.debug(f"âœ… æœ¬åœ°è®¡ç®—å®Œæˆ {len(computation_required_fields)} ä¸ªæŒ‡æ ‡")

                except Exception as e:
                    logger.error(f"âŒ æœ¬åœ°æŒ‡æ ‡è®¡ç®—è¿‡ç¨‹å¤±è´¥: {e}")
                    calculation_errors.append(f"æœ¬åœ°è®¡ç®—è¿‡ç¨‹å¤±è´¥: {e}")

            # ç¬¬å››æ­¥ï¼šæ„å»ºæœ€ç»ˆç»“æœ
            # è·å–æœ€æ–°å€¼
            latest_values = {}
            for indicator_name, series in all_indicators.items():
                if series is not None and not series.empty:
                    latest_val = series.iloc[-1]
                    if pd.notna(latest_val):
                        latest_values[indicator_name] = float(latest_val)

            # æ„å»ºè¿”å›ç»“æœ
            result = {
                'indicators_df': pd.DataFrame(all_indicators),
                'latest_values': latest_values,
                'errors': calculation_errors
            }

            logger.debug(f"âœ… åŒæºæŒ‡æ ‡è®¡ç®—å®Œæˆ: {len(all_indicators)} ä¸ªæŒ‡æ ‡")

            return result

        except Exception as e:
            logger.error(f"âŒ åŒæºæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}

# ============================================================================
# æ•°æ®è´¨é‡è¯„ä¼°å™¨
# ============================================================================

class DataQualityEvaluator:
    """
    Data Quality Evaluator
    """

    # Data quality standards
    QUALITY_STANDARDS = {
        'valuation': {
            'required_fields': ['pe_ratio', 'pb_ratio', 'ps_ratio', 'pcf_ratio', 'market_cap'],
            'min_valid_ratio': 0.7,
            'max_nan_ratio': 0.5,
            'value_ranges': {
                'pe_ratio': {'min': -100, 'max': 1000},
                'pb_ratio': {'min': 0, 'max': 50},
                'ps_ratio': {'min': 0, 'max': 100},
                'pcf_ratio': {'min': -100, 'max': 1000},
                'market_cap': {'min': 1e6, 'max': 1e13}
            }
        },
        'technical': {
            'required_indicators': ['RSI_14', 'MACD', 'BB_UPPER', 'BB_LOWER', 'SMA_5', 'SMA_10', 'SMA_20'],
            'min_valid_ratio': 0.6,
            'max_nan_ratio': 0.4,
            'value_ranges': {
                'RSI_14': {'min': 0, 'max': 100},
                'MACD': {'min': -100, 'max': 100},
                'BB_UPPER': {'min': 0, 'max': 1e6},
                'BB_LOWER': {'min': 0, 'max': 1e6},
                'SMA_5': {'min': 0, 'max': 1e6},
                'SMA_10': {'min': 0, 'max': 1e6},
                'SMA_20': {'min': 0, 'max': 1e6}
            }
        },
        'price': {
            'required_fields': ['open', 'close', 'high', 'low', 'volume'],
            'min_valid_ratio': 0.8,
            'max_nan_ratio': 0.2,
            'value_ranges': {
                'open': {'min': 0.01, 'max': 1e6},
                'close': {'min': 0.01, 'max': 1e6},
                'high': {'min': 0.01, 'max': 1e6},
                'low': {'min': 0.01, 'max': 1e6},
                'volume': {'min': 0, 'max': 1e10}
            },
            'price_consistency': True
        }
    }

    def __init__(self, manager):
        self.manager = manager
        self.quality_reports = {}
        self.logger = get_logger()  # æ·»åŠ  logger

    def evaluate_data_quality(self, stock_info: Dict, technical_indicators: Dict,
                            data_types: Optional[List[str]] = None) -> Dict[str, bool]:
        """
        Evaluate data quality - é›†æˆ data_formats.py çš„æ ‡å‡†è´¨é‡æ£€æŸ¥
        """
        if data_types is None:
            data_types = ['valuation', 'technical', 'price']

        results = {}
        quality_report = {}

        for data_type in data_types:
            if data_type == 'valuation':
                is_valid, report = self._evaluate_valuation_quality(stock_info)
            elif data_type == 'technical':
                is_valid, report = self._evaluate_technical_quality(technical_indicators)
            elif data_type == 'price':
                is_valid, report = self._evaluate_price_quality(technical_indicators)
            else:
                continue

            results[data_type] = is_valid
            quality_report[data_type] = report

        # é›†æˆ data_formats.py çš„é€šç”¨è´¨é‡æ£€æŸ¥
        if 'price' in data_types and technical_indicators.get('price_data') is not None:
            price_df = technical_indicators['price_data']
            if isinstance(price_df, pd.DataFrame):
                # ä½¿ç”¨ data_formats.py çš„æ ‡å‡†è´¨é‡æ£€æŸ¥
                formats_quality = check_data_quality(price_df)
                if not formats_quality['valid']:
                    results['price'] = False
                    quality_report['price']['formats_issues'] = formats_quality['issues']
                    self.logger.warning(f"ä»·æ ¼æ•°æ®ä¸ç¬¦åˆ data_formats.py æ ‡å‡†: {formats_quality['issues']}")

        # Store quality report
        stock_code = stock_info.get('stock_code', 'unknown')
        self.quality_reports[stock_code] = quality_report

        return results

    def _evaluate_valuation_quality(self, stock_info: Dict) -> Tuple[bool, Dict]:
        """
        Evaluate valuation data quality
        """
        standards = self.QUALITY_STANDARDS['valuation']
        report = {
            'total_fields': 0,
            'valid_fields': 0,
            'nan_fields': 0,
            'out_of_range_fields': 0,
            'valid_ratio': 0.0,
            'nan_ratio': 0.0,
            'issues': []
        }

        required_fields = standards['required_fields']
        value_ranges = standards['value_ranges']

        for field in required_fields:
            report['total_fields'] += 1
            value = stock_info.get(field)

            if value is None or pd.isna(value):
                report['nan_fields'] += 1
                report['issues'].append(f"{field}: NaN")
                continue

            # Check value range
            if field in value_ranges:
                min_val = value_ranges[field]['min']
                max_val = value_ranges[field]['max']
                if not (min_val <= value <= max_val):
                    report['out_of_range_fields'] += 1
                    report['issues'].append(f"{field}: value {value} out of range [{min_val}, {max_val}]")
                    continue

            report['valid_fields'] += 1

        # Calculate ratios
        if report['total_fields'] > 0:
            report['valid_ratio'] = report['valid_fields'] / report['total_fields']
            report['nan_ratio'] = report['nan_fields'] / report['total_fields']

        # Quality assessment
        is_valid = (
            report['valid_ratio'] >= standards['min_valid_ratio'] and
            report['nan_ratio'] <= standards['max_nan_ratio'] and
            report['out_of_range_fields'] == 0
        )

        report['overall_quality'] = 'PASS' if is_valid else 'FAIL'

        return is_valid, report

    def _evaluate_technical_quality(self, technical_indicators: Dict) -> Tuple[bool, Dict]:
        """
        Evaluate technical indicators quality
        """
        standards = self.QUALITY_STANDARDS['technical']
        report = {
            'total_indicators': 0,
            'valid_indicators': 0,
            'nan_indicators': 0,
            'out_of_range_indicators': 0,
            'valid_ratio': 0.0,
            'nan_ratio': 0.0,
            'issues': []
        }

        required_indicators = standards['required_indicators']
        value_ranges = standards['value_ranges']

        # Get latest values
        latest_values = technical_indicators.get('latest_values', {})

        for indicator in required_indicators:
            report['total_indicators'] += 1
            value = latest_values.get(indicator)

            if value is None or pd.isna(value):
                report['nan_indicators'] += 1
                report['issues'].append(f"{indicator}: NaN")
                continue

            # Check value range
            if indicator in value_ranges:
                min_val = value_ranges[indicator]['min']
                max_val = value_ranges[indicator]['max']
                if not (min_val <= value <= max_val):
                    report['out_of_range_indicators'] += 1
                    report['issues'].append(f"{indicator}: value {value} out of range [{min_val}, {max_val}]")
                    continue

            report['valid_indicators'] += 1

        # Calculate ratios
        if report['total_indicators'] > 0:
            report['valid_ratio'] = report['valid_indicators'] / report['total_indicators']
            report['nan_ratio'] = report['nan_indicators'] / report['total_indicators']

        # Quality assessment
        is_valid = (
            report['valid_ratio'] >= standards['min_valid_ratio'] and
            report['nan_ratio'] <= standards['max_nan_ratio'] and
            report['out_of_range_indicators'] == 0
        )

        report['overall_quality'] = 'PASS' if is_valid else 'FAIL'

        return is_valid, report

    def _evaluate_price_quality(self, technical_indicators: Dict) -> Tuple[bool, Dict]:
        """
        Evaluate price data quality
        """
        standards = self.QUALITY_STANDARDS['price']
        report = {
            'total_fields': 0,
            'valid_fields': 0,
            'nan_fields': 0,
            'out_of_range_fields': 0,
            'consistency_issues': 0,
            'valid_ratio': 0.0,
            'nan_ratio': 0.0,
            'issues': []
        }

        required_fields = standards['required_fields']
        value_ranges = standards['value_ranges']

        # Get latest values
        latest_values = technical_indicators.get('latest_values', {})

        for field in required_fields:
            report['total_fields'] += 1
            value = latest_values.get(field)

            if value is None or pd.isna(value):
                report['nan_fields'] += 1
                report['issues'].append(f"{field}: NaN")
                continue

            # Check value range
            if field in value_ranges:
                min_val = value_ranges[field]['min']
                max_val = value_ranges[field]['max']
                if not (min_val <= value <= max_val):
                    report['out_of_range_fields'] += 1
                    report['issues'].append(f"{field}: value {value} out of range [{min_val}, {max_val}]")
                    continue

            report['valid_fields'] += 1

        # Check price consistency
        if standards.get('price_consistency', False):
            open_price = latest_values.get('open')
            close_price = latest_values.get('close')
            high_price = latest_values.get('high')
            low_price = latest_values.get('low')

            if all(p is not None and not pd.isna(p) for p in [open_price, close_price, high_price, low_price]):
                # Check price logic
                if not (high_price >= close_price >= low_price):
                    report['consistency_issues'] += 1
                    report['issues'].append("Price logic error: high >= close >= low")

                if not (high_price >= open_price >= low_price):
                    report['consistency_issues'] += 1
                    report['issues'].append("Price logic error: high >= open >= low")

        # Calculate ratios
        if report['total_fields'] > 0:
            report['valid_ratio'] = report['valid_fields'] / report['total_fields']
            report['nan_ratio'] = report['nan_fields'] / report['total_fields']

        # Quality assessment
        is_valid = (
            report['valid_ratio'] >= standards['min_valid_ratio'] and
            report['nan_ratio'] <= standards['max_nan_ratio'] and
            report['out_of_range_fields'] == 0 and
            report['consistency_issues'] == 0
        )

        report['overall_quality'] = 'PASS' if is_valid else 'FAIL'

        return is_valid, report

    def get_quality_report(self, stock_code: Optional[str] = None) -> Dict:
        """
        Get quality report
        """
        if stock_code:
            return self.quality_reports.get(stock_code, {})
        return self.quality_reports

    def is_data_quality_acceptable(self, stock_info: Dict, technical_indicators: Dict,
                                 data_types: Optional[List[str]] = None) -> bool:
        """
        Check if data quality is acceptable
        """
        quality_results = self.evaluate_data_quality(stock_info, technical_indicators, data_types)
        return all(quality_results.values())

class ScoringEngine:
    """
    Unified Scoring Engine
    """

    def __init__(self, manager):
        self.manager = manager
        # DataQualityEvaluator will be initialized later
        self.quality_evaluator = None
        self.rules = self._define_rules()

    def _init_quality_evaluator(self, manager):
        """Initialize data quality evaluator"""
        self.quality_evaluator = DataQualityEvaluator(manager)

    def _define_rules(self):
        """Define scoring rules"""
        return {
            'pe': {
                'field': 'pe_ratio',
                'data_source': 'stock_info',
                'ideal_range': (8, 25),
                'ideal_score': 15,
                'good_range': (5, 40),
                'good_score': 8
            },
            'pb': {
                'field': 'pb_ratio',
                'data_source': 'stock_info',
                'ideal_range': (0.5, 3.0),
                'ideal_score': 12,
                'good_range': (0.3, 5.0),
                'good_score': 6
            },
            'rsi': {
                'field': 'RSI_14',
                'data_source': 'technical',
                'ideal_range': (40, 60),
                'ideal_score': 10,
                'good_range': (30, 70),
                'good_score': 5
            },
            'turnover': {
                'field': 'turnover_ratio',
                'data_source': 'stock_info',
                'ideal_range': (1.0, 8.0),
                'ideal_score': 8,
                'good_range': (0.5, 15.0),
                'good_score': 4
            }
        }

    def calculate_score(self, stock_info: Dict, technical_indicators: Dict,
    rule_keys: List[str], layer: str = 'basic') -> float:
        """
        Calculate score based on rules (with data quality assessment)
        """
        try:
            # First perform data quality assessment
            stock_code = stock_info.get('stock_code', 'unknown')

            # Determine data types to check
            data_types_to_check = []
            for rule_key in rule_keys:
                if rule_key in self.rules:
                    rule = self.rules[rule_key]
                    data_source = rule.get('data_source')
                    if data_source not in data_types_to_check:
                        data_types_to_check.append(data_source)

            # Data quality assessment
            if self.quality_evaluator is not None:
                quality_results = self.quality_evaluator.evaluate_data_quality(
                    stock_info, technical_indicators, data_types_to_check
                )
            else:
                self.manager.logger.warning(f"Quality evaluator not available for {stock_code}, skipping quality check")
                quality_results = {dt: True for dt in data_types_to_check}  # Assume all pass if no evaluator

            # Check if all data types pass
            if not all(quality_results.values()):
                self.manager.logger.warning(f"Data quality failed for {stock_code}, skipping scoring")
                return 0.0

            # Data quality passed, start scoring
            self.manager.logger.debug(f"Data quality passed for {stock_code}, starting scoring")

            total_score = 0
            for rule_key in rule_keys:
                if rule_key not in self.rules:
                    continue

                rule = self.rules[rule_key]
                score = self._apply_rule(rule, stock_info, technical_indicators, layer)
                total_score += score

            final_score = max(0, min(100, total_score))
            self.manager.logger.debug(f"Scoring completed for {stock_code}: {final_score:.1f}")

            return final_score

        except Exception as e:
            stock_code = stock_info.get('stock_code', 'unknown')
            self.manager.logger.error(f"Scoring calculation failed for {stock_code}: {e}")
            return 0.0

    def _apply_rule(self, rule: Dict, stock_info: Dict, technical_indicators: Dict, layer: str) -> float:
        """Apply single scoring rule"""
        try:
            # Get data value
            value = self._get_value_for_rule(rule, stock_info, technical_indicators)
            if value is None or pd.isna(value):
                return 0

            # Range scoring
            return self._calculate_range_score(rule, value, layer)

        except Exception as e:
            self.manager.logger.error(f"Rule application failed for {rule.get('field', 'unknown')}: {e}")
            return 0

    def _get_value_for_rule(self, rule: Dict, stock_info: Dict, technical_indicators: Dict):
        """Get data value for rule"""
        if rule['data_source'] == 'stock_info':
            return stock_info.get(rule['field'])
        elif rule['data_source'] == 'technical':
            return technical_indicators.get('latest_values', {}).get(rule['field'])
        return None

    def _calculate_range_score(self, rule: Dict, value: float, layer: str) -> float:
        """Calculate range score"""
        # Ideal range
        if 'ideal_range' in rule:
            min_val, max_val = rule['ideal_range']
            if min_val <= value <= max_val:
                return rule.get('ideal_score', 10)

        # Good range
        if 'good_range' in rule:
            min_val, max_val = rule['good_range']
            if min_val <= value <= max_val:
                return rule.get('good_score', 5)

        return 0

    def _calculate_common_score_factors(self, stock_info: Dict, technical_indicators: Dict) -> Dict:
        """
        è®¡ç®—é€šç”¨è¯„åˆ†å› å­
        """
        try:
            factors = {}

            # ä¼°å€¼å› å­
            pe = stock_info.get('pe_ratio')
            if pe is not None and not pd.isna(pe):
                if 8 <= pe <= 25:
                    factors['pe_score'] = 15
                elif 5 <= pe <= 40:
                    factors['pe_score'] = 8
                else:
                    factors['pe_score'] = 0

            # æŠ€æœ¯å› å­
            rsi = technical_indicators.get('latest_values', {}).get('RSI_14')
            if rsi is not None and not pd.isna(rsi):
                if 40 <= rsi <= 60:
                    factors['rsi_score'] = 10
                elif 30 <= rsi <= 70:
                    factors['rsi_score'] = 5
                else:
                    factors['rsi_score'] = 0

            # æˆäº¤é‡å› å­
            turnover = stock_info.get('turnover_ratio')
            if turnover is not None and not pd.isna(turnover):
                if 1.0 <= turnover <= 8.0:
                    factors['turnover_score'] = 8
                elif 0.5 <= turnover <= 15.0:
                    factors['turnover_score'] = 4
                else:
                    factors['turnover_score'] = 0

            return factors

        except Exception as e:
            self.manager.logger.error(f"é€šç”¨è¯„åˆ†å› å­è®¡ç®—å¤±è´¥: {e}")
            return {}

# ============================================================================
# ä¸»è‚¡ç¥¨æ± ç®¡ç†å™¨
# ============================================================================

def init_rqdatac():
    """
    Initialize rqdatac data source connection

    Returns:
        bool: True if initialization successful, False otherwise
    """
    try:
        logger.info("ğŸ”„ Initializing rqdatac connection...")
        if rqdatac is not None:
            rqdatac.init()
        logger.info("âœ… rqdatac initialization successful")
        return True
    except Exception as e:
        logger.error(f"âŒ rqdatac initialization failed: {e}")
        return False

def main():
    """ä¸»å¯åŠ¨å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°çš„è‚¡ç¥¨æ± ç®¡ç†å·¥å…·"""
    import argparse

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Stock Pool Management System")
    parser.add_argument("--sync", action="store_true", help="Sync and build stock pools")
    parser.add_argument("--monitor", action="store_true", help="Start stock monitoring")
    parser.add_argument("--analyze", action="store_true", help="Run technical analysis")

    # å¦‚æœæ²¡æœ‰æä¾›å‚æ•°ï¼Œé»˜è®¤æ‰§è¡Œsync
    if len(sys.argv) == 1:
        args = parser.parse_args(['--sync'])
    else:
        args = parser.parse_args()

    # åˆå§‹åŒ–ç¯å¢ƒç®¡ç†å™¨
    env_manager = EnvironmentManager()
    env_manager.ensure_environment_with_fallback()

    # åˆå§‹åŒ–æ—¥å¿—
    from modules.log_manager import get_system_logger
    logger = get_system_logger()

    logger.info("=" * 60)
    logger.info("è‚¡ç¥¨æ± ç®¡ç†ç³»ç»Ÿå¯åŠ¨")
    logger.info("=" * 60)

    # åˆå§‹åŒ–RQDatac
    if not init_rqdatac():
        logger.error("RQDatacåˆå§‹åŒ–å¤±è´¥")
        return False

    try:
        # åˆ›å»ºPoolManagerå®ä¾‹
        pool_manager = PoolManager()

        if args.sync:
            # æ‰§è¡Œæ¯æ—¥åŒæ­¥è®¡ç®—å»ºæ± 
            logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œè‚¡ç¥¨æ± åŒæ­¥...")
            success = pool_manager.sync_and_build_pools_optimized()

            if success:
                logger.info("âœ… è‚¡ç¥¨æ± åŒæ­¥å®Œæˆ")
                return True
            else:
                logger.error("âŒ è‚¡ç¥¨æ± åŒæ­¥å¤±è´¥")
                return False

        elif args.monitor:
            # å¯åŠ¨è‚¡ç¥¨ç›‘æ§
            logger.info("ğŸ“Š å¯åŠ¨è‚¡ç¥¨ç›‘æ§...")
            try:
                import stockmonitor
                # è¿™é‡Œå¯ä»¥æ·»åŠ ç›‘æ§é€»è¾‘
                logger.info("âœ… è‚¡ç¥¨ç›‘æ§å·²å¯åŠ¨")
                return True
            except ImportError as e:
                logger.error(f"âŒ æ— æ³•å¯åŠ¨ç›‘æ§æ¨¡å—: {e}")
                return False

        elif args.analyze:
            # è¿è¡ŒæŠ€æœ¯åˆ†æ
            logger.info("ğŸ“ˆ è¿è¡ŒæŠ€æœ¯åˆ†æ...")
            try:
                # è¿™é‡Œå¯ä»¥æ·»åŠ æŠ€æœ¯åˆ†æé€»è¾‘
                logger.info("âœ… æŠ€æœ¯åˆ†æå®Œæˆ")
                return True
            except Exception as e:
                logger.error(f"âŒ æŠ€æœ¯åˆ†æå¤±è´¥: {e}")
                return False

        else:
            # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
            parser.print_help()
            return True

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)
