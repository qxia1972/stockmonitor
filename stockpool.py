# -*- coding: utf-8 -*-
"""
Stock Pool Manager - Enhanced Stock Pool Management System
å¢å¼ºç‰ˆè‚¡ç¥¨æ± ç®¡ç†ç³»ç»Ÿ

æ ¸å¿ƒåŠŸèƒ½:
- ï¿½ï¸ ä¸‰å±‚è‚¡ç¥¨æ± æ¶æ„ (åŸºç¡€æ±  â†’ è§‚å¯Ÿæ±  â†’ æ ¸å¿ƒæ± )
- ğŸ“Š å®Œæ•´çš„è¯„åˆ†ç®—æ³• (åŸºç¡€/è§‚å¯Ÿ/æ ¸å¿ƒå±‚çº§)
- ğŸ”§ é›†æˆæŠ€æœ¯æŒ‡æ ‡å¼•æ“ (11ç§æ ¸å¿ƒç®—æ³•)
- ğŸ’¾ ç‹¬ç«‹æ•°æ®å­˜å‚¨ (é¿å…ä¸å…¶ä»–æ¨¡å—å†²çª)
- ï¿½ æ™ºèƒ½äº¤æ˜“æ—¥ç¡®å®š (åŸºäºäº¤æ˜“æ—¶æ®µåˆ¤æ–­)
- ğŸ¯ æ•°æ®è´¨é‡è¯„ä¼° (å¤šç»´åº¦è´¨é‡æ£€æŸ¥)

æ¶æ„ä¼˜åŠ¿:
- æ¨¡å—åŒ–è®¾è®¡: æ¯ä¸ªç»„ä»¶ç‹¬ç«‹å¯ç»´æŠ¤
- é«˜æ€§èƒ½ä¼˜åŒ–: æ‰¹é‡å¤„ç† + æ™ºèƒ½ç¼“å­˜
- é”™è¯¯å¤„ç†: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•
- å¯æ‰©å±•æ€§: æ˜“äºæ·»åŠ æ–°çš„æŒ‡æ ‡å’Œè§„åˆ™

ä½¿ç”¨è¯´æ˜:
- è‡ªåŠ¨ç¯å¢ƒæ£€æµ‹å’Œåˆ‡æ¢
- æ”¯æŒå®Œæ•´çš„æŠ€æœ¯åˆ†ææŒ‡æ ‡
- ç‹¬ç«‹ç¼“å­˜æœºåˆ¶ï¼Œé¿å…æ–‡ä»¶å†²çª
- è¯¦ç»†çš„è°ƒè¯•å’Œé”™è¯¯æ—¥å¿—
"""

# Standard library imports
import sys
import os
import json
import logging
import time
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional, Tuple, Callable, Any, Union
from pathlib import Path

# Third-party imports
import pandas as pd
import numpy as np
import talib

# Project imports
from modules.log_manager import get_stockpool_logger
from modules.data_formats import INDICATOR_CONFIG, STANDARD_INDICATOR_COLUMNS
from modules.python_manager import EnvironmentManager

def get_logger():
    """Get configured logger for stockpool module"""
    return get_stockpool_logger()

# Initialize logger
logger = get_logger()

# RQDatac initialization with proper error handling
rqdatac_available = False
rqdatac = None

try:
    import rqdatac
    rqdatac.init()
    rqdatac_available = True
    logger.info("âœ… RQDatac initialized successfully")
except ImportError:
    logger.warning("âš ï¸ RQDatac not available - some features will be limited")
except Exception as e:
    logger.error(f"âŒ RQDatac initialization failed: {e}")

# Use constant for availability check
RQDATAC_AVAILABLE = rqdatac_available

# Environment setup
def setup_environment():
    """Setup and validate Python environment"""
    logger.debug("ğŸ”§ Setting up Python environment")

    # Add project root to path
    project_root = Path(__file__).parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        logger.debug(f"ğŸ“ Added project root to sys.path: {project_root}")

    # Environment management
    env_manager = EnvironmentManager()
    env_manager.ensure_environment_with_fallback()
    logger.debug("âœ… Environment setup completed")

# Initialize environment on module load
setup_environment()

class PoolManager:
    """
    Unified Stock Pool Manager (Enhanced Version)
    å¢å¼ºç‰ˆç»Ÿä¸€è‚¡ç¥¨æ± ç®¡ç†å™¨

    é›†æˆåŠŸèƒ½:
    - ğŸ†• å†…ç½®StockPoolDataStoreç‹¬ç«‹æ•°æ®å­˜å‚¨
    - ğŸ†• é›†æˆStockPoolIndicatorEngineæŠ€æœ¯æŒ‡æ ‡å¼•æ“
    - ğŸ”„ ä¿æŒåŸæœ‰ä¸‰å±‚ç­›é€‰ç³»ç»Ÿ (Basic â†’ Watch â†’ Core)
    - âš¡ ä¼˜åŒ–çš„æ•°æ®è·å–å’ŒæŠ€æœ¯åˆ†æèƒ½åŠ›
    - ğŸ“Š å®Œæ•´çš„è¯„åˆ†æ’åºç®—æ³•
    - ğŸ’¾ æŒä¹…åŒ–æ•°æ®å­˜å‚¨å’ŒçŠ¶æ€ç®¡ç†

    ä¸»è¦æ”¹è¿›:
    - é¿å…ä¸DataStoreçš„æ–‡ä»¶æ“ä½œå†²çª
    - æ”¯æŒå®Œæ•´çš„11ç§æŠ€æœ¯æŒ‡æ ‡ç®—æ³•
    - ç‹¬ç«‹çš„æ•°æ®ç¼“å­˜æœºåˆ¶
    - å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
    """

    # ===== è‚¡ç¥¨æ± é…ç½® =====
    STOCK_POOL_CONFIG = {
        # æ± å¤§å°é…ç½®
        "basic_pool_size": 500,                 #
        "watch_pool_size": 50,                  #
        "core_pool_size": 5,                   #

        # æ•°æ®è·å–é…ç½®
        "basic_info_batch_size": 200,            #
        "price_data_batch_size": 100,           # ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°ï¼Œä»400è°ƒæ•´ä¸º100ä»¥è·å¾—æœ€ä½³æ€§èƒ½
        "history_days": 60,                     # å†å²æ•°æ®å¤©æ•°

        # åŸºç¡€è¿‡æ»¤æ¡ä»¶
        "market_cap_min": 10e8,                 # 10äº¿ (é™ä½é—¨æ§›)
        "market_cap_max": 5000e8,               # 5000
        "price_min": 3.0,                       # 3å…ƒ (é™ä½é—¨æ§›)
        "price_max": 200.0,                     # 200å…ƒ (æé«˜ä¸Šé™)
        "volume_days": 20,                      #
        "volume_min_ratio": 0.5,                #
        "turnover_min": 0.005,                  #  0.5% (é™ä½é—¨æ§›)
        "pe_acceptable_max": 100,               # PEæœ€å¤§å€¼
        "volume_min_threshold": 50000,          # æˆäº¤é‡æœ€å°é˜ˆå€¼

        # RSIæç«¯å€¼è¿‡æ»¤
        "rsi_extreme_low": 10,                  # RSIæç«¯ä½å€¼
        "rsi_extreme_high": 90,                 # RSIæç«¯é«˜å€¼

        # åˆ†çº§è¿‡æ»¤æ¡ä»¶ (åŸºç¡€å±‚çº§)
        "watch_turnover_min": 0.5,              #
        "watch_turnover_max": 15,               #
        "watch_rsi_min": 20,                    # RSI
        "watch_rsi_max": 80,                    # RSI
        "watch_market_cap_min": 100e8,          # 100
        "watch_market_cap_max": 2000e8,         # 2000
        "watch_pe_max": 50,                     # PE
        "watch_ma_above_min": 2,                #

        # åˆ†çº§è¿‡æ»¤æ¡ä»¶ (æ ¸å¿ƒå±‚çº§)
        "core_turnover_min": 1.0,               #
        "core_turnover_max": 10,                #
        "core_rsi_min": 30,                     # RSI
        "core_rsi_max": 70,                     # RSI
        "core_market_cap_min": 200e8,           # 200
        "core_market_cap_max": 1000e8,          # 1000
        "core_pe_min": 8,                       # PE
        "core_pe_max": 30,                      # PE
        "core_pb_max": 5,                       # PB
        "core_volatility_max": 0.4,             # 40%
        "core_ma_above_min": 3,                 #
        "core_adx_min": 20,                     # ADXæœ€å°å€¼
    }

    def __init__(self, data_dir="data", force_refresh_cache: bool = False):
        """
        Initialize enhanced stock pool manager

        Args:
            data_dir (str): Directory path for storing pool data and logs
            force_refresh_cache (bool): Whether to force refresh all cached data from network
        """
        from pathlib import Path
        import sys

        # è§£æå‘½ä»¤è¡Œå‚æ•°
        self.force_refresh_cache = force_refresh_cache
        if len(sys.argv) > 1:
            # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
            if '--force-refresh' in sys.argv or '-f' in sys.argv:
                self.force_refresh_cache = True
                logger.info("ğŸ”„ æ£€æµ‹åˆ°å¼ºåˆ¶åˆ·æ–°å‚æ•°ï¼Œå°†ä»ç½‘ç»œè·å–æœ€æ–°æ•°æ®")

        # Use pathlib for unified path management
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # ğŸ†• åˆå§‹åŒ–é›†æˆçš„ç‹¬ç«‹ç»„ä»¶
        self.data_store = StockPoolDataStore()
        self.indicator_engine = StockPoolIndicatorEngine(self.data_store)

        # åˆå§‹åŒ–ç»Ÿä¸€è¯„åˆ†å¼•æ“
        self.scoring_engine = ScoringEngine(self)
        self.scoring_engine._init_quality_evaluator(self)

        # åˆå§‹åŒ–æ•°æ®è´¨é‡è¯„ä¼°å™¨
        self.quality_evaluator: Optional[DataQualityEvaluator] = self.scoring_engine.quality_evaluator

        # ç¡®ä¿quality_evaluatorå·²æ­£ç¡®åˆå§‹åŒ–
        if self.quality_evaluator is None:
            logger.error("âŒ æ•°æ®è´¨é‡è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError("Failed to initialize quality evaluator")

        # æ•°æ®è´¨é‡ç»Ÿè®¡
        self.quality_stats = {
            'total_evaluations': 0,
            'passed_evaluations': 0,
            'failed_evaluations': 0,
            'quality_issues': {}
        }

        # Three-layer data file paths for persistent storage
        self.basic_pool_file = self.data_dir / "basic_pool.json"       # Basic layer (500 stocks)
        self.watch_pool_file = self.data_dir / "watch_pool.json"       # Watch layer (50 stocks)
        self.core_pool_file = self.data_dir / "core_pool.json"         # Core layer (10 stocks)
        self.sync_log_file = self.data_dir / "sync_log.json"           # Sync operation logs

        # Sync status management for thread safety
        self.is_syncing = False
        self.last_sync_time = None

        # Use global configured logger instance
        self.logger = logger

        # æ ¹æ®ç¯å¢ƒä¼˜åŒ–æ—¥å¿—çº§åˆ«
        self._optimize_logging_for_performance()

        # Load comprehensive configuration parameters
        self.config = self.STOCK_POOL_CONFIG

        if self.force_refresh_cache:
            logger.info("ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼šå°†ä»ç½‘ç»œè·å–æœ€æ–°æ•°æ®")

    def _optimize_logging_for_performance(self):
        """
        æ ¹æ®è¿è¡Œç¯å¢ƒä¼˜åŒ–æ—¥å¿—è®°å½•æ€§èƒ½
        - ç”Ÿäº§ç¯å¢ƒï¼šå‡å°‘debugæ—¥å¿—è¾“å‡º
        - å¼€å‘ç¯å¢ƒï¼šä¿æŒè¯¦ç»†æ—¥å¿—
        """
        import os

        # æ£€æŸ¥æ˜¯å¦ä¸ºç”Ÿäº§ç¯å¢ƒ
        is_production = (
            os.getenv('ENVIRONMENT') == 'production' or
            os.getenv('PYTHON_ENV') == 'production' or
            not os.getenv('DEBUG', '').lower() in ('true', '1', 'yes')
        )

        if is_production:
            # ç”Ÿäº§ç¯å¢ƒï¼šåªè®°å½•é‡è¦ä¿¡æ¯
            logging.getLogger('stockpool').setLevel(logging.INFO)
            self.logger.info("ğŸ­ ç”Ÿäº§ç¯å¢ƒæ¨¡å¼ï¼šä¼˜åŒ–æ—¥å¿—æ€§èƒ½")
        else:
            # å¼€å‘ç¯å¢ƒï¼šä¿æŒè¯¦ç»†æ—¥å¿—
            logging.getLogger('stockpool').setLevel(logging.DEBUG)
            self.logger.debug("ğŸ”§ å¼€å‘ç¯å¢ƒæ¨¡å¼ï¼šå¯ç”¨è¯¦ç»†æ—¥å¿—")

    def calculate_basic_layer_score(self, stock_info: Dict, technical_indicators: Dict) -> float:
        """
        è®¡ç®—åŸºç¡€å±‚è¯„åˆ† - è‚¡ç¥¨æ± çš„å…¥é—¨ç­›é€‰å±‚

        è¯„åˆ†é€»è¾‘:
        1. æ•°æ®è´¨é‡è¯„ä¼° - ç¡®ä¿æ•°æ®å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
        2. åŸºç¡€è¯„åˆ†åŸºå‡† - 50åˆ†ä½œä¸ºèµ·å§‹åˆ†æ•°
        3. è§„åˆ™è¯„åˆ†è®¡ç®— - åŸºäºPEã€PBã€RSIã€æ¢æ‰‹ç‡ç­‰å…³é”®æŒ‡æ ‡
        4. åˆ†æ•°èŒƒå›´é™åˆ¶ - ç¡®ä¿ç»“æœåœ¨0-100åˆ†èŒƒå›´å†…

        ç­›é€‰æ ‡å‡†:
        - æ•°æ®è´¨é‡å¿…é¡»å…¨éƒ¨é€šè¿‡
        - é‡ç‚¹å…³æ³¨åŸºæœ¬é¢å’Œä¼°å€¼æŒ‡æ ‡
        - ä¸ºè§‚å¯Ÿå±‚å’Œæ ¸å¿ƒå±‚æä¾›åŸºç¡€è¯„åˆ†

        Args:
            stock_info: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«å¸‚å€¼ã€PEã€PBç­‰
            technical_indicators: æŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«RSIã€æ¢æ‰‹ç‡ç­‰

        Returns:
            åŸºç¡€å±‚è¯„åˆ† (0-100)ï¼Œ0åˆ†è¡¨ç¤ºæ•°æ®è´¨é‡ä¸åˆæ ¼

        Raises:
            è¯¥æ–¹æ³•å†…éƒ¨å¤„ç†æ‰€æœ‰å¼‚å¸¸ï¼Œè¿”å›0åˆ†è¡¨ç¤ºè¯„åˆ†å¤±è´¥
        """
        stock_code = stock_info.get('stock_code', 'unknown')
        logger.debug(f"ğŸ§® å¼€å§‹è®¡ç®—åŸºç¡€å±‚è¯„åˆ†: {stock_code}")

        try:
            # æ•°æ®è´¨é‡è¯„ä¼° - é¦–è¦å…³å¡
            if self.quality_evaluator is None:
                logger.error("âŒ æ•°æ®è´¨é‡è¯„ä¼°å™¨æœªåˆå§‹åŒ–")
                return 0.0

            quality_results = self.quality_evaluator.evaluate_data_quality(
                stock_info, technical_indicators, ['valuation', 'technical']
            )

            # å¦‚æœæ•°æ®è´¨é‡ä¸åˆæ ¼ï¼Œè¿”å›0åˆ†
            if not all(quality_results.values()):
                logger.debug(f"âŒ æ•°æ®è´¨é‡ä¸åˆæ ¼ï¼Œè·³è¿‡è¯„åˆ†: {stock_code}")
                logger.debug(f"ğŸ“Š è´¨é‡æ£€æŸ¥ç»“æœ: {quality_results}")
                return 0.0

            logger.debug(f"âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡: {stock_code}")

            # åŸºç¡€è¯„åˆ†åŸºå‡†
            base_score = 50.0
            logger.debug(f"ğŸ“ˆ åŸºç¡€è¯„åˆ†åŸºå‡†: {base_score}")

            # ä½¿ç”¨ç»Ÿä¸€è¯„åˆ†å¼•æ“è®¡ç®—è§„åˆ™åˆ†æ•°
            rule_scores = self.scoring_engine.calculate_score(
                stock_info,
                technical_indicators,
                ['pe', 'pb', 'rsi', 'turnover'],
                'basic'
            )

            logger.debug(f"ğŸ¯ è§„åˆ™è¯„åˆ†: {rule_scores:.2f}")

            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = max(0, min(100, base_score + rule_scores))

            logger.debug(f"ğŸ† åŸºç¡€å±‚è¯„åˆ† - {stock_code}: {final_score:.1f} (åŸºå‡†:{base_score} + è§„åˆ™:{rule_scores:.1f})")

            return final_score

        except Exception as e:
            logger.error(f"âŒ åŸºç¡€å±‚è¯„åˆ†è®¡ç®—å¤±è´¥ - {stock_code}: {e}")
            logger.debug(f"ğŸ” å¼‚å¸¸è¯¦æƒ… - è‚¡ç¥¨ä¿¡æ¯: {stock_info.keys()}", exc_info=True)
            return 0.0

    def calculate_watch_layer_score(self, stock_info: Dict, technical_indicators: Dict) -> float:
        """
        è®¡ç®—è§‚å¯Ÿå±‚è¯„åˆ†

        Args:
            stock_info: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            technical_indicators: æŠ€æœ¯æŒ‡æ ‡æ•°æ®

        Returns:
            è§‚å¯Ÿå±‚è¯„åˆ† (0-100)
        """
        try:
            # åŸºäºåŸºç¡€å±‚è¯„åˆ†
            base_score = self.calculate_basic_layer_score(stock_info, technical_indicators)

            # å¦‚æœåŸºç¡€è¯„åˆ†å¤ªä½ï¼Œç›´æ¥è¿”å›
            if base_score < 30:
                return base_score

            # è§‚å¯Ÿå±‚é¢å¤–åŠ åˆ†
            bonus_score = 0

            # å¸‚å€¼åŠ åˆ† (200äº¿-1000äº¿æœ€ä¼˜åŒºé—´)
            market_cap = stock_info.get('market_cap')
            if market_cap is not None and not pd.isna(market_cap):
                if 200e8 <= market_cap <= 1000e8:
                    bonus_score += 5
                elif 100e8 <= market_cap <= 2000e8:
                    bonus_score += 3

            # MACDä¿¡å·åŠ åˆ†
            macd = technical_indicators.get('latest_values', {}).get('MACD')
            macd_signal = technical_indicators.get('latest_values', {}).get('MACD_SIGNAL')
            if macd is not None and macd_signal is not None:
                if macd > macd_signal:
                    bonus_score += 5
                elif abs(macd - macd_signal) < macd_signal * 0.01:  # MACDæ¥è¿‘SIGNAL
                    bonus_score += 2

            final_score = max(0, min(100, base_score + bonus_score))

            stock_code = stock_info.get('stock_code', 'unknown')
            self.logger.debug(f"è§‚å¯Ÿå±‚è¯„åˆ† - {stock_code}: {final_score:.1f} (åŸºç¡€: {base_score:.1f}, åŠ åˆ†: {bonus_score})")

            return final_score

        except Exception as e:
            stock_code = stock_info.get('stock_code', 'unknown')
            self.logger.error(f"è§‚å¯Ÿå±‚è¯„åˆ†è®¡ç®—å¤±è´¥ - {stock_code}: {e}")
            return 0.0

    def calculate_core_layer_score(self, stock_info: Dict, technical_indicators: Dict) -> float:
        """
        è®¡ç®—æ ¸å¿ƒå±‚è¯„åˆ† - è‚¡ç¥¨æ± çš„æ ¸å¿ƒç­›é€‰å±‚

        è¯„åˆ†é€»è¾‘:
        1. åŸºäºè§‚å¯Ÿå±‚è¯„åˆ† - å¿…é¡»è¾¾åˆ°60åˆ†é—¨æ§›
        2. PBæ¯”ç‡åŠ åˆ† - æ ¸å¿ƒå±‚å¯¹ä¼°å€¼è¦æ±‚æ›´ä¸¥æ ¼
        3. PEæ¯”ç‡åŠ åˆ† - ç›ˆåˆ©èƒ½åŠ›è¯„ä¼°
        4. æ³¢åŠ¨ç‡æƒ©ç½š - é«˜æ³¢åŠ¨è‚¡ç¥¨é™ä½è¯„åˆ†
        5. åˆ†æ•°èŒƒå›´é™åˆ¶ - ç¡®ä¿ç»“æœåœ¨0-100åˆ†èŒƒå›´å†…

        ç­›é€‰æ ‡å‡†:
        - è§‚å¯Ÿå±‚è¯„åˆ†å¿…é¡»â‰¥60åˆ†
        - PBæ¯”ç‡è¶Šä½è¯„åˆ†è¶Šé«˜ (PB<2å¾—8åˆ†ï¼ŒPB<3å¾—5åˆ†ï¼ŒPB<5å¾—3åˆ†)
        - PEæ¯”ç‡åœ¨8-25åŒºé—´è·å¾—æœ€é«˜åŠ åˆ†
        - æ³¢åŠ¨ç‡>40%æ‰£5åˆ†ï¼Œ>30%æ‰£3åˆ†

        Args:
            stock_info: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«PEã€PBç­‰ä¼°å€¼æŒ‡æ ‡
            technical_indicators: æŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«æ³¢åŠ¨ç‡ç­‰

        Returns:
            æ ¸å¿ƒå±‚è¯„åˆ† (0-100)ï¼Œ0åˆ†è¡¨ç¤ºæœªè¾¾åˆ°æ ¸å¿ƒæ ‡å‡†

        Raises:
            è¯¥æ–¹æ³•å†…éƒ¨å¤„ç†æ‰€æœ‰å¼‚å¸¸ï¼Œè¿”å›0åˆ†è¡¨ç¤ºè¯„åˆ†å¤±è´¥
        """
        stock_code = stock_info.get('stock_code', 'unknown')
        logger.debug(f"ğŸ§® å¼€å§‹è®¡ç®—æ ¸å¿ƒå±‚è¯„åˆ†: {stock_code}")

        try:
            # åŸºäºè§‚å¯Ÿå±‚è¯„åˆ†
            watch_score = self.calculate_watch_layer_score(stock_info, technical_indicators)
            logger.debug(f"ğŸ“Š è§‚å¯Ÿå±‚è¯„åˆ†: {watch_score:.1f}")

            # å¦‚æœè§‚å¯Ÿå±‚è¯„åˆ†å¤ªä½ï¼Œç›´æ¥è¿”å›
            if watch_score < 60:
                logger.debug(f"âŒ è§‚å¯Ÿå±‚è¯„åˆ†ä¸è¶³60åˆ†ï¼Œè·³è¿‡æ ¸å¿ƒå±‚è¯„åˆ†: {stock_code}")
                return watch_score

            # æ ¸å¿ƒå±‚é¢å¤–åŠ åˆ† - æ›´ä¸¥æ ¼çš„æ ‡å‡†
            bonus_score = 0

            # PBæ¯”ç‡åŠ åˆ† (æ ¸å¿ƒå±‚è¦æ±‚PB < 5)
            pb_ratio = stock_info.get('pb_ratio')
            if pb_ratio is not None and not pd.isna(pb_ratio) and pb_ratio < 5:
                if pb_ratio < 2:
                    bonus_score += 8
                    logger.debug(f"ğŸ’° PBæ¯”ç‡ä¼˜ç§€åŠ åˆ†: +8 (PB={pb_ratio:.2f})")
                elif pb_ratio < 3:
                    bonus_score += 5
                    logger.debug(f"ğŸ’° PBæ¯”ç‡è‰¯å¥½åŠ åˆ†: +5 (PB={pb_ratio:.2f})")
                elif pb_ratio < 5:
                    bonus_score += 3
                    logger.debug(f"ğŸ’° PBæ¯”ç‡åˆæ ¼åŠ åˆ†: +3 (PB={pb_ratio:.2f})")

            # PEæ¯”ç‡åŠ åˆ† (æ ¸å¿ƒå±‚è¦æ±‚PEåœ¨åˆç†åŒºé—´)
            pe_ratio = stock_info.get('pe_ratio')
            if pe_ratio is not None and not pd.isna(pe_ratio):
                if 8 <= pe_ratio <= 25:
                    bonus_score += 5
                    logger.debug(f"ğŸ“ˆ PEæ¯”ç‡æœ€ä¼˜åŒºé—´åŠ åˆ†: +5 (PE={pe_ratio:.2f})")
                elif 5 <= pe_ratio <= 40:
                    bonus_score += 3
                    logger.debug(f"ğŸ“Š PEæ¯”ç‡è‰¯å¥½åŒºé—´åŠ åˆ†: +3 (PE={pe_ratio:.2f})")

            # æ³¢åŠ¨ç‡æƒ©ç½š (æ ¸å¿ƒå±‚è¦æ±‚ä½æ³¢åŠ¨)
            volatility = technical_indicators.get('latest_values', {}).get('volatility_20d')
            if volatility is not None and not pd.isna(volatility):
                if volatility > 0.4:  # 40%ä»¥ä¸Šçš„æ³¢åŠ¨ç‡
                    bonus_score -= 5
                    logger.debug(f"âš ï¸ é«˜æ³¢åŠ¨ç‡æƒ©ç½š: -5 (æ³¢åŠ¨ç‡={volatility:.1%})")
                elif volatility > 0.3:  # 30%ä»¥ä¸Šçš„æ³¢åŠ¨ç‡
                    bonus_score -= 3
                    logger.debug(f"âš ï¸ ä¸­ç­‰æ³¢åŠ¨ç‡æƒ©ç½š: -3 (æ³¢åŠ¨ç‡={volatility:.1%})")

            final_score = max(0, min(100, watch_score + bonus_score))

            logger.debug(f"ğŸ† æ ¸å¿ƒå±‚è¯„åˆ† - {stock_code}: {final_score:.1f} (è§‚å¯Ÿå±‚:{watch_score:.1f} + åŠ åˆ†:{bonus_score})")

            return final_score

        except Exception as e:
            logger.error(f"âŒ æ ¸å¿ƒå±‚è¯„åˆ†è®¡ç®—å¤±è´¥ - {stock_code}: {e}")
            logger.debug(f"ğŸ” å¼‚å¸¸è¯¦æƒ… - è‚¡ç¥¨ä¿¡æ¯: {stock_info.keys()}", exc_info=True)
            return 0.0

    # ===== åŸæœ‰åŠŸèƒ½æ–¹æ³• =====
    # è¿™é‡Œéœ€è¦ä»å¤‡ä»½æ–‡ä»¶ä¸­æ¢å¤æ‰€æœ‰åŸæœ‰æ–¹æ³•
    # ç”±äºæ–‡ä»¶è¿‡é•¿ï¼Œæˆ‘ä¼šé€æ­¥æ¢å¤å…³é”®æ–¹æ³•

    def evaluate_stock_data_quality(self, stock_info: Dict, technical_indicators: Dict,
                                  data_types: List[str] = None) -> Dict[str, bool]:
        """
        è¯„ä¼°å•åªè‚¡ç¥¨çš„æ•°æ®è´¨é‡

        Args:
            stock_info: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            technical_indicators: æŠ€æœ¯æŒ‡æ ‡æ•°æ®
            data_types: è¦è¯„ä¼°çš„æ•°æ®ç±»å‹ï¼Œé»˜è®¤è¯„ä¼°æ‰€æœ‰ç±»å‹

        Returns:
            å„æ•°æ®ç±»å‹çš„è´¨é‡è¯„ä¼°ç»“æœ
        """
        try:
            self.quality_stats['total_evaluations'] += 1

            if self.quality_evaluator is None:
                self.logger.error("æ•°æ®è´¨é‡è¯„ä¼°å™¨æœªåˆå§‹åŒ–")
                return {}

            results = self.quality_evaluator.evaluate_data_quality(
                stock_info, technical_indicators, data_types
            )

            # ç»Ÿè®¡ç»“æœ
            if all(results.values()):
                self.quality_stats['passed_evaluations'] += 1
            else:
                self.quality_stats['failed_evaluations'] += 1

                # è®°å½•è´¨é‡é—®é¢˜
                stock_code = stock_info.get('stock_code', 'unknown')
                if self.quality_evaluator is not None:
                    quality_report = self.quality_evaluator.get_quality_report(stock_code)
                    for data_type, report in quality_report.items():
                        if report.get('overall_quality') == 'FAIL':
                            issues = report.get('issues', [])
                            if issues:
                                issue_key = f"{data_type}_{issues[0][:50]}"  # å–å‰50ä¸ªå­—ç¬¦ä½œä¸ºé”®
                                self.quality_stats['quality_issues'][issue_key] = \
                                    self.quality_stats['quality_issues'].get(issue_key, 0) + 1

            return results

        except Exception as e:
            self.logger.error(f"æ•°æ®è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return {}

    def process_daily_sync_caculate_buildpool(self) -> bool:
        """è¿è¡Œæ¯æ—¥åŒæ­¥ã€è®¡ç®—å’Œæ„å»ºè‚¡ç¥¨æ± çš„å®Œæ•´æµç¨‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ¯æ—¥è‚¡ç¥¨æ± åŒæ­¥...")

            # ===== ç¬¬ä¸€é˜¶æ®µï¼šè·å–è‚¡ç¥¨åˆ—è¡¨ =====
            self.logger.info("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
            stock_list_df = self.data_store.fetch_stock_list()

            if stock_list_df is None or stock_list_df.empty:
                self.logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ŒåŒæ­¥å¤±è´¥")
                return False

            # æå–è‚¡ç¥¨ä»£ç åˆ—è¡¨
            stock_codes = stock_list_df['order_book_id'].tolist()
            self.logger.info(f"âœ… è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")

            # ===== ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è·å–åŸºæœ¬é¢æ•°æ® =====
            self.logger.info("ï¿½ ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è·å–åŸºæœ¬é¢æ•°æ®...")
            target_date = self.data_store.get_target_trading_date()
            if not target_date:
                self.logger.error("âŒ æ— æ³•ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥ï¼ŒåŒæ­¥å¤±è´¥")
                return False
            self.logger.info(f"ğŸ¯ ç›®æ ‡åˆ†ææ—¥æœŸ: {target_date} (ä½¿ç”¨æœ€åä¸€ä¸ªå·²å®Œæˆäº¤æ˜“æ—¥çš„æ•°æ®)")

            # è®¡ç®—å†å²æ•°æ®èŒƒå›´
            start_date = (datetime.strptime(target_date, '%Y-%m-%d') -
                         timedelta(days=self.config['history_days'])).strftime('%Y-%m-%d')
            self.logger.info(f"ğŸ“… å†å²æ•°æ®èŒƒå›´: {start_date} è‡³ {target_date}")

            # æ‰¹é‡è·å–ä¼°å€¼æ•°æ®ï¼ˆåŒ…å«åŸºæœ¬é¢ä¿¡æ¯ï¼‰
            self.logger.info("ğŸ’° è·å–ä¼°å€¼æ•°æ®ï¼ˆå¸‚å€¼ã€PEã€PBç­‰ï¼‰...")
            valuation_data = self._batch_get_valuation_data(stock_codes, target_date, return_dataframe=True)
            if isinstance(valuation_data, pd.DataFrame) and not valuation_data.empty:
                valuation_dict = valuation_data.set_index('stock_code').to_dict('index')
                self.logger.info(f"âœ… è·å–åˆ° {len(valuation_data)} åªè‚¡ç¥¨çš„ä¼°å€¼æ•°æ®")
            else:
                valuation_dict = {}
                self.logger.warning("âš ï¸ æœªè·å–åˆ°ä¼°å€¼æ•°æ®")

            # ===== ç¬¬ä¸‰é˜¶æ®µï¼šæ‰¹é‡è·å–ä»·æ ¼åºåˆ—æ•°æ® =====
            self.logger.info("ğŸ“ˆ ç¬¬ä¸‰æ­¥ï¼šæ‰¹é‡è·å–ä»·æ ¼åºåˆ—æ•°æ®...")
            price_data = self._batch_get_price_data(stock_codes, start_date, target_date)
            valid_price_stocks = [code for code, df in price_data.items() if df is not None and not df.empty]
            self.logger.info(f"âœ… è·å–åˆ° {len(valid_price_stocks)} åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®")

            # ===== ç¬¬å››é˜¶æ®µï¼šè®¡ç®—æŠ€æœ¯æŒ‡æ ‡ =====
            self.logger.info("ğŸ”§ ç¬¬å››æ­¥ï¼šè®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
            precomputed_data = {}
            processed_count = 0

            for stock_code in stock_codes:
                try:
                    processed_count += 1
                    if processed_count % 100 == 0:
                        self.logger.info(f"â³ å·²å¤„ç† {processed_count}/{len(stock_codes)} åªè‚¡ç¥¨...")

                    # è·å–åŸºæœ¬é¢æ•°æ®
                    stock_info = valuation_dict.get(stock_code, {'stock_code': stock_code})

                    # è·å–ä»·æ ¼æ•°æ®
                    price_df = price_data.get(stock_code)

                    if price_df is not None and not price_df.empty:
                        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                        technical_indicators = self.calculate_technical_indicators(price_df, stock_code)

                        precomputed_data[stock_code] = {
                            'stock_info': stock_info,
                            'technical_indicators': technical_indicators
                        }
                    else:
                        self.logger.debug(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ— ä»·æ ¼æ•°æ®ï¼Œè·³è¿‡")

                except Exception as e:
                    self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                    continue

            self.logger.info(f"âœ… æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ: {len(precomputed_data)} åªè‚¡ç¥¨")

            # ===== ç¬¬äº”é˜¶æ®µï¼šæ„å»ºè‚¡ç¥¨æ± ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨DataFrameï¼‰ =====
            self.logger.info("ğŸ—ï¸ ç¬¬äº”æ­¥ï¼šæ„å»ºä¸‰ä¸ªè‚¡ç¥¨æ± ...")
            result = self.build_all_pools_from_precomputed_data(precomputed_data, target_date)

            if result and all(not df.empty for df in result.values()):
                self.logger.info("âœ… è‚¡ç¥¨æ± æ„å»ºæˆåŠŸ")
                return True
            else:
                self.logger.error("âŒ è‚¡ç¥¨æ± æ„å»ºå¤±è´¥")
                return False

        except Exception as e:
            self.logger.error(f"âŒ æ¯æ—¥åŒæ­¥å¤±è´¥: {e}")
            return False

    def build_stock_pool(self, scored_stocks: Union[List[Dict], pd.DataFrame], target_date: Optional[str] = None) -> Dict[str, pd.DataFrame]:
        """
        æ„å»ºä¸‰ä¸ªè‚¡ç¥¨æ±  - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒDataFrameè¾“å…¥ä»¥å‡å°‘è½¬æ¢å¼€é”€

        ç›´æ¥ä½¿ç”¨DataFrameè¿›è¡Œæ± æ„å»ºï¼Œé¿å…å­—å…¸åˆ°DataFrameçš„é‡å¤è½¬æ¢
        ç”ŸæˆåŸºç¡€æ± ã€è§‚å¯Ÿæ± ã€æ ¸å¿ƒæ± ä¸‰ä¸ªå±‚çº§çš„è‚¡ç¥¨æ± 

        Args:
            scored_stocks: è¯„åˆ†åçš„è‚¡ç¥¨æ•°æ®ï¼Œå¯ä»¥æ˜¯å­—å…¸åˆ—è¡¨æˆ–DataFrame
            target_date: ç›®æ ‡åˆ†ææ—¥æœŸ(YYYY-MM-DDæ ¼å¼)

        Returns:
            åŒ…å«ä¸‰ä¸ªè‚¡ç¥¨æ± çš„å­—å…¸:
            {
                'basic_pool': pd.DataFrame,
                'watch_pool': pd.DataFrame,
                'core_pool': pd.DataFrame
            }
        """
        try:
            # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'start_time': datetime.now(),
                'total_stocks': 0,
                'basic_pool_size': 0,
                'watch_pool_size': 0,
                'core_pool_size': 0,
                'errors': []
            }

            if target_date is None:
                target_date = self._get_latest_trading_date()

            self.logger.info(f"ğŸ¯ ä¸ºæ—¥æœŸ {target_date} æ„å»ºä¸‰ä¸ªè‚¡ç¥¨æ± ")

            # ===== ç¬¬ä¸€é˜¶æ®µï¼šç»Ÿä¸€æ•°æ®æ ¼å¼ =====
            if isinstance(scored_stocks, list):
                # å¦‚æœè¾“å…¥æ˜¯å­—å…¸åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºDataFrame
                if scored_stocks:
                    df_scored = pd.DataFrame(scored_stocks)
                else:
                    df_scored = pd.DataFrame()
            elif isinstance(scored_stocks, pd.DataFrame):
                # å¦‚æœå·²ç»æ˜¯DataFrameï¼Œç›´æ¥ä½¿ç”¨ï¼Œé¿å…æ‹·è´
                df_scored = scored_stocks
            else:
                self.logger.error("âŒ è¾“å…¥æ•°æ®æ ¼å¼ä¸æ”¯æŒ")
                return {
                    'basic_pool': pd.DataFrame(),
                    'watch_pool': pd.DataFrame(),
                    'core_pool': pd.DataFrame()
                }

            if df_scored.empty:
                self.logger.warning("âš ï¸ æ— å¯ç”¨è¯„åˆ†æ•°æ®ï¼Œä½¿ç”¨ç©ºæ•°æ®")
                return {
                    'basic_pool': pd.DataFrame(),
                    'watch_pool': pd.DataFrame(),
                    'core_pool': pd.DataFrame()
                }

            stats['total_stocks'] = len(df_scored)
            self.logger.info(f"ğŸ“Š å¤„ç† {len(df_scored)} åªè¯„åˆ†è‚¡ç¥¨æ•°æ®")

            # ===== ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºåŸºç¡€æ±  =====
            self.logger.info("ğŸ—ï¸ æ„å»ºåŸºç¡€æ± ...")
            # æŒ‰åŸºç¡€è¯„åˆ†æ’åºï¼Œé€‰æ‹©å‰Nå
            basic_candidates = df_scored.nlargest(self.config['basic_pool_size'], 'basic_score')
            stats['basic_pool_size'] = len(basic_candidates)

            # ===== ç¬¬ä¸‰é˜¶æ®µï¼šæ„å»ºè§‚å¯Ÿæ±  =====
            self.logger.info("ğŸ‘€ æ„å»ºè§‚å¯Ÿæ± ...")
            # ä»åŸºç¡€æ± ä¸­é€‰æ‹©è§‚å¯Ÿæ± ï¼Œä½†ä½¿ç”¨è§‚å¯Ÿå±‚è¯„åˆ†æ’åº
            watch_candidates = basic_candidates[basic_candidates['watch_score'] >= 50]  # è§‚å¯Ÿæ± æœ€ä½åˆ†æ•°è¦æ±‚
            watch_candidates = watch_candidates.nlargest(self.config['watch_pool_size'], 'watch_score')
            stats['watch_pool_size'] = len(watch_candidates)

            # ===== ç¬¬å››é˜¶æ®µï¼šæ„å»ºæ ¸å¿ƒæ±  =====
            self.logger.info("ğŸ’ æ„å»ºæ ¸å¿ƒæ± ...")
            # ä»è§‚å¯Ÿæ± ä¸­é€‰æ‹©æ ¸å¿ƒæ± ï¼Œä½†ä½¿ç”¨æ ¸å¿ƒå±‚è¯„åˆ†æ’åº
            core_candidates = watch_candidates[watch_candidates['core_score'] >= 70]  # æ ¸å¿ƒæ± æœ€ä½åˆ†æ•°è¦æ±‚
            core_candidates = core_candidates.nlargest(self.config['core_pool_size'], 'core_score')
            stats['core_pool_size'] = len(core_candidates)

            # ===== ç¬¬äº”é˜¶æ®µï¼šç›´æ¥ä¿å­˜DataFrame =====
            self.logger.info("ğŸ’¾ ä¿å­˜ä¸‰ä¸ªè‚¡ç¥¨æ± æ•°æ®...")

            # ä¿å­˜åŸºç¡€æ± 
            if not basic_candidates.empty:
                # è®¾ç½®å†…éƒ¨å®ä¾‹ï¼ˆé¿å…æ‹·è´ï¼Œç›´æ¥èµ‹å€¼ï¼‰
                self.data_store.basic_pool = basic_candidates
                if self.data_store.save_basic_pool():
                    self.logger.info("âœ… åŸºç¡€æ± æ•°æ®ä¿å­˜æˆåŠŸ")
                else:
                    self.logger.error("âŒ åŸºç¡€æ± æ•°æ®ä¿å­˜å¤±è´¥")
                    stats['errors'].append("ä¿å­˜åŸºç¡€æ± æ•°æ®å¤±è´¥")

            # ä¿å­˜è§‚å¯Ÿæ± 
            if not watch_candidates.empty:
                # è®¾ç½®å†…éƒ¨å®ä¾‹ï¼ˆé¿å…æ‹·è´ï¼Œç›´æ¥èµ‹å€¼ï¼‰
                self.data_store.watch_pool = watch_candidates
                if self.data_store.save_watch_pool():
                    self.logger.info("âœ… è§‚å¯Ÿæ± æ•°æ®ä¿å­˜æˆåŠŸ")
                else:
                    self.logger.error("âŒ è§‚å¯Ÿæ± æ•°æ®ä¿å­˜å¤±è´¥")
                    stats['errors'].append("ä¿å­˜è§‚å¯Ÿæ± æ•°æ®å¤±è´¥")

            # ä¿å­˜æ ¸å¿ƒæ± 
            if not core_candidates.empty:
                # è®¾ç½®å†…éƒ¨å®ä¾‹ï¼ˆé¿å…æ‹·è´ï¼Œç›´æ¥èµ‹å€¼ï¼‰
                self.data_store.core_pool = core_candidates
                if self.data_store.save_core_pool():
                    self.logger.info("âœ… æ ¸å¿ƒæ± æ•°æ®ä¿å­˜æˆåŠŸ")
                else:
                    self.logger.error("âŒ æ ¸å¿ƒæ± æ•°æ®ä¿å­˜å¤±è´¥")
                    stats['errors'].append("ä¿å­˜æ ¸å¿ƒæ± æ•°æ®å¤±è´¥")

            # ===== ç¬¬å…­é˜¶æ®µï¼šè¾“å‡ºç»Ÿè®¡ä¿¡æ¯ =====
            self._log_build_statistics(stats)

            # è¿”å›ä¸‰ä¸ªè‚¡ç¥¨æ± çš„DataFrame
            result = {
                'basic_pool': basic_candidates,
                'watch_pool': watch_candidates,
                'core_pool': core_candidates
            }

            self.logger.info("ğŸ‰ ä¸‰ä¸ªè‚¡ç¥¨æ± æ„å»ºå®Œæˆï¼"            f"åŸºç¡€æ± : {len(basic_candidates)} åª, è§‚å¯Ÿæ± : {len(watch_candidates)} åª, æ ¸å¿ƒæ± : {len(core_candidates)} åª")

            return result

        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºè‚¡ç¥¨æ± å¤±è´¥: {e}")
            return {
                'basic_pool': pd.DataFrame(),
                'watch_pool': pd.DataFrame(),
                'core_pool': pd.DataFrame()
            }

    def _batch_get_valuation_data(self, stock_codes: List[str], target_date: str,
                                 return_dataframe: bool = False) -> Union[List[Dict], pd.DataFrame]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨ä¼°å€¼æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬

        æ”¯æŒè¿”å›DataFrameæ ¼å¼ä»¥å‡å°‘è½¬æ¢å¼€é”€

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ
            return_dataframe: æ˜¯å¦è¿”å›DataFrameæ ¼å¼ï¼Œé»˜è®¤Falseä¿æŒå‘åå…¼å®¹æ€§

        Returns:
            ä¼°å€¼æ•°æ®åˆ—è¡¨æˆ–DataFrame
        """
        if return_dataframe:
            # ç›´æ¥è¿”å›DataFrameæ ¼å¼
            valuation_data = []
            batch_size = self.config.get('basic_info_batch_size', 200)

            for i in range(0, len(stock_codes), batch_size):
                batch_stocks = stock_codes[i:i + batch_size]

                if (i // batch_size + 1) % 10 == 0:  # æ¯10ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡
                    self.logger.info(f"â³ å·²å¤„ç† {i + len(batch_stocks)}/{len(stock_codes)} åªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®...")

                for stock_code in batch_stocks:
                    try:
                        # ä½¿ç”¨datastoreè·å–å•åªè‚¡ç¥¨çš„ä¼°å€¼æ•°æ®
                        valuation_df = self.data_store.get_smart_data(stock_code, 'valuation')
                        if valuation_df is not None and not valuation_df.empty:
                            # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
                            valuation_df = valuation_df.copy()
                            valuation_df['stock_code'] = stock_code
                            valuation_data.append(valuation_df.iloc[0])  # åªå–æœ€æ–°ä¸€è¡Œ
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è·å–è‚¡ç¥¨ {stock_code} ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
                        continue

            return pd.DataFrame(valuation_data) if valuation_data else pd.DataFrame()

        else:
            # ä¿æŒåŸæœ‰å­—å…¸æ ¼å¼è¿”å›
            basic_info_list = []
            batch_size = self.config.get('basic_info_batch_size', 200)

            for i in range(0, len(stock_codes), batch_size):
                batch_stocks = stock_codes[i:i + batch_size]

                if (i // batch_size + 1) % 10 == 0:  # æ¯10ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡
                    self.logger.info(f"â³ å·²å¤„ç† {i + len(batch_stocks)}/{len(stock_codes)} åªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®...")

                for stock_code in batch_stocks:
                    try:
                        # ä½¿ç”¨datastoreè·å–å•åªè‚¡ç¥¨çš„ä¼°å€¼æ•°æ®
                        valuation_df = self.data_store.get_smart_data(stock_code, 'valuation')
                        if valuation_df is not None and not valuation_df.empty:
                            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                            stock_info = {
                                'stock_code': stock_code,
                                'market_cap': valuation_df.get('market_cap', [None])[0] if 'market_cap' in valuation_df.columns else None,
                                'pe_ratio': valuation_df.get('pe_ratio', [None])[0] if 'pe_ratio' in valuation_df.columns else None,
                                'pb_ratio': valuation_df.get('pb_ratio', [None])[0] if 'pb_ratio' in valuation_df.columns else None,
                                'ps_ratio': valuation_df.get('ps_ratio', [None])[0] if 'ps_ratio' in valuation_df.columns else None,
                                'pcf_ratio': valuation_df.get('pcf_ratio', [None])[0] if 'pcf_ratio' in valuation_df.columns else None,
                                'turnover_ratio': valuation_df.get('turnover_ratio', [None])[0] if 'turnover_ratio' in valuation_df.columns else None,
                            }
                            basic_info_list.append(stock_info)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è·å–è‚¡ç¥¨ {stock_code} ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
                        continue

            return basic_info_list

    def _batch_get_price_data(self, stock_codes: List[str], start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            ä»·æ ¼æ•°æ®å­—å…¸ {stock_code: price_df}
        """
        price_data = {}
        batch_size = self.config.get('price_data_batch_size', 100)

        for i in range(0, len(stock_codes), batch_size):
            batch_stocks = stock_codes[i:i + batch_size]

            if (i // batch_size + 1) % 5 == 0:  # æ¯5ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡
                self.logger.info(f"â³ å·²å¤„ç† {i + len(batch_stocks)}/{len(stock_codes)} åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®...")

            for stock_code in batch_stocks:
                try:
                    # ä½¿ç”¨datastoreè·å–å•åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®
                    price_df = self.data_store.get_smart_data(stock_code, 'price')
                    if price_df is not None and not price_df.empty:
                        price_data[stock_code] = price_df
                except Exception as e:
                    self.logger.warning(f"âš ï¸ è·å–è‚¡ç¥¨ {stock_code} ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                    continue

        return price_data

    def _process_candidates_parallel(self, basic_info_df: pd.DataFrame, price_data: Dict[str, pd.DataFrame],
                                   target_date: str, stats: Dict) -> List[Dict]:
        """
        å¹¶è¡Œå¤„ç†å€™é€‰è‚¡ç¥¨

        Args:
            basic_info_df: åŸºæœ¬é¢æ•°æ®DataFrame
            price_data: ä»·æ ¼æ•°æ®å­—å…¸
            target_date: ç›®æ ‡æ—¥æœŸ
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸

        Returns:
            å€™é€‰è‚¡ç¥¨åˆ—è¡¨
        """
        candidate_stocks = []
        processed_count = 0

        for _, row in basic_info_df.iterrows():
            stock_code = row['stock_code']
            processed_count += 1

            if processed_count % 500 == 0:
                self.logger.info(f"â³ å·²å¤„ç† {processed_count}/{len(basic_info_df)} åªè‚¡ç¥¨...")

            if stock_code not in price_data:
                continue

            try:
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                technical_indicators = self.calculate_technical_indicators(price_data[stock_code], stock_code)

                # åº”ç”¨åŸºç¡€è¿‡æ»¤æ¡ä»¶
                stock_info = row.to_dict()
                if not self.apply_basic_filters(stock_info, technical_indicators):
                    stats['filtered_stocks'] += 1
                    continue

                # ä½¿ç”¨scoring engineè¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°
                if self.quality_evaluator is None:
                    self.logger.error("æ•°æ®è´¨é‡è¯„ä¼°å™¨æœªåˆå§‹åŒ–")
                    stats['filtered_stocks'] += 1
                    continue

                quality_results = self.quality_evaluator.evaluate_data_quality(
                    stock_info, technical_indicators, ['valuation', 'technical']
                )

                if not all(quality_results.values()):
                    stats['filtered_stocks'] += 1
                    continue

                # è®¡ç®—ç»¼åˆè¯„åˆ†
                score = self.calculate_basic_layer_score(stock_info, technical_indicators)
                stats['scored_stocks'] += 1

                candidate_stocks.append({
                    'stock_code': stock_code,
                    'score': score,
                    'market_cap': stock_info.get('market_cap'),
                    'pe_ratio': stock_info.get('pe_ratio'),
                    'pb_ratio': stock_info.get('pb_ratio'),
                    'current_price': technical_indicators.get('latest_values', {}).get('current_price'),
                    'rsi': technical_indicators.get('latest_values', {}).get('RSI_14'),
                    'turnover_rate': technical_indicators.get('latest_values', {}).get('turnover_rate'),
                    'volatility': technical_indicators.get('latest_values', {}).get('volatility_20d'),
                    'date': target_date,
                    'technical_indicators': technical_indicators,
                    'data_quality': quality_results
                })

            except Exception as e:
                self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                stats['errors'].append(f"{stock_code}: {str(e)}")
                continue

        return candidate_stocks

    def _prepare_pool_data(self, stocks: List[Dict], target_date: str, layer: str) -> Dict:
        """
        å‡†å¤‡è‚¡ç¥¨æ± æ•°æ®

        Args:
            stocks: è‚¡ç¥¨åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ
            layer: å±‚çº§

        Returns:
            æ± æ•°æ®å­—å…¸
        """
        return {
            'date': target_date,
            'stocks': stocks,
            'layer': layer,
            'description': f'{layer}è‚¡ç¥¨æ± ï¼ˆå¸‚å€¼ã€ä»·æ ¼ã€æµåŠ¨æ€§åˆæ­¥ç­›é€‰ï¼‰',
            'created_at': datetime.now().isoformat(),
            'metadata': {
                'total_stocks': len(stocks),
                'avg_score': sum(s.get('score', 0) for s in stocks) / len(stocks) if stocks else 0,
                'score_range': {
                    'min': min((s.get('score', 0) for s in stocks), default=0),
                    'max': max((s.get('score', 0) for s in stocks), default=0)
                },
                'market_cap_distribution': self._analyze_market_cap_distribution(stocks)
            }
        }

    def _analyze_market_cap_distribution(self, stocks: List[Dict]) -> Dict:
        """
        åˆ†æå¸‚å€¼åˆ†å¸ƒ

        Args:
            stocks: è‚¡ç¥¨åˆ—è¡¨

        Returns:
            å¸‚å€¼åˆ†å¸ƒåˆ†æ
        """
        market_caps = []
        for s in stocks:
            mc = s.get('market_cap')
            if mc is not None and isinstance(mc, (int, float)) and not pd.isna(mc):
                market_caps.append(float(mc))

        if not market_caps:
            return {'error': 'æ— å¸‚å€¼æ•°æ®'}

        return {
            'count': len(market_caps),
            'avg_market_cap': sum(market_caps) / len(market_caps),
            'median_market_cap': sorted(market_caps)[len(market_caps) // 2],
            'distribution': {
                'small': len([mc for mc in market_caps if mc < 100e8]),  # < 100äº¿
                'medium': len([mc for mc in market_caps if 100e8 <= mc < 500e8]),  # 100-500äº¿
                'large': len([mc for mc in market_caps if mc >= 500e8])  # >= 500äº¿
            }
        }

    def _log_build_statistics(self, stats: Dict):
        """
        è®°å½•æ„å»ºç»Ÿè®¡ä¿¡æ¯

        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        duration = datetime.now() - stats['start_time']

        self.logger.info("ğŸ“Š ä¸‰ä¸ªè‚¡ç¥¨æ± æ„å»ºç»Ÿè®¡ä¿¡æ¯:")
        self.logger.info(f"   â±ï¸ æ€»è€—æ—¶: {duration.total_seconds():.2f}ç§’")
        self.logger.info(f"   ğŸ“ˆ æ€»è‚¡ç¥¨æ•°: {stats['total_stocks']}")
        self.logger.info(f"   ğŸ—ï¸ åŸºç¡€æ± : {stats['basic_pool_size']} åª")
        self.logger.info(f"   ï¿½ï¸ è§‚å¯Ÿæ± : {stats['watch_pool_size']} åª")
        self.logger.info(f"   ğŸ’ æ ¸å¿ƒæ± : {stats['core_pool_size']} åª")

        if stats['errors']:
            self.logger.warning(f"   âš ï¸ é”™è¯¯æ•°é‡: {len(stats['errors'])}")
            for error in stats['errors'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                self.logger.warning(f"     - {error}")

        # è®¡ç®—å„æ± çš„æˆåŠŸç‡
        if stats['total_stocks'] > 0:
            basic_rate = (stats['basic_pool_size'] / stats['total_stocks'] * 100)
            watch_rate = (stats['watch_pool_size'] / stats['total_stocks'] * 100)
            core_rate = (stats['core_pool_size'] / stats['total_stocks'] * 100)

            self.logger.info(f"   ğŸ“Š å…¥é€‰ç‡ - åŸºç¡€æ± : {basic_rate:.2f}%, è§‚å¯Ÿæ± : {watch_rate:.2f}%, æ ¸å¿ƒæ± : {core_rate:.2f}%")

        self.logger.info("âœ… è‚¡ç¥¨æ± æ„å»ºå®Œæˆ")

    def prepare_precomputed_data(self, stock_codes: List[str], target_date: Optional[str] = None) -> Dict[str, Dict]:
        """
        å‡†å¤‡é¢„è®¡ç®—æ•°æ® - ä¾›build_stock_poolä½¿ç”¨

        è¿™ä¸ªæ–¹æ³•æ¼”ç¤ºäº†å¦‚ä½•å‡†å¤‡é¢„è®¡ç®—æ•°æ®ï¼Œç„¶åä¼ é€’ç»™build_stock_pool

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            é¢„è®¡ç®—æ•°æ®å­—å…¸
        """
        if target_date is None:
            target_date = self._get_latest_trading_date()

        self.logger.info(f"ğŸ”§ å‡†å¤‡é¢„è®¡ç®—æ•°æ®: {len(stock_codes)} åªè‚¡ç¥¨, æ—¥æœŸ: {target_date}")

        precomputed_data = {}

        # è®¡ç®—å†å²æ•°æ®èŒƒå›´
        start_date = (datetime.strptime(target_date, '%Y-%m-%d') -
                     timedelta(days=self.config['history_days'])).strftime('%Y-%m-%d')

        try:
            # æ‰¹é‡è·å–åŸºæœ¬é¢æ•°æ®
            valuation_data = self._batch_get_valuation_data(stock_codes, target_date, return_dataframe=True)
            if isinstance(valuation_data, pd.DataFrame) and not valuation_data.empty:
                valuation_dict = valuation_data.set_index('stock_code').to_dict('index')
            else:
                valuation_dict = {}

            # æ‰¹é‡è·å–ä»·æ ¼æ•°æ®
            price_data = self._batch_get_price_data(stock_codes, start_date, target_date)

            # ä¸ºæ¯åªè‚¡ç¥¨è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            for stock_code in stock_codes:
                try:
                    stock_info = valuation_dict.get(stock_code, {'stock_code': stock_code})
                    price_df = price_data.get(stock_code)

                    if price_df is not None and not price_df.empty:
                        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                        technical_indicators = self.calculate_technical_indicators(price_df, stock_code)

                        precomputed_data[stock_code] = {
                            'stock_info': stock_info,
                            'technical_indicators': technical_indicators
                        }
                    else:
                        self.logger.warning(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ— ä»·æ ¼æ•°æ®ï¼Œè·³è¿‡")

                except Exception as e:
                    self.logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                    continue

            self.logger.info(f"âœ… é¢„è®¡ç®—æ•°æ®å‡†å¤‡å®Œæˆ: {len(precomputed_data)}/{len(stock_codes)} åªè‚¡ç¥¨")

        except Exception as e:
            self.logger.error(f"âŒ å‡†å¤‡é¢„è®¡ç®—æ•°æ®å¤±è´¥: {e}")

        return precomputed_data

    def build_all_pools_from_precomputed_data(self, precomputed_data: Dict[str, Dict],
                                            target_date: Optional[str] = None) -> Dict[str, pd.DataFrame]:
        """
        ä»é¢„è®¡ç®—æ•°æ®æ„å»ºæ‰€æœ‰è‚¡ç¥¨æ±  - ä¼˜åŒ–ç‰ˆæœ¬

        ä½¿ç”¨DataFrameè¿›è¡Œè¯„åˆ†è®¡ç®—ï¼Œé¿å…å­—å…¸åˆ°DataFrameçš„é‡å¤è½¬æ¢

        Args:
            precomputed_data: é¢„è®¡ç®—æ•°æ®
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            ä¸‰ä¸ªè‚¡ç¥¨æ± çš„å­—å…¸
        """
        self.logger.info("ğŸš€ å¼€å§‹ä»é¢„è®¡ç®—æ•°æ®æ„å»ºæ‰€æœ‰è‚¡ç¥¨æ± ")

        try:
            # ç›´æ¥åˆ›å»ºDataFrameæ¥å­˜å‚¨è¯„åˆ†ç»“æœï¼Œé¿å…ä¸­é—´åˆ—è¡¨
            scored_rows = []

            for stock_code, stock_data in precomputed_data.items():
                try:
                    stock_info = stock_data.get('stock_info', {})
                    technical_indicators = stock_data.get('technical_indicators', {})

                    # ç¡®ä¿è‚¡ç¥¨ä»£ç åœ¨stock_infoä¸­
                    if 'stock_code' not in stock_info:
                        stock_info['stock_code'] = stock_code

                    # è®¡ç®—ä¸‰ä¸ªå±‚çº§çš„è¯„åˆ†
                    basic_score = self.calculate_basic_layer_score(stock_info, technical_indicators)
                    watch_score = self.calculate_watch_layer_score(stock_info, technical_indicators)
                    core_score = self.calculate_core_layer_score(stock_info, technical_indicators)

                    # ç›´æ¥åˆ›å»ºDataFrameè¡Œå­—å…¸
                    scored_rows.append({
                        'stock_code': stock_code,
                        'basic_score': basic_score,
                        'watch_score': watch_score,
                        'core_score': core_score,
                        'market_cap': stock_info.get('market_cap'),
                        'pe_ratio': stock_info.get('pe_ratio'),
                        'pb_ratio': stock_info.get('pb_ratio'),
                        'current_price': technical_indicators.get('latest_values', {}).get('current_price'),
                        'rsi': technical_indicators.get('latest_values', {}).get('RSI_14'),
                        'turnover_rate': technical_indicators.get('latest_values', {}).get('turnover_rate'),
                        'volatility': technical_indicators.get('latest_values', {}).get('volatility_20d'),
                        'date': target_date,
                        # å­˜å‚¨å®Œæ•´çš„åŸå§‹æ•°æ®ä»¥å¤‡åç»­ä½¿ç”¨
                        'technical_indicators': technical_indicators,
                        'stock_info': stock_info
                    })

                except Exception as e:
                    self.logger.warning(f"âš ï¸ è¯„åˆ†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                    continue

            # ä¸€æ¬¡æ€§åˆ›å»ºDataFrame
            if scored_rows:
                df_scored = pd.DataFrame(scored_rows)
                self.logger.info(f"âœ… è¯„åˆ†è®¡ç®—å®Œæˆ: {len(df_scored)} åªè‚¡ç¥¨")
            else:
                df_scored = pd.DataFrame()
                self.logger.warning("âš ï¸ æ— æœ‰æ•ˆè¯„åˆ†æ•°æ®")

            # ä½¿ç”¨ä¼˜åŒ–çš„build_stock_poolæ–¹æ³•ï¼ˆç›´æ¥ä½¿ç”¨DataFrameï¼‰
            result = self.build_stock_pool(df_scored, target_date)

            return result

        except Exception as e:
            self.logger.error(f"âŒ ä»é¢„è®¡ç®—æ•°æ®æ„å»ºè‚¡ç¥¨æ± å¤±è´¥: {e}")
            return {
                'basic_pool': pd.DataFrame(),
                'watch_pool': pd.DataFrame(),
                'core_pool': pd.DataFrame()
            }





    def _validate_stock_codes(self, stocks: List[str]) -> List[str]:
        """
        éªŒè¯è‚¡ç¥¨ä»£ç çš„æœ‰æ•ˆæ€§

        Args:
            stocks: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        valid_stocks = []
        for stock in stocks:
            if isinstance(stock, str) and (stock.endswith('.XSHE') or stock.endswith('.XSHG')):
                valid_stocks.append(stock)
        return valid_stocks

    def apply_basic_filters(self, stock_info: Dict, technical_indicators: Dict) -> bool:
        """
        åº”ç”¨åŸºç¡€å±‚è¿‡æ»¤æ¡ä»¶
        """
        try:
            # ä»·æ ¼è¿‡æ»¤
            current_price = technical_indicators.get('latest_values', {}).get('current_price')
            if not self._check_range(current_price, 'price_min', 'price_max'):
                return False

            # æ¢æ‰‹ç‡è¿‡æ»¤
            turnover_rate = technical_indicators.get('latest_values', {}).get('turnover_rate')
            if turnover_rate is not None and not pd.isna(turnover_rate):
                if turnover_rate < self._get_config_value('turnover_min', 0):
                    return False

            # RSIè¿‡æ»¤
            rsi_14 = technical_indicators.get('latest_values', {}).get('RSI_14')
            if not self._check_range(rsi_14, 'rsi_extreme_low', 'rsi_extreme_high'):
                return False

            # PEè¿‡æ»¤
            pe_ratio = stock_info.get('pe_ratio')
            if pe_ratio is not None and not pd.isna(pe_ratio):
                if pe_ratio < 0 or pe_ratio > self._get_config_value('pe_acceptable_max', 100):
                    return False

            # æˆäº¤é‡è¿‡æ»¤
            volume_sma_5 = technical_indicators.get('latest_values', {}).get('VOLUME_SMA_5')
            if volume_sma_5 is not None and not pd.isna(volume_sma_5):
                if volume_sma_5 < self._get_config_value('volume_min_threshold', 50000):
                    return False

            return True

        except Exception as e:
            self.logger.error(f"åŸºç¡€å±‚è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def calculate_technical_indicators(self, price_data: pd.DataFrame, stock_code: Optional[str] = None) -> Dict:
        """
        è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ - ä½¿ç”¨StockPoolIndicatorEngineç”Ÿæˆå®Œæ•´çš„dataframeæ ¼å¼æŒ‡æ ‡

        Args:
            price_data: åŒ…å«OHLCVçš„ä»·æ ¼æ•°æ®DataFrame
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºç¼“å­˜é”®ï¼‰

        Returns:
            Dict: åŒ…å«å®Œæ•´æŠ€æœ¯æŒ‡æ ‡çš„å­—å…¸ï¼ŒåŒ…å«full_serieså’Œlatest_values
        """
        if price_data is None or price_data.empty:
            return {}

        try:
            self.logger.debug(f"ğŸ“ˆ ä½¿ç”¨StockPoolIndicatorEngineè®¡ç®— {stock_code or 'unknown'} çš„æŠ€æœ¯æŒ‡æ ‡...")

            # ä½¿ç”¨StockPoolIndicatorEngineè®¡ç®—æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
            indicators_result = self.indicator_engine.calculate_all_indicators(
                price_data,
                stock_code=stock_code
            )

            if not indicators_result:
                self.logger.warning(f"âš ï¸ {stock_code}: æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥")
                return {}

            # è·å–æœ€æ–°å€¼ç”¨äºç­›é€‰å’Œè¯„åˆ†
            latest_values = indicators_result.get('latest_values', {})

            # ç›´æ¥åœ¨latest_valuesä¸Šè¿›è¡Œä¿®æ”¹ï¼Œé¿å…åˆ›å»ºæ–°å­—å…¸
            if not price_data.empty:
                # å½“å‰ä»·æ ¼
                current_price = price_data['close'].iloc[-1]
                latest_values['current_price'] = current_price

                # æœ€è¿‘5æ—¥å¹³å‡æˆäº¤é‡å’Œæ¢æ‰‹ç‡
                if 'volume' in price_data.columns and len(price_data) >= 5:
                    recent_volume = price_data['volume'].tail(5).mean()
                    latest_values['avg_volume_5d'] = recent_volume

                    # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆå¦‚æœæœ‰æ€»å¸‚å€¼æ•°æ®çš„è¯ï¼Œè¿™é‡Œæš‚æ—¶ç”¨æˆäº¤é‡ä½œä¸ºæ›¿ä»£ï¼‰
                    if recent_volume > 0:
                        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ¢æ‰‹ç‡çš„è®¡ç®—é€»è¾‘
                        latest_values['turnover_rate'] = recent_volume / 1000000  # ç®€åŒ–çš„æ¢æ‰‹ç‡è®¡ç®—

                # æ³¢åŠ¨ç‡è®¡ç®—
                if len(price_data) >= 20:
                    returns = price_data['close'].pct_change().dropna()
                    volatility = returns.tail(20).std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
                    latest_values['volatility_20d'] = volatility

            # ç¡®ä¿current_priceå§‹ç»ˆè¢«è®¾ç½®
            if not price_data.empty and 'current_price' not in latest_values:
                latest_values['current_price'] = price_data['close'].iloc[-1]

            # è¿”å›ç»“æœï¼Œå¤ç”¨indicators_resultçš„ç»“æ„
            result = indicators_result
            result['latest_values'] = latest_values

            if result.get('errors'):
                self.logger.warning(f"âš ï¸ æŒ‡æ ‡è®¡ç®—å®Œæˆä½†æœ‰é”™è¯¯: {result['errors'][:3]}...")

            return result

        except Exception as e:
            self.logger.error(f"âŒ æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}

    def _get_latest_trading_date(self, target_date: Optional[str] = None, max_attempts: int = 5) -> str:
        """
        è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥æœŸ

        ä½¿ç”¨rqdatacçš„äº¤æ˜“æ—¥å†APIæ¥è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥ï¼Œè€Œä¸æ˜¯ä¾èµ–ä¼°å€¼æ•°æ®

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
            max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°

        Returns:
            æœ‰æ•ˆçš„äº¤æ˜“æ—¥æœŸ (YYYY-MM-DD)
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                fallback_date = (datetime.strptime(target_date, '%Y-%m-%d') if target_date
                               else datetime.now()).strftime('%Y-%m-%d')
                self.logger.warning(f"ğŸ“… rqdatacä¸å¯ç”¨ï¼Œä½¿ç”¨åå¤‡æ—¥æœŸ: {fallback_date}")
                return fallback_date

            # ç¡®å®šåŸºå‡†æ—¥æœŸ
            base_date = (datetime.strptime(target_date, '%Y-%m-%d') if target_date
                        else datetime.now()).date()

            # è·å–äº¤æ˜“æ—¥å† - ä½¿ç”¨rqdatacçš„get_trading_datesæ–¹æ³•
            start_date = (base_date - timedelta(days=15)).strftime('%Y-%m-%d')
            end_date = (base_date + timedelta(days=5)).strftime('%Y-%m-%d')

            trading_dates = rqdatac.get_trading_dates(
                start_date=start_date,
                end_date=end_date
            )

            if trading_dates is None or len(trading_dates) == 0:
                fallback_date = base_date.strftime('%Y-%m-%d')
                self.logger.warning(f"ğŸ“… æœªè·å–åˆ°äº¤æ˜“æ—¥å†ï¼Œä½¿ç”¨åŸºå‡†æ—¥æœŸ: {fallback_date}")
                return fallback_date

            # æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæˆ–æœ€è¿‘çš„äº¤æ˜“æ—¥
            for i in range(len(trading_dates) - 1, -1, -1):
                trade_date = trading_dates[i]

                # ç¡®ä¿trade_dateæ˜¯dateå¯¹è±¡
                if isinstance(trade_date, datetime):
                    trade_date = trade_date.date()
                elif isinstance(trade_date, str):
                    trade_date = datetime.strptime(trade_date, '%Y-%m-%d').date()
                elif not isinstance(trade_date, date):
                    # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç„¶åè§£æ
                    trade_date = datetime.strptime(str(trade_date), '%Y-%m-%d').date()

                if trade_date <= base_date:
                    return trade_date.strftime('%Y-%m-%d')

            fallback_date = base_date.strftime('%Y-%m-%d')
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„äº¤æ˜“æ—¥ï¼Œä½¿ç”¨åŸºå‡†æ—¥æœŸ: {fallback_date}")
            return fallback_date

        except Exception as e:
            fallback_date = (datetime.strptime(target_date, '%Y-%m-%d') if target_date
                           else datetime.now()).strftime('%Y-%m-%d')
            self.logger.error(f"âŒ è·å–äº¤æ˜“æ—¥æœŸå¤±è´¥ï¼Œä½¿ç”¨åå¤‡æ—¥æœŸ: {fallback_date}, é”™è¯¯: {e}")
            return fallback_date

    def _get_config_value(self, key: str, default=None):
        """
        è·å–é…ç½®å€¼

        Args:
            key: é…ç½®é”®
            default: é»˜è®¤å€¼

        Returns:
            é…ç½®å€¼æˆ–é»˜è®¤å€¼
        """
        return self.config.get(key, default)

    def _check_range(self, value, min_key: str, max_key: str) -> bool:
        """
        æ£€æŸ¥å€¼æ˜¯å¦åœ¨é…ç½®èŒƒå›´å†…

        Args:
            value: è¦æ£€æŸ¥çš„å€¼
            min_key: é…ç½®ä¸­çš„æœ€å°å€¼é”®å
            max_key: é…ç½®ä¸­çš„æœ€å¤§å€¼é”®å

        Returns:
            æ˜¯å¦åœ¨èŒƒå›´å†…
        """
        if value is None or pd.isna(value):
            return False

        min_val = self._get_config_value(min_key)
        max_val = self._get_config_value(max_key)

        if min_val is not None and value < min_val:
            return False
        if max_val is not None and value > max_val:
            return False

        return True

    def get_sync_status(self) -> Dict:
        """
        è·å–åŒæ­¥çŠ¶æ€

        Returns:
            åŒ…å«åŒæ­¥çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
        """
        return {
            'is_syncing': self.is_syncing,
            'last_sync_time': self.last_sync_time,
            'data_date': date.today().isoformat(),
            'basic_pool_exists': self.basic_pool_file.exists(),
            'watch_pool_exists': self.watch_pool_file.exists(),
            'core_pool_exists': self.core_pool_file.exists(),
            'basic_pool_count': self._get_file_count(self.basic_pool_file),
            'watch_pool_count': self._get_file_count(self.watch_pool_file),
            'core_pool_count': self._get_file_count(self.core_pool_file)
        }

    def _get_file_count(self, file_path) -> int:
        """
        è·å–æ–‡ä»¶ä¸­çš„è‚¡ç¥¨æ•°é‡

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            è‚¡ç¥¨æ•°é‡
        """
        try:
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'stocks' in data:
                        return len(data['stocks'])
                    elif isinstance(data, list):
                        return len(data)
            return 0
        except:
            return 0

    def load_all_pools(self, return_dict: bool = False) -> Union[Dict[str, pd.DataFrame], Dict[str, List[Dict]]]:
        """
        åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ± æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬

        é»˜è®¤è¿”å›DataFrameæ ¼å¼ä»¥å‡å°‘è½¬æ¢å¼€é”€ï¼Œå¯é€‰æ‹©è¿”å›å­—å…¸æ ¼å¼ä»¥ä¿æŒå‘åå…¼å®¹æ€§

        Args:
            return_dict: æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œé»˜è®¤ä¸ºFalseè¿”å›DataFrame

        Returns:
            åŒ…å«æ‰€æœ‰å±‚çº§è‚¡ç¥¨æ± çš„å­—å…¸ï¼Œæ ¼å¼ç”±return_dictå‚æ•°å†³å®š
        """
        try:
            result = {}

            # åŠ è½½åŸºç¡€å±‚
            basic_data = self.data_store.load_pool('basic')
            if not basic_data.empty:
                result['basic_layer'] = basic_data

            # åŠ è½½è§‚å¯Ÿå±‚
            watch_data = self.data_store.load_pool('watch')
            if not watch_data.empty:
                result['watch_layer'] = watch_data

            # åŠ è½½æ ¸å¿ƒå±‚
            core_data = self.data_store.load_pool('core')
            if not core_data.empty:
                result['core_layer'] = core_data

            # å¦‚æœéœ€è¦å­—å…¸æ ¼å¼ï¼Œè¿›è¡Œè½¬æ¢ï¼ˆåªåœ¨éœ€è¦æ—¶è¿›è¡Œï¼‰
            if return_dict:
                dict_result = {}
                for key, df in result.items():
                    if not df.empty:
                        # ä½¿ç”¨to_dict('records')è¿›è¡Œé«˜æ•ˆè½¬æ¢
                        dict_result[key] = df.to_dict('records')
                    else:
                        dict_result[key] = []
                return dict_result

            return result

        except Exception as e:
            self.logger.error(f"åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ± å¤±è´¥: {e}")
            if return_dict:
                return {'basic_layer': [], 'watch_layer': [], 'core_layer': []}
            else:
                return {
                    'basic_layer': pd.DataFrame(),
                    'watch_layer': pd.DataFrame(),
                    'core_layer': pd.DataFrame()
                }

    def save_basic_pool(self, pool_data: List[Dict]) -> bool:
        """
        ä¿å­˜åŸºç¡€è‚¡ç¥¨æ±  (ä½¿ç”¨datastore)

        Args:
            pool_data: è¦ä¿å­˜çš„è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # è®¾ç½®å†…éƒ¨å®ä¾‹å¹¶ä¿å­˜
        self.data_store.basic_pool = pd.DataFrame(pool_data)
        return self.data_store.save_basic_pool()

    def save_watch_pool(self, pool_data: List[Dict]) -> bool:
        """
        ä¿å­˜è§‚å¯Ÿè‚¡ç¥¨æ±  (ä½¿ç”¨datastore)

        Args:
            pool_data: è¦ä¿å­˜çš„è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # è®¾ç½®å†…éƒ¨å®ä¾‹å¹¶ä¿å­˜
        self.data_store.watch_pool = pd.DataFrame(pool_data)
        return self.data_store.save_watch_pool()

    def save_core_pool(self, pool_data: List[Dict]) -> bool:
        """
        ä¿å­˜æ ¸å¿ƒè‚¡ç¥¨æ±  (ä½¿ç”¨datastore)

        Args:
            pool_data: è¦ä¿å­˜çš„è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # è®¾ç½®å†…éƒ¨å®ä¾‹å¹¶ä¿å­˜
        self.data_store.core_pool = pd.DataFrame(pool_data)
        return self.data_store.save_core_pool()

    def save_pool(self, pool_type: str, pool_data: List[Dict]) -> bool:
        """
        ä¿å­˜æŒ‡å®šç±»å‹çš„è‚¡ç¥¨æ±  (ä½¿ç”¨datastore)

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')
            pool_data: è¦ä¿å­˜çš„è‚¡ç¥¨æ± æ•°æ®

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # è®¾ç½®å†…éƒ¨å®ä¾‹å¹¶ä¿å­˜
        df_data = pd.DataFrame(pool_data)
        if pool_type == 'basic':
            self.data_store.basic_pool = df_data
        elif pool_type == 'watch':
            self.data_store.watch_pool = df_data
        elif pool_type == 'core':
            self.data_store.core_pool = df_data

        return self.data_store.save_pool_by_name(pool_type)


# ============================================================================
# ä¸»ç¨‹åºå…¥å£ - ç›´æ¥å¯åŠ¨è‚¡ç¥¨æ± åŒæ­¥ç¨‹åº
# ============================================================================

# Initialize environment on module load
setup_environment()

# ============================================================================
# STOCK POOL DATA STORE - ç‹¬ç«‹æ•°æ®å­˜å‚¨å±‚
# ============================================================================

class StockPoolDataStore:
    """
    StockPoolä¸“ç”¨æ•°æ®å­˜å‚¨å™¨

    æ ¸å¿ƒåŠŸèƒ½:
    - ç‹¬ç«‹æ•°æ®ç¼“å­˜: é¿å…ä¸å…¶ä»–æ¨¡å—çš„æ–‡ä»¶æ“ä½œå†²çª
    - æ™ºèƒ½äº¤æ˜“æ—¥ç¡®å®š: åŸºäºäº¤æ˜“æ—¶æ®µçš„å…¨å±€æ—¥æœŸåˆ¤æ–­
    - ä¸‰å±‚è‚¡ç¥¨æ± ç®¡ç†: åŸºç¡€/è§‚å¯Ÿ/æ ¸å¿ƒæ± çš„å†…å­˜å’ŒæŒä¹…åŒ–ç®¡ç†
    - æ‰¹é‡æ•°æ®è·å–: ä¼˜åŒ–çš„RQDatacæ•°æ®è·å–æ¥å£

    è®¾è®¡ä¼˜åŠ¿:
    - è½»é‡çº§æ¶æ„: ä¸“æ³¨äºè‚¡ç¥¨æ± æ•°æ®çš„å­˜å‚¨å’Œè®¿é—®
    - é«˜æ€§èƒ½ç¼“å­˜: å†…å­˜ç¼“å­˜ + æ–‡ä»¶æŒä¹…åŒ–åŒé‡ä¿éšœ
    - æ™ºèƒ½é”™è¯¯å¤„ç†: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé™çº§ç­–ç•¥
    - æ¨¡å—åŒ–è®¾è®¡: ä¸å…¶ä»–ç»„ä»¶è§£è€¦ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•

    æ•°æ®æµ:
    RQDatac â†’ æ‰¹é‡ç¼“å­˜ â†’ å†…å­˜å®ä¾‹ â†’ æ–‡ä»¶æŒä¹…åŒ–
    """

    def __init__(self, cache_dir: str = "stockpool_cache"):
        """
        åˆå§‹åŒ–æ•°æ®å­˜å‚¨å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œç”¨äºå­˜å‚¨ä¸´æ—¶æ•°æ®æ–‡ä»¶

        Raises:
            OSError: å½“æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½•æ—¶æŠ›å‡º
        """
        logger.debug(f"ğŸ”§ åˆå§‹åŒ– StockPoolDataStore, ç¼“å­˜ç›®å½•: {cache_dir}")

        self.cache_dir = cache_dir
        self.data_dir = "stockpool_data"

        # åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
        try:
            os.makedirs(self.cache_dir, exist_ok=True)
            os.makedirs(self.data_dir, exist_ok=True)
            logger.debug(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {self.cache_dir}, æ•°æ®ç›®å½•: {self.data_dir}")
        except OSError as e:
            logger.error(f"âŒ æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½•: {e}")
            raise

        # åˆå§‹åŒ–æ•°æ®ç¼“å­˜ç»“æ„
        self._init_cache_structures()

        # åˆå§‹åŒ–è‚¡ç¥¨æ± å®ä¾‹
        self._init_pool_instances()

        # é¢„å…ˆç¡®å®šç›®æ ‡äº¤æ˜“æ—¥ï¼ˆå…¨å±€ä¼˜åŒ–ï¼‰
        self._determine_target_trading_date()

        logger.info(f"âœ… StockPoolDataStore åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“… é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date}")
        logger.debug(f"ğŸ—ï¸ è‚¡ç¥¨æ± å®ä¾‹çŠ¶æ€: basic={not self.basic_pool.empty}, watch={not self.watch_pool.empty}, core={not self.core_pool.empty}")

    def _init_cache_structures(self):
        """åˆå§‹åŒ–ç¼“å­˜æ•°æ®ç»“æ„"""
        logger.debug("ğŸ”§ åˆå§‹åŒ–ç¼“å­˜æ•°æ®ç»“æ„")

        # è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜
        self.stock_list_cache: Optional[pd.DataFrame] = None
        self.stock_list_timestamp: Optional[float] = None

        # Kçº¿æ•°æ®ç¼“å­˜ - {stock_code: DataFrame}
        self.kline_cache: Dict[str, pd.DataFrame] = {}

        # æ‰¹é‡æ•°æ®ç¼“å­˜ - {cache_key: DataFrame}
        self.batch_cache: Dict[str, pd.DataFrame] = {}

        logger.debug("âœ… ç¼“å­˜ç»“æ„åˆå§‹åŒ–å®Œæˆ")

    def _init_pool_instances(self):
        """åˆå§‹åŒ–è‚¡ç¥¨æ± å®ä¾‹"""
        logger.debug("ğŸ”§ åˆå§‹åŒ–è‚¡ç¥¨æ± å®ä¾‹")

        # ä¸‰å±‚è‚¡ç¥¨æ± çš„å†…å­˜å®ä¾‹
        self.basic_pool: pd.DataFrame = pd.DataFrame()      # åŸºç¡€è‚¡ç¥¨æ± 
        self.watch_pool: pd.DataFrame = pd.DataFrame()      # è§‚å¯Ÿè‚¡ç¥¨æ± 
        self.core_pool: pd.DataFrame = pd.DataFrame()       # æ ¸å¿ƒè‚¡ç¥¨æ± 

        # è‚¡ç¥¨æ± å…ƒæ•°æ®
        self.pool_metadata = {
            'basic_pool': {'last_updated': None, 'count': 0, 'date': None},
            'watch_pool': {'last_updated': None, 'count': 0, 'date': None},
            'core_pool': {'last_updated': None, 'count': 0, 'date': None}
        }

        # å…¨å±€äº¤æ˜“æ—¥ç¡®å®šå™¨
        self.target_trading_date: Optional[str] = None
        self.target_trading_date_determined = False

        logger.debug("âœ… è‚¡ç¥¨æ± å®ä¾‹åˆå§‹åŒ–å®Œæˆ")

    def load_data_from_file(self, filename: str) -> Optional[Dict]:
        """
        ä»æ–‡ä»¶åŠ è½½æ•°æ®

        Args:
            filename: æ–‡ä»¶å

        Returns:
            Dict: åŠ è½½çš„æ•°æ®æˆ–None
        """
        try:
            from pathlib import Path

            # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ç›®å½•
            search_dirs = [
                Path(self.data_dir),    # stockpool_data
                Path("data"),           # dataç›®å½•
                Path("../data")         # ç›¸å¯¹è·¯å¾„çš„dataç›®å½•
            ]

            for search_dir in search_dirs:
                filepath = search_dir / filename
                if filepath.exists():
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    logger.info(f"ğŸ“ æ•°æ®åŠ è½½æˆåŠŸ: {filename} (from {search_dir})")
                    return data

            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°
            logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return None

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ {filename}: {e}")
            return None

    def save_data_to_file(self, data: Dict, filename: str) -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶å

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            filepath = os.path.join(self.data_dir, filename)

            # åŸå­å†™å…¥æ“ä½œ
            temp_filepath = filepath + ".tmp"
            with open(temp_filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)

            # åŸå­ç§»åŠ¨
            if os.path.exists(filepath):
                os.remove(filepath)
            os.rename(temp_filepath, filepath)

            logger.info(f"ğŸ’¾ æ•°æ®ä¿å­˜æˆåŠŸ: {filename}")
            return True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥ {filename}: {e}")
            return False

    def save_pool(self, pool_type: str) -> bool:
        """
        æ ¹æ®ç±»å‹ä¿å­˜è‚¡ç¥¨æ± æ•°æ®åˆ°æ–‡ä»¶

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ

        Raises:
            ValueError: å½“pool_typeæ— æ•ˆæ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # æ ¹æ®pool_typeè·å–å¯¹åº”çš„æ± æ•°æ®å’Œé…ç½®
            if pool_type == 'basic':
                pool_data = self.basic_pool
                pool_name = 'åŸºç¡€è‚¡ç¥¨æ± '
                filename = 'basic_pool.json'
                metadata_key = 'basic_pool'
            elif pool_type == 'watch':
                pool_data = self.watch_pool
                pool_name = 'è§‚å¯Ÿè‚¡ç¥¨æ± '
                filename = 'watch_pool.json'
                metadata_key = 'watch_pool'
            elif pool_type == 'core':
                pool_data = self.core_pool
                pool_name = 'æ ¸å¿ƒè‚¡ç¥¨æ± '
                filename = 'core_pool.json'
                metadata_key = 'core_pool'
            else:
                raise ValueError(f"æ— æ•ˆçš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}ã€‚æ”¯æŒçš„ç±»å‹: 'basic', 'watch', 'core'")

            # æ£€æŸ¥æ± æ•°æ®æ˜¯å¦æœ‰æ•°æ®
            if pool_data.empty:
                logger.warning(f"âš ï¸ {pool_name}ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                return True

            # æ›´æ–°å…ƒæ•°æ®
            self.pool_metadata[metadata_key] = {
                'last_updated': datetime.now().isoformat(),
                'count': len(pool_data),
                'date': pool_data.iloc[0].get('date') if len(pool_data) > 0 else None
            }

            # ä¿å­˜åˆ°æ–‡ä»¶ - è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨æ ¼å¼
            data_list = pool_data.to_dict('records')
            data = {
                'type': f'{pool_type}_pool',
                'timestamp': datetime.now().isoformat(),
                'count': len(data_list),
                'data': data_list
            }

            success = self.save_data_to_file(data, filename)
            if success:
                logger.info(f"âœ… {pool_name}å·²ä¿å­˜: {len(pool_data)} åªè‚¡ç¥¨")
            return success

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜{pool_type}è‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False

    def save_basic_pool(self) -> bool:
        """
        ä¿å­˜åŸºç¡€è‚¡ç¥¨æ± æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self.save_pool('basic')

    def save_watch_pool(self) -> bool:
        """
        ä¿å­˜è§‚å¯Ÿè‚¡ç¥¨æ± æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self.save_pool('watch')

    def save_core_pool(self) -> bool:
        """
        ä¿å­˜æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self.save_pool('core')

    # ===== è‚¡ç¥¨æ± å®ä¾‹è®¿é—®æ–¹æ³• =====

    def get_pool(self, pool_type: str, copy: bool = True) -> pd.DataFrame:
        """
        æ ¹æ®ç±»å‹è·å–è‚¡ç¥¨æ± å®ä¾‹

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')
            copy: æ˜¯å¦è¿”å›æ‹·è´ï¼Œé»˜è®¤ä¸ºTrueä»¥ä¿æŒå‘åå…¼å®¹æ€§

        Returns:
            pd.DataFrame: æŒ‡å®šç±»å‹çš„è‚¡ç¥¨æ± æ•°æ®

        Raises:
            ValueError: å½“pool_typeæ— æ•ˆæ—¶æŠ›å‡ºå¼‚å¸¸
        """
        if pool_type == 'basic':
            if not self.basic_pool.empty:
                return self.basic_pool.copy() if copy else self.basic_pool
            else:
                return pd.DataFrame()
        elif pool_type == 'watch':
            if not self.watch_pool.empty:
                return self.watch_pool.copy() if copy else self.watch_pool
            else:
                return pd.DataFrame()
        elif pool_type == 'core':
            if not self.core_pool.empty:
                return self.core_pool.copy() if copy else self.core_pool
            else:
                return pd.DataFrame()
        else:
            raise ValueError(f"æ— æ•ˆçš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}ã€‚æ”¯æŒçš„ç±»å‹: 'basic', 'watch', 'core'")

    def get_basic_pool(self) -> pd.DataFrame:
        """
        è·å–åŸºç¡€è‚¡ç¥¨æ± å®ä¾‹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            pd.DataFrame: åŸºç¡€è‚¡ç¥¨æ± æ•°æ®
        """
        return self.get_pool('basic')

    def get_watch_pool(self) -> pd.DataFrame:
        """
        è·å–è§‚å¯Ÿè‚¡ç¥¨æ± å®ä¾‹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            pd.DataFrame: è§‚å¯Ÿè‚¡ç¥¨æ± æ•°æ®
        """
        return self.get_pool('watch')

    def get_core_pool(self) -> pd.DataFrame:
        """
        è·å–æ ¸å¿ƒè‚¡ç¥¨æ± å®ä¾‹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            pd.DataFrame: æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®
        """
        return self.get_pool('core')

    def get_all_pools(self) -> Dict[str, pd.DataFrame]:
        """
        è·å–æ‰€æœ‰è‚¡ç¥¨æ± å®ä¾‹

        Returns:
            Dict[str, pd.DataFrame]: åŒ…å«æ‰€æœ‰è‚¡ç¥¨æ± çš„å­—å…¸
        """
        return {
            'basic_pool': self.get_basic_pool(),
            'watch_pool': self.get_watch_pool(),
            'core_pool': self.get_core_pool()
        }

    def get_pool_info(self, pool_type: str) -> Dict:
        """
        è·å–æŒ‡å®šè‚¡ç¥¨æ± çš„ä¿¡æ¯

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            Dict: è‚¡ç¥¨æ± ä¿¡æ¯
        """
        if pool_type not in self.pool_metadata:
            return {'error': f'æœªçŸ¥çš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}'}

        info = self.pool_metadata[pool_type].copy()
        info['pool_type'] = pool_type

        # æ·»åŠ å®æ—¶æ•°æ®
        if pool_type == 'basic':
            info['current_count'] = len(self.basic_pool) if not self.basic_pool.empty else 0
        elif pool_type == 'watch':
            info['current_count'] = len(self.watch_pool) if not self.watch_pool.empty else 0
        elif pool_type == 'core':
            info['current_count'] = len(self.core_pool) if not self.core_pool.empty else 0

        return info

    def get_all_pool_info(self) -> Dict[str, Dict]:
        """
        è·å–æ‰€æœ‰è‚¡ç¥¨æ± çš„ä¿¡æ¯

        Returns:
            Dict[str, Dict]: æ‰€æœ‰è‚¡ç¥¨æ± çš„ä¿¡æ¯
        """
        return {
            'basic_pool': self.get_pool_info('basic'),
            'watch_pool': self.get_pool_info('watch'),
            'core_pool': self.get_pool_info('core')
        }

    def is_pool_available(self, pool_type: str) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šè‚¡ç¥¨æ± æ˜¯å¦å¯ç”¨

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            bool: è‚¡ç¥¨æ± æ˜¯å¦å¯ç”¨
        """
        if pool_type == 'basic':
            return not self.basic_pool.empty
        elif pool_type == 'watch':
            return not self.watch_pool.empty
        elif pool_type == 'core':
            return not self.core_pool.empty
        return False

    def retrieve_stock_codes_from_pool(self, pool_type: str) -> List[str]:
        """
        è·å–æŒ‡å®šè‚¡ç¥¨æ± ä¸­çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            List[str]: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        pool_data = pd.DataFrame()
        if pool_type == 'basic':
            pool_data = self.basic_pool
        elif pool_type == 'watch':
            pool_data = self.watch_pool
        elif pool_type == 'core':
            pool_data = self.core_pool

        if pool_data.empty:
            return []

        # ä»DataFrameä¸­æå–è‚¡ç¥¨ä»£ç åˆ—
        if 'stock_code' in pool_data.columns:
            return pool_data['stock_code'].dropna().tolist()
        else:
            return []

    def find_stock_in_pools_by_code(self, stock_code: str) -> pd.DataFrame:
        """
        åœ¨æ‰€æœ‰è‚¡ç¥¨æ± ä¸­æŸ¥æ‰¾æŒ‡å®šè‚¡ç¥¨

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 

        Returns:
            pd.DataFrame: åŒ…å«æ‰¾åˆ°çš„è‚¡ç¥¨ä¿¡æ¯ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€è¡Œï¼ŒåŒ…å«pool_typeåˆ—æ ‡è¯†æ¥æº
        """
        found_stocks = []

        # åœ¨åŸºç¡€æ± ä¸­æŸ¥æ‰¾
        if not self.basic_pool.empty and 'stock_code' in self.basic_pool.columns:
            mask = self.basic_pool['stock_code'] == stock_code
            if mask.any():
                stock_info = self.basic_pool.loc[mask].iloc[0].to_dict()
                stock_info['pool_type'] = 'basic'
                found_stocks.append(stock_info)

        # åœ¨è§‚å¯Ÿæ± ä¸­æŸ¥æ‰¾
        if not self.watch_pool.empty and 'stock_code' in self.watch_pool.columns:
            mask = self.watch_pool['stock_code'] == stock_code
            if mask.any():
                stock_info = self.watch_pool.loc[mask].iloc[0].to_dict()
                stock_info['pool_type'] = 'watch'
                found_stocks.append(stock_info)

        # åœ¨æ ¸å¿ƒæ± ä¸­æŸ¥æ‰¾
        if not self.core_pool.empty and 'stock_code' in self.core_pool.columns:
            mask = self.core_pool['stock_code'] == stock_code
            if mask.any():
                stock_info = self.core_pool.loc[mask].iloc[0].to_dict()
                stock_info['pool_type'] = 'core'
                found_stocks.append(stock_info)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è‚¡ç¥¨ï¼Œè¿”å›ç©ºDataFrame
        if not found_stocks:
            return pd.DataFrame()

        # å°†æ‰€æœ‰æ‰¾åˆ°çš„è‚¡ç¥¨ä¿¡æ¯åˆå¹¶æˆDataFrame
        result_df = pd.DataFrame(found_stocks)

        # é‡æ–°æ’åˆ—åˆ—ï¼Œå°†pool_typeåˆ—æ”¾åœ¨å‰é¢
        if 'pool_type' in result_df.columns:
            cols = ['pool_type'] + [col for col in result_df.columns if col != 'pool_type']
            result_df = result_df[cols]

        return result_df

    def clear_pool_instances(self) -> None:
        """
        æ¸…ç©ºæ‰€æœ‰è‚¡ç¥¨æ± å®ä¾‹ï¼ˆå†…å­˜ä¸­çš„æ•°æ®ï¼‰
        """
        self.basic_pool = pd.DataFrame()
        self.watch_pool = pd.DataFrame()
        self.core_pool = pd.DataFrame()

        # é‡ç½®å…ƒæ•°æ®
        for pool_type in self.pool_metadata:
            self.pool_metadata[pool_type] = {
                'last_updated': None,
                'count': 0,
                'date': None
            }

        logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰è‚¡ç¥¨æ± å®ä¾‹")

    def reload_pools_from_files(self) -> bool:
        """
        ä»æ–‡ä»¶é‡æ–°åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ± åˆ°å†…å­˜å®ä¾‹

        Returns:
            bool: é‡æ–°åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            success_count = 0

            # é‡æ–°åŠ è½½åŸºç¡€æ± 
            if self.load_basic_pool():
                success_count += 1

            # é‡æ–°åŠ è½½è§‚å¯Ÿæ± 
            if self.load_watch_pool():
                success_count += 1

            # é‡æ–°åŠ è½½æ ¸å¿ƒæ± 
            if self.load_core_pool():
                success_count += 1

            logger.info(f"âœ… å·²ä»æ–‡ä»¶é‡æ–°åŠ è½½ {success_count} ä¸ªè‚¡ç¥¨æ± åˆ°å†…å­˜å®ä¾‹")
            return success_count > 0

        except Exception as e:
            logger.error(f"âŒ ä»æ–‡ä»¶é‡æ–°åŠ è½½è‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False

    def load_pool(self, pool_type: str) -> pd.DataFrame:
        """
        åŠ è½½æŒ‡å®šç±»å‹çš„è‚¡ç¥¨æ± 

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            pd.DataFrame: è‚¡ç¥¨æ± æ•°æ®
        """
        try:
            from pathlib import Path

            # æ ¹æ®pool_typeç¡®å®šæ–‡ä»¶åå’Œæ—¥å¿—ä¿¡æ¯
            pool_configs = {
                'basic': {
                    'filename': 'basic_pool.json',
                    'display_name': 'åŸºç¡€è‚¡ç¥¨æ± ',
                    'emoji': 'ğŸ“‚'
                },
                'watch': {
                    'filename': 'watch_pool.json',
                    'display_name': 'è§‚å¯Ÿè‚¡ç¥¨æ± ',
                    'emoji': 'ğŸ‘€'
                },
                'core': {
                    'filename': 'core_pool.json',
                    'display_name': 'æ ¸å¿ƒè‚¡ç¥¨æ± ',
                    'emoji': 'â­'
                }
            }

            if pool_type not in pool_configs:
                logger.error(f"âŒ ä¸æ”¯æŒçš„è‚¡ç¥¨æ± ç±»å‹: {pool_type}")
                return pd.DataFrame()

            config = pool_configs[pool_type]

            # ä¼˜å…ˆä»ä¿å­˜ç›®å½•åŠ è½½ï¼ˆstockpool_dataï¼‰ï¼Œç„¶åå°è¯•å…¶ä»–ç›®å½•
            search_paths = [
                Path(self.data_dir) / config['filename'],  # ä¼˜å…ˆï¼šstockpool_dataç›®å½•
                Path("data") / config['filename'],         # å¤‡é€‰ï¼šdataç›®å½•
                Path("../data") / config['filename']       # å¤‡é€‰ï¼šä¸Šçº§dataç›®å½•
            ]

            for pool_file in search_paths:
                if pool_file.exists():
                    with open(pool_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    # è§£æä¿å­˜çš„æ•°æ®æ ¼å¼ï¼ˆåªæ”¯æŒæ–°æ ¼å¼ï¼‰
                    if isinstance(data, dict) and 'data' in data:
                        # è¿™æ˜¯save_poolä¿å­˜çš„æ ¼å¼
                        stocks = data['data']
                        df = pd.DataFrame(stocks)
                        logger.info(f"{config['emoji']} åŠ è½½{config['display_name']}: {len(df)} åªè‚¡ç¥¨ (from {pool_file.parent})")
                        return df
                    # å¦‚æœæ•°æ®ç›´æ¥æ˜¯åˆ—è¡¨æ ¼å¼
                    elif isinstance(data, list):
                        df = pd.DataFrame(data)
                        logger.info(f"{config['emoji']} åŠ è½½{config['display_name']}: {len(df)} åªè‚¡ç¥¨ (from {pool_file.parent})")
                        return df
                    else:
                        logger.warning(f"âš ï¸ {config['display_name']}æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®: {pool_file}")
                        continue

            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°æ–‡ä»¶
            logger.warning(f"âš ï¸ {config['display_name']}æ–‡ä»¶ä¸å­˜åœ¨")
            return pd.DataFrame()

        except Exception as e:
            logger.error(f"âŒ åŠ è½½{config['display_name']}å¤±è´¥: {e}")
            return pd.DataFrame()
        
    def save_pool_by_name(self, pool_type: str) -> bool:
        """
        é€šç”¨ä¿å­˜è‚¡ç¥¨æ± æ–¹æ³•ï¼ˆä½¿ç”¨å†…éƒ¨å®ä¾‹ï¼‰

        Args:
            pool_type: è‚¡ç¥¨æ± ç±»å‹ ('basic', 'watch', 'core')

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self.save_pool(pool_type)

    def load_basic_pool(self) -> bool:
        """
        åŠ è½½åŸºç¡€è‚¡ç¥¨æ± æ•°æ®åˆ°å†…éƒ¨å®ä¾‹

        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            data = self.load_pool('basic')
            if not data.empty:
                self.basic_pool = data
                self.pool_metadata['basic_pool'] = {
                    'last_updated': datetime.now().isoformat(),
                    'count': len(data),
                    'date': data.iloc[0].get('date') if len(data) > 0 else None
                }
                logger.info(f"âœ… åŸºç¡€è‚¡ç¥¨æ± å·²åŠ è½½åˆ°å†…å­˜: {len(data)} åªè‚¡ç¥¨")
                return True
            else:
                logger.warning("âš ï¸ åŸºç¡€è‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
                return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åŸºç¡€è‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False

    def load_watch_pool(self) -> bool:
        """
        åŠ è½½è§‚å¯Ÿè‚¡ç¥¨æ± æ•°æ®åˆ°å†…éƒ¨å®ä¾‹

        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            data = self.load_pool('watch')
            if not data.empty:
                self.watch_pool = data
                self.pool_metadata['watch_pool'] = {
                    'last_updated': datetime.now().isoformat(),
                    'count': len(data),
                    'date': data.iloc[0].get('date') if len(data) > 0 else None
                }
                logger.info(f"âœ… è§‚å¯Ÿè‚¡ç¥¨æ± å·²åŠ è½½åˆ°å†…å­˜: {len(data)} åªè‚¡ç¥¨")
                return True
            else:
                logger.warning("âš ï¸ è§‚å¯Ÿè‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
                return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è§‚å¯Ÿè‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False

    def load_core_pool(self) -> bool:
        """
        åŠ è½½æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®åˆ°å†…éƒ¨å®ä¾‹

        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            data = self.load_pool('core')
            if not data.empty:
                self.core_pool = data
                self.pool_metadata['core_pool'] = {
                    'last_updated': datetime.now().isoformat(),
                    'count': len(data),
                    'date': data.iloc[0].get('date') if len(data) > 0 else None
                }
                logger.info(f"âœ… æ ¸å¿ƒè‚¡ç¥¨æ± å·²åŠ è½½åˆ°å†…å­˜: {len(data)} åªè‚¡ç¥¨")
                return True
            else:
                logger.warning("âš ï¸ æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
                return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ ¸å¿ƒè‚¡ç¥¨æ± å¤±è´¥: {e}")
            return False




    def filter_by_type(self, df: pd.DataFrame, include_st: bool = False, status_filter: str = 'Active') -> pd.Series:
        """
        Aè‚¡è‚¡ç¥¨è¿‡æ»¤å™¨ - è¿”å›å¸ƒå°”Seriesç”¨äºè¿‡æ»¤

        Args:
            df: è¦è¿‡æ»¤çš„DataFrame
            include_st: æ˜¯å¦åŒ…å«STè‚¡ç¥¨ï¼Œé»˜è®¤Falseï¼ˆæ’é™¤STè‚¡ç¥¨ï¼‰
            status_filter: çŠ¶æ€è¿‡æ»¤å™¨ï¼Œé»˜è®¤'Active'ï¼ˆåªåŒ…å«æ´»è·ƒè‚¡ç¥¨ï¼‰

        Returns:
            pd.Series: å¸ƒå°”Seriesï¼Œè¡¨ç¤ºæ¯è¡Œæ˜¯å¦æ»¡è¶³è¿‡æ»¤æ¡ä»¶
        """
        try:
            if df is None or df.empty:
                logger.warning("âš ï¸ è¾“å…¥DataFrameä¸ºç©º")
                return pd.Series([], dtype=bool)

            # åˆå§‹åŒ–å…¨ä¸ºTrueçš„å¸ƒå°”Series
            mask = pd.Series(True, index=df.index)

            # è¿‡æ»¤ä¸ºAè‚¡è‚¡ç¥¨ï¼ˆä¸Šæµ·å’Œæ·±åœ³äº¤æ˜“æ‰€ï¼‰
            if 'order_book_id' in df.columns:
                a_stock_mask = (
                    df['order_book_id'].str.endswith('.XSHE') |
                    df['order_book_id'].str.endswith('.XSHG')
                )
                mask = mask & a_stock_mask

            # æ ¹æ®çŠ¶æ€è¿‡æ»¤
            if status_filter and status_filter != 'All' and 'status' in df.columns:
                status_mask = df['status'] == status_filter
                mask = mask & status_mask

            # æ ¹æ®STè‚¡ç¥¨è¿‡æ»¤
            if not include_st and 'abbrev_symbol' in df.columns:
                st_mask = ~df['abbrev_symbol'].str.contains('ST', na=False, case=False)
                mask = mask & st_mask

            logger.debug(f"âœ… Aè‚¡ç±»å‹è¿‡æ»¤: {mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")
            return mask

        except Exception as e:
            logger.error(f"âŒ Aè‚¡ç±»å‹è¿‡æ»¤å¤±è´¥: {e}")
            return pd.Series([], dtype=bool)

    def filter_by_type_to_cache(self, include_st: bool = False, status_filter: str = 'Active') -> bool:
        """
        Aè‚¡è‚¡ç¥¨è¿‡æ»¤å™¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰

        Args:
            include_st: æ˜¯å¦åŒ…å«STè‚¡ç¥¨ï¼Œé»˜è®¤Falseï¼ˆæ’é™¤STè‚¡ç¥¨ï¼‰
            status_filter: çŠ¶æ€è¿‡æ»¤å™¨ï¼Œé»˜è®¤'Active'ï¼ˆåªåŒ…å«æ´»è·ƒè‚¡ç¥¨ï¼‰

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            # è¿‡æ»¤ä¸ºAè‚¡è‚¡ç¥¨ï¼ˆä¸Šæµ·å’Œæ·±åœ³äº¤æ˜“æ‰€ï¼‰
            a_stocks = self.stock_list_cache[
                (self.stock_list_cache['order_book_id'].str.endswith('.XSHE')) |
                (self.stock_list_cache['order_book_id'].str.endswith('.XSHG'))
            ].copy()

            if a_stocks.empty:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°Aè‚¡è‚¡ç¥¨æ•°æ®")
                self.stock_list_cache = pd.DataFrame()
                return False

            # æ ¹æ®çŠ¶æ€è¿‡æ»¤
            if status_filter and 'status' in a_stocks.columns:
                a_stocks = a_stocks[a_stocks['status'] == status_filter]

            # æ ¹æ®STè‚¡ç¥¨è¿‡æ»¤
            if not include_st and 'abbrev_symbol' in a_stocks.columns:
                a_stocks = a_stocks[
                    ~a_stocks['abbrev_symbol'].str.contains('ST', na=False, case=False)
                ]

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = a_stocks
            logger.info(f"âœ… Aè‚¡è¿‡æ»¤å®Œæˆ: {len(a_stocks)} åªè‚¡ç¥¨ (STè¿‡æ»¤: {not include_st}, çŠ¶æ€: {status_filter})")
            return True

        except Exception as e:
            logger.error(f"âŒ Aè‚¡è‚¡ç¥¨è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_market_cap_to_cache(self, min_cap: float = None, max_cap: float = None) -> bool:
        """
        æŒ‰å¸‚å€¼è¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰

        Args:
            min_cap: æœ€å°å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            max_cap: æœ€å¤§å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            filtered_df = self.stock_list_cache.copy()

            # æŒ‰å¸‚å€¼ä¸‹é™è¿‡æ»¤
            if min_cap is not None and 'market_cap' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['market_cap'] >= min_cap]

            # æŒ‰å¸‚å€¼ä¸Šé™è¿‡æ»¤
            if max_cap is not None and 'market_cap' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['market_cap'] <= max_cap]

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ’° å¸‚å€¼è¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (å¸‚å€¼èŒƒå›´: {min_cap} - {max_cap})")
            return True

        except Exception as e:
            logger.error(f"âŒ å¸‚å€¼è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_market_cap(self, min_cap: float = None, max_cap: float = None) -> bool:
        """
        æŒ‰å¸‚å€¼è¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®

        Args:
            min_cap: æœ€å°å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            max_cap: æœ€å¤§å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            filtered_df = self.stock_list_cache.copy()

            # æŒ‰å¸‚å€¼ä¸‹é™è¿‡æ»¤
            if min_cap is not None and 'market_cap' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['market_cap'] >= min_cap]

            # æŒ‰å¸‚å€¼ä¸Šé™è¿‡æ»¤
            if max_cap is not None and 'market_cap' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['market_cap'] <= max_cap]

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ’° å¸‚å€¼è¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (å¸‚å€¼èŒƒå›´: {min_cap} - {max_cap})")
            return True

        except Exception as e:
            logger.error(f"âŒ å¸‚å€¼è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_industry_to_cache(self, industries: List[str] = None) -> bool:
        """
        æŒ‰è¡Œä¸šè¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰

        Args:
            industries: è¡Œä¸šåˆ—è¡¨

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            if not industries:
                logger.info("â„¹ï¸ æ— è¡Œä¸šè¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡è¡Œä¸šè¿‡æ»¤")
                return True

            filtered_df = self.stock_list_cache.copy()

            # æŒ‰è¡Œä¸šè¿‡æ»¤
            if 'industry_name' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['industry_name'].isin(industries)]
            elif 'citics_industry_name' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['citics_industry_name'].isin(industries)]
            else:
                logger.warning("âš ï¸ è‚¡ç¥¨æ•°æ®ä¸­æ²¡æœ‰è¡Œä¸šä¿¡æ¯å­—æ®µ")
                return False

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ­ è¡Œä¸šè¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (è¡Œä¸š: {industries})")
            return True

        except Exception as e:
            logger.error(f"âŒ è¡Œä¸šè¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_exchange_to_cache(self, exchange: str = None) -> bool:
        """
        æŒ‰äº¤æ˜“æ‰€è¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰

        Args:
            exchange: äº¤æ˜“æ‰€ä»£ç  ('XSHE' æˆ– 'XSHG')

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            if not exchange:
                logger.info("â„¹ï¸ æ— äº¤æ˜“æ‰€è¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡äº¤æ˜“æ‰€è¿‡æ»¤")
                return True

            if exchange not in ['XSHE', 'XSHG']:
                logger.error(f"âŒ ä¸æ”¯æŒçš„äº¤æ˜“æ‰€: {exchange}")
                return False

            filtered_df = self.stock_list_cache[
                self.stock_list_cache['order_book_id'].str.endswith(f'.{exchange}')
            ].copy()

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ¢ äº¤æ˜“æ‰€è¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (äº¤æ˜“æ‰€: {exchange})")
            return True

        except Exception as e:
            logger.error(f"âŒ äº¤æ˜“æ‰€è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_industry(self, industries: List[str] = None) -> bool:
        """
        æŒ‰è¡Œä¸šè¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®

        Args:
            industries: è¡Œä¸šåˆ—è¡¨

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            if not industries:
                logger.info("â„¹ï¸ æ— è¡Œä¸šè¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡è¡Œä¸šè¿‡æ»¤")
                return True

            filtered_df = self.stock_list_cache.copy()

            # æŒ‰è¡Œä¸šè¿‡æ»¤
            if 'industry_name' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['industry_name'].isin(industries)]
            elif 'citics_industry_name' in filtered_df.columns:
                filtered_df = filtered_df[filtered_df['citics_industry_name'].isin(industries)]
            else:
                logger.warning("âš ï¸ è‚¡ç¥¨æ•°æ®ä¸­æ²¡æœ‰è¡Œä¸šä¿¡æ¯å­—æ®µ")
                return False

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ­ è¡Œä¸šè¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (è¡Œä¸š: {industries})")
            return True

        except Exception as e:
            logger.error(f"âŒ è¡Œä¸šè¿‡æ»¤å¤±è´¥: {e}")
            return False

    def filter_by_exchange(self, exchange: str = None) -> bool:
        """
        æŒ‰äº¤æ˜“æ‰€è¿‡æ»¤è‚¡ç¥¨ - ç›´æ¥æ“ä½œå†…éƒ¨è‚¡ç¥¨æ•°æ®

        Args:
            exchange: äº¤æ˜“æ‰€ä»£ç  ('XSHE' æˆ– 'XSHG')

        Returns:
            bool: è¿‡æ»¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.stock_list_cache is None or self.stock_list_cache.empty:
                logger.warning("âš ï¸ å†…éƒ¨è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨fetch_stock_list()è·å–æ•°æ®")
                return False

            if not exchange:
                logger.info("â„¹ï¸ æ— äº¤æ˜“æ‰€è¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡äº¤æ˜“æ‰€è¿‡æ»¤")
                return True

            if exchange not in ['XSHE', 'XSHG']:
                logger.error(f"âŒ ä¸æ”¯æŒçš„äº¤æ˜“æ‰€: {exchange}")
                return False

            filtered_df = self.stock_list_cache[
                self.stock_list_cache['order_book_id'].str.endswith(f'.{exchange}')
            ].copy()

            # æ›´æ–°å†…éƒ¨ç¼“å­˜
            self.stock_list_cache = filtered_df
            logger.info(f"ğŸ¢ äº¤æ˜“æ‰€è¿‡æ»¤å®Œæˆ: {len(filtered_df)} åªè‚¡ç¥¨ (äº¤æ˜“æ‰€: {exchange})")
            return True

        except Exception as e:
            logger.error(f"âŒ äº¤æ˜“æ‰€è¿‡æ»¤å¤±è´¥: {e}")
            return False

    def apply_filters_to_cache(self, filters: Dict[str, Any] = None) -> bool:
        """
        åº”ç”¨å¤šä¸ªè¿‡æ»¤å™¨åˆ°å†…éƒ¨ç¼“å­˜ - é“¾å¼è¿‡æ»¤ï¼ˆæ—§ç‰ˆå‡½æ•°ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰

        Args:
            filters: è¿‡æ»¤å™¨é…ç½®å­—å…¸
                {
                    'include_st': bool,          # æ˜¯å¦åŒ…å«STè‚¡ç¥¨
                    'status_filter': str,        # çŠ¶æ€è¿‡æ»¤å™¨
                    'min_market_cap': float,     # æœ€å°å¸‚å€¼
                    'max_market_cap': float,     # æœ€å¤§å¸‚å€¼
                    'industries': List[str],     # è¡Œä¸šåˆ—è¡¨
                    'exchange': str              # äº¤æ˜“æ‰€
                }

        Returns:
            bool: æ‰€æœ‰è¿‡æ»¤å™¨æ˜¯å¦éƒ½åº”ç”¨æˆåŠŸ
        """
        try:
            if not filters:
                logger.info("â„¹ï¸ æ— è¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡è¿‡æ»¤")
                return True

            success_count = 0
            total_filters = len(filters)

            # Aè‚¡åŸºç¡€è¿‡æ»¤
            if 'include_st' in filters or 'status_filter' in filters:
                include_st = filters.get('include_st', False)
                status_filter = filters.get('status_filter', 'Active')
                if self.filter_by_type_to_cache(include_st=include_st, status_filter=status_filter):
                    success_count += 1

            # å¸‚å€¼è¿‡æ»¤
            if 'min_market_cap' in filters or 'max_market_cap' in filters:
                min_cap = filters.get('min_market_cap')
                max_cap = filters.get('max_market_cap')
                if self.filter_by_market_cap_to_cache(min_cap=min_cap, max_cap=max_cap):
                    success_count += 1

            # è¡Œä¸šè¿‡æ»¤
            if 'industries' in filters:
                industries = filters['industries']
                if self.filter_by_industry_to_cache(industries=industries):
                    success_count += 1

            # äº¤æ˜“æ‰€è¿‡æ»¤
            if 'exchange' in filters:
                exchange = filters['exchange']
                if self.filter_by_exchange_to_cache(exchange=exchange):
                    success_count += 1

            logger.info(f"ğŸ¯ ç»¼åˆè¿‡æ»¤å®Œæˆ: {success_count}/{total_filters} ä¸ªè¿‡æ»¤å™¨æˆåŠŸåº”ç”¨")
            return success_count == total_filters

        except Exception as e:
            logger.error(f"âŒ ç»¼åˆè¿‡æ»¤å¤±è´¥: {e}")
            return False



    def fetch_stock_list(self, filters: Dict[str, Any] = None) -> Optional[pd.DataFrame]:
        """
        è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨ (å¸¦ç¼“å­˜å’Œè¿‡æ»¤)

        Args:
            filters: è¿‡æ»¤å™¨é…ç½®å­—å…¸
                {
                    'include_st': bool,          # æ˜¯å¦åŒ…å«STè‚¡ç¥¨ï¼Œé»˜è®¤False
                    'status_filter': str,        # çŠ¶æ€è¿‡æ»¤å™¨ï¼Œé»˜è®¤'Active'
                    'min_market_cap': float,     # æœ€å°å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
                    'max_market_cap': float,     # æœ€å¤§å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
                    'industries': List[str],     # è¡Œä¸šåˆ—è¡¨
                    'exchange': str              # äº¤æ˜“æ‰€ ('XSHE' æˆ– 'XSHG')
                }

        Returns:
            DataFrame: æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨æˆ–Noneï¼ŒåŒ…å«è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

        ç¤ºä¾‹:
            # è·å–é»˜è®¤è¿‡æ»¤çš„è‚¡ç¥¨ï¼ˆæ´»è·ƒéSTè‚¡ç¥¨ï¼‰
            stocks_df = datastore.fetch_stock_list()

            # è·å–åŒ…å«STè‚¡ç¥¨çš„è‚¡ç¥¨
            stocks_with_st = datastore.fetch_stock_list({
                'include_st': True
            })

            # è·å–å¤§å¸‚å€¼è‚¡ç¥¨ï¼ˆå¸‚å€¼>50äº¿ï¼‰
            large_cap_stocks = datastore.fetch_stock_list({
                'min_market_cap': 500000  # 50äº¿ = 500000ä¸‡å…ƒ
            })

            # è·å–ç‰¹å®šè¡Œä¸šçš„è‚¡ç¥¨
            tech_stocks = datastore.fetch_stock_list({
                'industries': ['è®¡ç®—æœº', 'é€šä¿¡', 'ç”µå­']
            })

            # è·å–æ·±åœ³äº¤æ˜“æ‰€çš„è‚¡ç¥¨
            sz_stocks = datastore.fetch_stock_list({
                'exchange': 'XSHE'
            })

            # ç»¼åˆè¿‡æ»¤ï¼šéSTã€å¤§å¸‚å€¼ã€ç§‘æŠ€è¡Œä¸šã€æ·±åœ³äº¤æ˜“æ‰€
            filtered_stocks = datastore.fetch_stock_list({
                'include_st': False,
                'min_market_cap': 100000,  # 10äº¿
                'industries': ['è®¡ç®—æœº', 'é€šä¿¡'],
                'exchange': 'XSHE'
            })
        """
        try:
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆä¸€æ¬¡æ€§è¿è¡Œï¼Œæ— éœ€è¿‡æœŸæ£€æŸ¥ï¼‰
            if (self.stock_list_cache is not None and
                isinstance(self.stock_list_cache, pd.DataFrame) and
                not self.stock_list_cache.empty):
                logger.debug(f"âœ… ä½¿ç”¨ç¼“å­˜çš„Aè‚¡è‚¡ç¥¨åˆ—è¡¨: {len(self.stock_list_cache)} åªè‚¡ç¥¨")
                return self.stock_list_cache.copy()

            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ rqdatacä¸å¯ç”¨")
                return None

            # è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
            all_stocks = rqdatac.all_instruments(type='Stock', market='cn')

            if all_stocks is None or all_stocks.empty:
                logger.warning("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨æ•°æ®")
                return None

            # å…ˆåº”ç”¨è¿‡æ»¤å™¨ï¼Œç„¶åè®¾ç½®ç¼“å­˜
            filtered_stocks = all_stocks

            # åº”ç”¨è¿‡æ»¤å™¨
            if filters:
                filter_mask = self.apply_filters(all_stocks, filters)
                if filter_mask.empty:
                    logger.warning("âš ï¸ è¿‡æ»¤å™¨åº”ç”¨å¤±è´¥")
                    return None
                # ä½¿ç”¨å¸ƒå°”Seriesè¿‡æ»¤DataFrame
                filtered_stocks = all_stocks[filter_mask]
            else:
                # é»˜è®¤è¿‡æ»¤ï¼šåªä¿ç•™æ´»è·ƒè‚¡ç¥¨ï¼ˆæ’é™¤é€€å¸‚è‚¡ç¥¨ï¼‰
                default_filters = {'status_filter': 'Active'}
                filter_mask = self.apply_filters(all_stocks, default_filters)
                if filter_mask.empty:
                    logger.warning("âš ï¸ é»˜è®¤è¿‡æ»¤å™¨åº”ç”¨å¤±è´¥")
                    return None
                # ä½¿ç”¨å¸ƒå°”Seriesè¿‡æ»¤DataFrame
                filtered_stocks = all_stocks[filter_mask]

            # è®¾ç½®è¿‡æ»¤åçš„ç»“æœåˆ°ç¼“å­˜
            self.stock_list_cache = filtered_stocks

            if filtered_stocks.empty:
                logger.warning("âš ï¸ è¿‡æ»¤åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„Aè‚¡è‚¡ç¥¨")
                return None

            # æ›´æ–°æ—¶é—´æˆ³
            self.stock_list_timestamp = time.time()

            filter_desc = "é»˜è®¤è¿‡æ»¤" if not filters else f"è‡ªå®šä¹‰è¿‡æ»¤({len(filters)}ä¸ªæ¡ä»¶)"
            logger.info(f"ğŸ“‹ è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨: {len(filtered_stocks)} åªè‚¡ç¥¨ ({filter_desc})")
            return filtered_stocks

        except Exception as e:
            logger.error(f"âŒ è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def apply_filters(self, df: pd.DataFrame, filters: Dict[str, Any]) -> pd.Series:
        """
        åº”ç”¨è¿‡æ»¤å™¨å¹¶è¿”å›å¸ƒå°”Series

        Args:
            df: è¦è¿‡æ»¤çš„DataFrame
            filters: è¿‡æ»¤å™¨é…ç½®å­—å…¸

        Returns:
            pd.Series: å¸ƒå°”Seriesï¼Œè¡¨ç¤ºæ¯è¡Œæ˜¯å¦æ»¡è¶³è¿‡æ»¤æ¡ä»¶
        """
        try:
            if df is None or df.empty:
                logger.warning("âš ï¸ æ²¡æœ‰è‚¡ç¥¨æ•°æ®å¯ä¾›è¿‡æ»¤")
                return pd.Series([], dtype=bool)

            # åˆå§‹åŒ–å…¨ä¸ºTrueçš„å¸ƒå°”Series
            mask = pd.Series(True, index=df.index)

            # 1. STè‚¡ç¥¨è¿‡æ»¤
            if not filters.get('include_st', False):
                # è¿‡æ»¤æ‰STè‚¡ç¥¨ï¼ˆåç§°åŒ…å«STæˆ–*STï¼‰
                st_mask = ~df['symbol'].str.contains(r'ST|\*ST', case=False, na=False)
                mask = mask & st_mask
                logger.debug(f"ğŸ“Š STè¿‡æ»¤: {st_mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")

            # 2. çŠ¶æ€è¿‡æ»¤
            status_filter = filters.get('status_filter', 'Active')
            if status_filter != 'All':
                status_mask = df['status'] == status_filter
                mask = mask & status_mask
                logger.debug(f"ğŸ“Š çŠ¶æ€è¿‡æ»¤({status_filter}): {status_mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")

            # 3. å¸‚å€¼è¿‡æ»¤
            min_market_cap = filters.get('min_market_cap')
            max_market_cap = filters.get('max_market_cap')

            if min_market_cap is not None or max_market_cap is not None:
                # è¿™é‡Œéœ€è¦è·å–å¸‚å€¼æ•°æ®ï¼Œæš‚æ—¶è·³è¿‡å¸‚å€¼è¿‡æ»¤
                logger.debug("ğŸ“Š å¸‚å€¼è¿‡æ»¤: æš‚ä¸æ”¯æŒï¼ˆéœ€è¦å¸‚å€¼æ•°æ®ï¼‰")

            # 4. è¡Œä¸šè¿‡æ»¤
            industries = filters.get('industries')
            if industries:
                industry_mask = df['industry_code'].isin(industries)
                mask = mask & industry_mask
                logger.debug(f"ğŸ“Š è¡Œä¸šè¿‡æ»¤({industries}): {industry_mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")

            # 5. äº¤æ˜“æ‰€è¿‡æ»¤
            exchange = filters.get('exchange')
            if exchange:
                exchange_mask = df['exchange'] == exchange
                mask = mask & exchange_mask
                logger.debug(f"ğŸ“Š äº¤æ˜“æ‰€è¿‡æ»¤({exchange}): {exchange_mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ¡ä»¶")

            logger.info(f"âœ… è¿‡æ»¤å™¨åº”ç”¨æˆåŠŸ: {mask.sum()} åªè‚¡ç¥¨æ»¡è¶³æ‰€æœ‰æ¡ä»¶")
            return mask

        except Exception as e:
            logger.error(f"âŒ åº”ç”¨è¿‡æ»¤å™¨å¤±è´¥: {e}")
            return pd.Series([], dtype=bool)

    def _fetch_price_series(self, stock_code: str, start_date: str, end_date: str, force_refresh: bool = False) -> Optional[pd.DataFrame]:
        """
        è·å–Kçº¿æ•°æ® (ä¼˜åŒ–åçš„ç¼“å­˜é€»è¾‘)

        ç¼“å­˜ç­–ç•¥:
        - å†…å­˜ç¼“å­˜ï¼šå­˜å‚¨å®Œæ•´æ•°æ®ç”¨äºåç»­è®¡ç®—
        - ç£ç›˜ç¼“å­˜ï¼šæŒä¹…åŒ–å­˜å‚¨ï¼Œå‡å°‘ç½‘ç»œè®¿é—®
        - æ¯æ¬¡ä»ç½‘ç»œè·å–æ•°æ®ååŒæ­¥ä¿å­˜åˆ°ç£ç›˜
        - ä¸‹æ¬¡è·å–æ—¶å…ˆæ£€æŸ¥ç£ç›˜ç¼“å­˜ï¼Œå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®åˆ™åŠ è½½åˆ°å†…å­˜ï¼Œä¸å†è®¿é—®ç½‘ç»œ
        - ä¸è€ƒè™‘å¢é‡æ›´æ–°ï¼Œæ¯æ¬¡éƒ½æ‹‰å®Œæ•´æ•°æ®
        - æ”¯æŒå¼ºåˆ¶åˆ·æ–°å‚æ•°

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            force_refresh: æ˜¯å¦å¼ºåˆ¶ä»ç½‘ç»œè·å–æœ€æ–°æ•°æ®

        Returns:
            DataFrame: Kçº¿æ•°æ®æˆ–None
        """
        import time
        start_time = time.time()

        # ç”Ÿæˆç¼“å­˜é”®å’Œæ–‡ä»¶åï¼ˆæç®€è®¾è®¡ï¼‰
        cache_key = stock_code  # ç›´æ¥ä½¿ç”¨è‚¡ç¥¨ä»£ç ä½œä¸ºç¼“å­˜é”®
        cache_filename = f"kline_{cache_key}.json"

        # å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ï¼Œå…ˆå°è¯•ä»ç£ç›˜ç¼“å­˜åŠ è½½
        if not force_refresh:
            cached_data = self.load_data_from_file(cache_filename)
            if cached_data is not None:
                try:
                    # ä»ç¼“å­˜çš„æ•°æ®é‡å»ºDataFrame
                    records = cached_data['data']
                    df = pd.DataFrame(records)
                    if not df.empty:
                        # å°†dateåˆ—è®¾ç½®ä¸ºç´¢å¼•
                        if 'date' in df.columns:
                            df['date'] = pd.to_datetime(df['date'])
                            df = df.set_index('date')

                        # ä»ç¼“å­˜å…ƒæ•°æ®ä¸­è·å–è‚¡ç¥¨ä»£ç ï¼ˆç°åœ¨ç›´æ¥æ˜¯order_book_idï¼‰
                        cached_stock_code = cached_data.get('stock_code', stock_code)

                        # ç¼“å­˜åˆ°å†…å­˜ç”¨äºè®¡ç®—ï¼ˆæç®€è®¾è®¡ï¼‰
                        self.kline_cache[cached_stock_code] = df.copy()

                        # æ³¨æ„ï¼šç¨‹åºä¸€æ¬¡æ€§è¿è¡Œï¼Œæ— éœ€ç¼“å­˜å¤§å°é™åˆ¶

                        logger.debug(f"âœ… ä»ç£ç›˜ç¼“å­˜åŠ è½½Kçº¿æ•°æ®: {cached_stock_code}, {len(df)}æ¡è®°å½•")
                        return df
                except Exception as e:
                    logger.warning(f"âš ï¸ ç£ç›˜ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯: {cache_filename}, {e}")

        # æ£€æŸ¥å†…å­˜ç¼“å­˜ï¼ˆä»…å½“ä¸æ˜¯å¼ºåˆ¶åˆ·æ–°æ—¶ï¼‰
        if not force_refresh:
            # é¦–å…ˆå°è¯•ä½¿ç”¨è¾“å…¥çš„stock_codeæŸ¥æ‰¾
            if stock_code in self.kline_cache:
                cached_df = self.kline_cache[stock_code]
                # éªŒè¯ç¼“å­˜æ•°æ®çš„è‚¡ç¥¨ä»£ç æ˜¯å¦åŒ¹é… - ç°åœ¨æ£€æŸ¥order_book_idåˆ—
                if 'order_book_id' in cached_df.columns and cached_df['order_book_id'].iloc[0] == stock_code:
                    logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­: {stock_code}")
                    return cached_df.copy()
                else:
                    # ç¼“å­˜æ•°æ®ä¸åŒ¹é…ï¼Œæ¸…é™¤æ— æ•ˆç¼“å­˜
                    del self.kline_cache[stock_code]
                    logger.debug(f"ğŸ§¹ æ¸…é™¤æ— æ•ˆå†…å­˜ç¼“å­˜: {stock_code}")
            
            # å¦‚æœè¾“å…¥çš„stock_codeæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„order_book_id
            for cache_key, cached_df in self.kline_cache.items():
                if 'order_book_id' in cached_df.columns and cached_df['order_book_id'].iloc[0] == stock_code:
                    logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­ (é€šè¿‡order_book_id): {stock_code}")
                    return cached_df.copy()

        # ä»rqdatacè·å–æ•°æ®
        if not RQDATAC_AVAILABLE or rqdatac is None:
            logger.warning(f"âš ï¸ rqdatacä¸å¯ç”¨ï¼Œæ— æ³•è·å–{stock_code}çš„æ•°æ®")
            return None

        # æ·»åŠ é‡è¯•æœºåˆ¶å’Œè¶…æ—¶æ§åˆ¶
        max_retries = 3
        timeout_seconds = 30  # 30ç§’è¶…æ—¶

        for attempt in range(max_retries):
            try:
                logger.debug(f"ğŸ“¡ è·å– {stock_code} æ•°æ® (å°è¯• {attempt + 1}/{max_retries})...")

                # ç›´æ¥åœ¨ä¸»çº¿ç¨‹è°ƒç”¨rqdatac.get_price()
                df = rqdatac.get_price(
                    stock_code,
                    start_date=start_date,
                    end_date=end_date,
                    frequency='1d',
                    fields=['open', 'close', 'high', 'low', 'limit_up', 'limit_down', 'total_turnover', 'volume', 'num_trades']
                )

                if df is not None and not df.empty:
                    # ä»MultiIndexä¸­æå–è‚¡ç¥¨ä»£ç 
                    if isinstance(df.index, pd.MultiIndex) and 'order_book_id' in df.index.names:
                        # é‡ç½®ç´¢å¼•ï¼Œå°†order_book_idå’Œdateä½œä¸ºåˆ—
                        df_reset = df.reset_index()
                        # æå–è‚¡ç¥¨ä»£ç  - ç›´æ¥ä½¿ç”¨order_book_id
                        extracted_stock_code = df_reset['order_book_id'].iloc[0]
                        # é‡æ–°è®¾ç½®dateä¸ºç´¢å¼•
                        df_reset = df_reset.set_index('date')
                        df = df_reset

                        logger.debug(f"ğŸ“Š ä»DataFrameæå–è‚¡ç¥¨ä»£ç : {extracted_stock_code}")
                    else:
                        # å¦‚æœæ²¡æœ‰MultiIndexï¼Œä½¿ç”¨ä¼ å…¥çš„stock_code
                        extracted_stock_code = stock_code
                        logger.debug(f"ğŸ“Š ä½¿ç”¨ä¼ å…¥çš„è‚¡ç¥¨ä»£ç : {stock_code}")

                    # ç›´æ¥ä½¿ç”¨order_book_idä½œä¸ºç¼“å­˜é”®
                    cache_key = extracted_stock_code
                    cache_filename = f"kline_{cache_key}.json"

                    # ç«‹å³ä¿å­˜åˆ°ç£ç›˜ç¼“å­˜
                    cache_data = {
                        'stock_code': extracted_stock_code,
                        'start_date': start_date,
                        'end_date': end_date,
                        'data': df.reset_index().to_dict('records'),  # é‡ç½®ç´¢å¼•åè½¬æ¢ä¸ºè®°å½•åˆ—è¡¨ï¼Œé¿å…datetimeåºåˆ—åŒ–é—®é¢˜
                        'cached_time': datetime.now().isoformat(),
                        'data_points': len(df)
                    }

                    if self.save_data_to_file(cache_data, cache_filename):
                        logger.debug(f"ğŸ’¾ Kçº¿æ•°æ®å·²ä¿å­˜åˆ°ç£ç›˜ç¼“å­˜: {cache_filename}")
                    else:
                        logger.warning(f"âš ï¸ Kçº¿æ•°æ®ä¿å­˜åˆ°ç£ç›˜ç¼“å­˜å¤±è´¥: {cache_filename}")

                    # ç¼“å­˜åˆ°å†…å­˜ç”¨äºè®¡ç®—ï¼ˆæç®€è®¾è®¡ï¼‰
                    self.kline_cache[extracted_stock_code] = df.copy()

                    logger.debug(f"âœ… è·å–Kçº¿æ•°æ®: {extracted_stock_code}, {len(df)}æ¡è®°å½•")

                    return df
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°{stock_code}çš„Kçº¿æ•°æ®")
                    return None

            except TimeoutError as e:
                logger.warning(f"âš ï¸ {stock_code} è¯·æ±‚è¶…æ—¶: {e}")
                if attempt < max_retries - 1:
                    logger.debug(f"é‡è¯•ä¸­ ({attempt + 1}/{max_retries})...")
                    continue
                else:
                    logger.error(f"âŒ {stock_code} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
                    return None

            except Exception as e:
                logger.warning(f"âš ï¸ è·å– {stock_code} æ•°æ®å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    logger.debug(f"é‡è¯•ä¸­ ({attempt + 1}/{max_retries})...")
                    continue
                else:
                    logger.error(f"âŒ {stock_code} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥: {e}")
                    return None

        return None

    def _fetch_instruments_info(self, stock_code: str) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

        rqdatac.instruments() è¿”å›çš„å­—æ®µåŒ…æ‹¬:
        - order_book_id: è®¢å•ç°¿ID (å¦‚: '000001.XSHE')
        - symbol: è‚¡ç¥¨åç§° (å¦‚: 'å¹³å®‰é“¶è¡Œ')
        - abbrev_symbol: ç¼©å†™ç¬¦å· (å¦‚: 'PAYH')
        - trading_code: äº¤æ˜“ä»£ç  (å¦‚: '000001')
        - exchange: äº¤æ˜“æ‰€ (å¦‚: 'XSHE')
        - type: è¯åˆ¸ç±»å‹ (å¦‚: 'CS' - è‚¡ç¥¨)
        - status: çŠ¶æ€ (å¦‚: 'Active')
        - listed_date: ä¸Šå¸‚æ—¥æœŸ (å¦‚: '1991-04-03')
        - de_listed_date: é€€å¸‚æ—¥æœŸ (å¦‚: '0000-00-00')
        - board_type: æ¿å—ç±»å‹ (å¦‚: 'MainBoard')
        - industry_code: è¡Œä¸šä»£ç  (å¦‚: 'J66')
        - industry_name: è¡Œä¸šåç§° (å¦‚: 'è´§å¸é‡‘èæœåŠ¡')
        - sector_code: æ¿å—ä»£ç  (å¦‚: 'Financials')
        - sector_code_name: æ¿å—åç§° (å¦‚: 'é‡‘è')
        - citics_industry_code: ä¸­ä¿¡è¡Œä¸šä»£ç  (å¦‚: '40')
        - citics_industry_name: ä¸­ä¿¡è¡Œä¸šåç§° (å¦‚: 'é“¶è¡Œ')
        - province: çœä»½ (å¦‚: 'å¹¿ä¸œçœ')
        - office_address: åŠå…¬åœ°å€
        - issue_price: å‘è¡Œä»· (å¦‚: 40.0)
        - round_lot: æ¯æ‰‹è‚¡æ•° (å¦‚: 100)
        - market_tplus: T+1å¸‚åœº (å¦‚: 1)
        - trading_hours: äº¤æ˜“æ—¶é—´ (å¦‚: '09:31-11:30,13:01-15:00')
        - special_type: ç‰¹æ®Šç±»å‹ (å¦‚: 'Normal')
        - concept_names: æ¦‚å¿µåç§°åˆ—è¡¨

        Returns:
            DataFrame: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µä½œä¸ºåˆ—
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ rqdatacä¸å¯ç”¨")
                return None

            # è·å–åŸºæœ¬ä¿¡æ¯ - ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°å
            basic_info = rqdatac.instruments(stock_code)
            if basic_info is not None:
                # å°† Instrument å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                info_dict = {}
                for attr in dir(basic_info):
                    if not attr.startswith('_') and not callable(getattr(basic_info, attr)):
                        try:
                            value = getattr(basic_info, attr)
                            info_dict[attr] = value
                        except Exception as e:
                            logger.debug(f"âš ï¸ è·å–å±æ€§ {attr} å¤±è´¥: {e}")

                # è½¬æ¢ä¸ºDataFrameæ ¼å¼ï¼Œè‚¡ç¥¨ä»£ç ä½œä¸ºç´¢å¼•
                if info_dict:
                    df = pd.DataFrame([info_dict])
                    df.index = [stock_code]
                    logger.debug(f"ğŸ“Š è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯: {stock_code}, {len(info_dict)} ä¸ªå­—æ®µ")
                    return df
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä¿¡æ¯å­—æ®µ: {stock_code}")
                    return None
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨ä¿¡æ¯: {stock_code}")
                return None

        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥ {stock_code}: {e}")
            return None

    def _fetch_valuation_series(self, stock_code: str, date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨ä¼°å€¼ä¿¡æ¯
        æ”¯æŒæ‰¹é‡è·å–ä¼˜åŒ–

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æŸ¥è¯¢æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©

        Returns:
            DataFrame: ä¼°å€¼æ•°æ®ï¼ŒåŒ…å«å¤šåˆ—ä¼°å€¼æŒ‡æ ‡
        """
        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ rqdatacä¸å¯ç”¨")
                return None

            if date is None:
                date = datetime.now().strftime('%Y-%m-%d')

            # è·å–ä¼°å€¼å› å­æ•°æ®
            valuation_factors = ['pe_ratio', 'pb_ratio', 'ps_ratio', 'pcf_ratio', 'market_cap', 'turnover_ratio']

            # å°è¯•ä»æ‰¹é‡ç¼“å­˜ä¸­è·å–
            cached_result = self._get_cached_batch_valuation(stock_code, date, valuation_factors)
            if cached_result is not None:
                return cached_result

            # å¦‚æœæ²¡æœ‰æ‰¹é‡ç¼“å­˜ï¼Œåˆ™é€ä¸ªè·å–
            valuation_data = rqdatac.get_factor(stock_code, valuation_factors, start_date=date, end_date=date)

            if valuation_data is not None and not valuation_data.empty:
                logger.debug(f"ğŸ’° è·å–ä¼°å€¼ä¿¡æ¯: {stock_code} ({date}), {len(valuation_data)} æ¡è®°å½•")

                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ¸å¿ƒå­—æ®µéƒ½ä¸ºNaNï¼ˆæ•°æ®è´¨é‡é—®é¢˜ï¼‰
                core_fields = ['pe_ratio', 'pb_ratio', 'market_cap']
                all_core_nan = True
                for field in core_fields:
                    if field in valuation_data.columns and not valuation_data[field].isna().all():
                        all_core_nan = False
                        break

                if all_core_nan:
                    logger.warning(f"âš ï¸ {stock_code} ({date}) æ ¸å¿ƒä¼°å€¼æ•°æ®å…¨éƒ¨ä¸ºNaNï¼Œå¯èƒ½æ˜¯æ•°æ®æ›´æ–°å»¶è¿Ÿ")
                    return None

                return valuation_data
            else:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°ä¼°å€¼æ•°æ®: {stock_code} ({date})")
                return None

        except Exception as e:
            logger.error(f"âŒ è·å–ä¼°å€¼ä¿¡æ¯å¤±è´¥ {stock_code}: {e}")
            return None

    def _get_latest_trading_date(self, stock_code: str, max_days_back: int = 30) -> Optional[str]:
        """
        è·å–æœ€æ–°çš„æœ‰æ•ˆäº¤æ˜“æ—¥
        ä½¿ç”¨äº¤æ˜“æ—¥å†APIç›´æ¥è·å–æœ‰æ•ˆäº¤æ˜“æ—¥ï¼Œé¿å…é€šè¿‡æ•°æ®éªŒè¯çš„ä½æ•ˆæ–¹å¼

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_days_back: æœ€å¤§å›æº¯å¤©æ•°

        Returns:
            æœ‰æ•ˆçš„äº¤æ˜“æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD) æˆ– None
        """
        try:
            # è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥å†
            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=max_days_back)

            # ä½¿ç”¨rqdatacè·å–äº¤æ˜“æ—¥å†
            trading_dates = rqdatac.get_trading_dates(start_date, end_date)  # type: ignore

            if trading_dates is None or len(trading_dates) == 0:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°äº¤æ˜“æ—¥å†æ•°æ®")
                return None

            # è½¬æ¢ä¸ºæ—¥æœŸåˆ—è¡¨å¹¶æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            trading_dates = sorted([pd.to_datetime(d).date() for d in trading_dates], reverse=True)

            # è¿”å›æœ€æ–°çš„äº¤æ˜“æ—¥
            if len(trading_dates) > 0:
                latest_date = trading_dates[0]
                date_str = latest_date.strftime('%Y-%m-%d')
                logger.debug(f"âœ… æ‰¾åˆ°æœ€æ–°äº¤æ˜“æ—¥: {stock_code} @ {date_str}")
                return date_str

            logger.warning(f"âš ï¸ åœ¨{max_days_back}å¤©å†…æœªæ‰¾åˆ°æœ‰æ•ˆçš„äº¤æ˜“æ—¥")
            return None

        except Exception as e:
            logger.error(f"âŒ è·å–æœ€æ–°äº¤æ˜“æ—¥å¤±è´¥ {stock_code}: {e}")
            return None

    def _determine_target_trading_date(self) -> None:
        """
        é¢„å…ˆç¡®å®šç›®æ ‡äº¤æ˜“æ—¥ï¼Œé¿å…å¯¹æ¯åªè‚¡ç¥¨é‡å¤æ£€æŸ¥
        ä½¿ç”¨ get_trading_hours è·å–ç²¾ç¡®çš„äº¤æ˜“æ—¶æ®µè¿›è¡Œåˆ¤æ–­

        æ ¸å¿ƒé€»è¾‘:
        1. è·å–æœ€æ–°äº¤æ˜“æ—¥å’Œäº¤æ˜“æ—¶æ®µ
        2. æ ¹æ®å½“å‰æ—¶é—´å’Œäº¤æ˜“æ—¶æ®µæ™ºèƒ½åˆ¤æ–­åº”è¯¥ä½¿ç”¨å“ªä¸ªäº¤æ˜“æ—¥
        3. äº¤æ˜“å‰/äº¤æ˜“ä¸­/æ”¶ç›˜å3å°æ—¶å†…ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
        4. æ”¶ç›˜å3å°æ—¶åï¼šä½¿ç”¨å½“å¤©æ•°æ®
        5. åªåšä¸€æ¬¡å…¨å±€åˆ¤æ–­ï¼Œåç»­æ‰€æœ‰è‚¡ç¥¨éƒ½ä½¿ç”¨ç›¸åŒçš„ç»“æœ

        æ€§èƒ½ä¼˜åŒ–:
        - å…¨å±€ä¸€æ¬¡åˆ¤æ–­ vs æ¯åªè‚¡ç¥¨å•ç‹¬åˆ¤æ–­
        - åŸºäºäº¤æ˜“æ—¶æ®µçš„ç²¾ç¡®åˆ¤æ–­ vs ç®€å•æ—¶é—´åˆ¤æ–­
        - æ™ºèƒ½é™çº§ç­–ç•¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§

        Raises:
            æ‰€æœ‰å¼‚å¸¸éƒ½ä¼šè¢«æ•è·å¹¶é™çº§å¤„ç†ï¼Œä¸ä¼šå½±å“ç³»ç»Ÿè¿è¡Œ
        """
        logger.debug("ğŸ” å¼€å§‹ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥")

        try:
            if not RQDATAC_AVAILABLE or rqdatac is None:
                logger.warning("âš ï¸ RQDatacä¸å¯ç”¨ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºåå¤‡")
                self.target_trading_date = datetime.now().strftime('%Y-%m-%d')
                self.target_trading_date_determined = True
                logger.debug(f"ğŸ“… ä½¿ç”¨åå¤‡æ—¥æœŸ: {self.target_trading_date}")
                return

            # è·å–æœ€æ–°äº¤æ˜“æ—¥
            latest_date = rqdatac.get_latest_trading_date()  # type: ignore
            if not latest_date:
                logger.warning("âš ï¸ æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
                self.target_trading_date = datetime.now().strftime('%Y-%m-%d')
                self.target_trading_date_determined = True
                return

            logger.info(f"ğŸ“… æœ€æ–°äº¤æ˜“æ—¥: {latest_date}")

            # è·å–å½“å‰æ—¶é—´
            now = datetime.now()
            latest_date_dt = pd.to_datetime(latest_date).to_pydatetime()
            logger.debug(f"ğŸ• å½“å‰æ—¶é—´: {now}, æœ€æ–°äº¤æ˜“æ—¥: {latest_date_dt.date()}")

            # åˆ¤æ–­æ˜¯å¦åœ¨äº¤æ˜“æ—¥å½“å¤©
            if now.date() == latest_date_dt.date():
                logger.debug("ğŸ“… å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ï¼Œéœ€è¦åˆ¤æ–­æ•°æ®æœ‰æ•ˆæ€§")

                # è·å–å®é™…äº¤æ˜“æ—¶æ®µè¿›è¡Œç²¾ç¡®åˆ¤æ–­
                try:
                    # ä½¿ç”¨å¹³å®‰é“¶è¡Œä½œä¸ºä»£è¡¨è‚¡ç¥¨è·å–äº¤æ˜“æ—¶æ®µ
                    trading_hours_str = rqdatac.get_trading_hours('000001.XSHE', latest_date.strftime('%Y-%m-%d'))  # type: ignore
                    logger.info(f"â° äº¤æ˜“æ—¶æ®µ: {trading_hours_str}")

                    # éªŒè¯äº¤æ˜“æ—¶æ®µæ•°æ®æ ¼å¼
                    if not isinstance(trading_hours_str, str):
                        logger.error(f"âŒ äº¤æ˜“æ—¶æ®µæ•°æ®æ ¼å¼å¼‚å¸¸: {type(trading_hours_str)}")
                        raise ValueError(f"äº¤æ˜“æ—¶æ®µæ ¼å¼å¼‚å¸¸: {trading_hours_str}")

                    # è§£æäº¤æ˜“æ—¶æ®µå¹¶åˆ¤æ–­æ•°æ®æœ‰æ•ˆæ€§
                    should_use_previous = self._should_use_previous_trading_date(trading_hours_str, now)
                    logger.debug(f"ğŸ¯ æ˜¯å¦ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥: {should_use_previous}")

                    if should_use_previous:
                        # ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
                        previous_date = rqdatac.get_previous_trading_date(latest_date)  # type: ignore
                        if previous_date:
                            self.target_trading_date = previous_date.strftime('%Y-%m-%d')
                            logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (å‰ä¸€ä¸ªäº¤æ˜“æ—¥)")
                        else:
                            # å¦‚æœæ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
                            self.target_trading_date = latest_date.strftime('%Y-%m-%d')
                            logger.warning(f"âš ï¸ æ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥: {self.target_trading_date}")
                    else:
                        # ä½¿ç”¨å½“å¤©æ•°æ®
                        self.target_trading_date = latest_date.strftime('%Y-%m-%d')
                        logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (æœ€æ–°äº¤æ˜“æ—¥)")

                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–äº¤æ˜“æ—¶æ®µå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ—¶é—´åˆ¤æ–­: {e}")
                    logger.debug("ğŸ”„ æ‰§è¡Œé™çº§æ—¶é—´æ£€æŸ¥é€»è¾‘", exc_info=True)
                    # é™çº§åˆ°ç®€å•çš„æ—¶é—´åˆ¤æ–­é€»è¾‘
                    self._fallback_time_check(latest_date_dt, now)
            else:
                # å½“å‰ä¸åœ¨äº¤æ˜“æ—¥å½“å¤©ï¼Œæœ€æ–°äº¤æ˜“æ—¥æ•°æ®åº”è¯¥æœ‰æ•ˆ
                logger.info("ğŸ“… å½“å‰ä¸åœ¨äº¤æ˜“æ—¥å½“å¤©ï¼Œæœ€æ–°äº¤æ˜“æ—¥æ•°æ®åº”è¯¥æœ‰æ•ˆ")
                self.target_trading_date = latest_date.strftime('%Y-%m-%d')
                logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (æœ€æ–°äº¤æ˜“æ—¥)")

            self.target_trading_date_determined = True
            logger.debug("âœ… ç›®æ ‡äº¤æ˜“æ—¥ç¡®å®šå®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥å¤±è´¥: {e}")
            logger.debug("ğŸ”„ ä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºæœ€ç»ˆåå¤‡", exc_info=True)
            # é™çº§ä½¿ç”¨å½“å‰æ—¥æœŸ
            self.target_trading_date = datetime.now().strftime('%Y-%m-%d')
            self.target_trading_date_determined = True

    def get_target_trading_date(self) -> Optional[str]:
        """
        è·å–é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥

        Returns:
            ç›®æ ‡äº¤æ˜“æ—¥å­—ç¬¦ä¸² (YYYY-MM-DD) æˆ– None
        """
        if not self.target_trading_date_determined:
            self._determine_target_trading_date()
        return self.target_trading_date

    def _get_cached_batch_valuation(self, stock_code: str, date: str, factors: List[str]) -> Optional[pd.DataFrame]:
        """
        ä»æ‰¹é‡ç¼“å­˜ä¸­è·å–ä¼°å€¼æ•°æ®
        å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåˆ™è§¦å‘æ‰¹é‡è·å–

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸ
            factors: å› å­åˆ—è¡¨

        Returns:
            DataFrame: ä¼°å€¼æ•°æ®æˆ–None
        """
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ‰¹é‡æ•°æ®ç¼“å­˜
        cache_key = f"batch_valuation_{date}"
        if cache_key not in self.batch_cache:
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„æ‰¹é‡è¯·æ±‚
            if not hasattr(self, '_pending_batch_stocks'):
                self._pending_batch_stocks = set()

            self._pending_batch_stocks.add(stock_code)

            # å¦‚æœç§¯ç´¯äº†è¶³å¤Ÿå¤šçš„è‚¡ç¥¨ï¼ˆæ¯”å¦‚5åªï¼‰ï¼Œå°±æ‰§è¡Œæ‰¹é‡è·å–
            if len(self._pending_batch_stocks) >= 5:
                self._execute_batch_valuation_fetch(date, factors)
                self._pending_batch_stocks.clear()

                # é‡æ–°æ£€æŸ¥ç¼“å­˜
                if cache_key in self.batch_cache:
                    batch_data = self.batch_cache[cache_key]
                    return self._extract_single_stock_from_batch(batch_data, stock_code, date)

        # ä»ç¼“å­˜ä¸­æå–å•ä¸ªè‚¡ç¥¨çš„æ•°æ®
        if cache_key in self.batch_cache:
            batch_data = self.batch_cache[cache_key]
            return self._extract_single_stock_from_batch(batch_data, stock_code, date)

        return None

    def _execute_batch_valuation_fetch(self, date: str, factors: List[str]) -> None:
        """
        æ‰§è¡Œæ‰¹é‡ä¼°å€¼æ•°æ®è·å–

        æ‰¹é‡è·å–ç­–ç•¥:
        - ç´¯ç§¯å¤šä¸ªè‚¡ç¥¨è¯·æ±‚åç»Ÿä¸€è·å–
        - å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼Œæé«˜æ•ˆç‡
        - æ™ºèƒ½ç¼“å­˜æ‰¹é‡ç»“æœ
        - å¤±è´¥æ—¶ä¸å½±å“å•ä¸ªè‚¡ç¥¨è·å–

        Args:
            date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
            factors: è¦è·å–çš„ä¼°å€¼å› å­åˆ—è¡¨

        Raises:
            è¯¥æ–¹æ³•å†…éƒ¨å¤„ç†æ‰€æœ‰å¼‚å¸¸ï¼Œä¸ä¼šå‘ä¸Šä¼ æ’­
        """
        logger.debug(f"ğŸ”„ å¼€å§‹æ‰¹é‡ä¼°å€¼è·å–: æ—¥æœŸ={date}, å› å­={factors}")

        try:
            if not hasattr(self, '_pending_batch_stocks') or not self._pending_batch_stocks:
                logger.debug("ğŸ“­ æ— å¾…å¤„ç†çš„æ‰¹é‡è¯·æ±‚ï¼Œè·³è¿‡")
                return

            stock_list = list(self._pending_batch_stocks)
            logger.info(f"ğŸš€ æ‰§è¡Œæ‰¹é‡ä¼°å€¼è·å–: {len(stock_list)} åªè‚¡ç¥¨ @ {date}")
            logger.debug(f"ğŸ“‹ è‚¡ç¥¨åˆ—è¡¨: {stock_list[:5]}{'...' if len(stock_list) > 5 else ''}")

            # æ‰§è¡Œæ‰¹é‡è·å–
            batch_result = rqdatac.get_factor(stock_list, factors, start_date=date, end_date=date)

            if batch_result is not None and not batch_result.empty:
                # ç¼“å­˜æ‰¹é‡ç»“æœ
                cache_key = f"batch_valuation_{date}"
                self.batch_cache[cache_key] = batch_result

                logger.info(f"âœ… æ‰¹é‡ä¼°å€¼è·å–æˆåŠŸ: {batch_result.shape[0]} æ¡è®°å½•")
                logger.debug(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {batch_result.shape}, åˆ—: {list(batch_result.columns)}")
            else:
                logger.warning(f"âš ï¸ æ‰¹é‡ä¼°å€¼è·å–å¤±è´¥: è¿”å›ç©ºç»“æœ")
                logger.debug("ğŸ” æ£€æŸ¥: RQDatacè¿æ¥çŠ¶æ€å’Œå‚æ•°æœ‰æ•ˆæ€§")

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¼°å€¼è·å–å¼‚å¸¸: {e}")
            logger.debug("ğŸ” å¼‚å¸¸è¯¦æƒ…", exc_info=True)
            # ä¸å‘ä¸Šä¼ æ’­å¼‚å¸¸ï¼Œä¿æŒç³»ç»Ÿç¨³å®šæ€§

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¼°å€¼è·å–å¤±è´¥: {e}")

    def _extract_single_stock_from_batch(self, batch_data: pd.DataFrame, stock_code: str, date: str) -> Optional[pd.DataFrame]:
        """
        ä»æ‰¹é‡æ•°æ®ä¸­æå–å•ä¸ªè‚¡ç¥¨çš„æ•°æ®

        Args:
            batch_data: æ‰¹é‡æ•°æ®
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸ

        Returns:
            DataFrame: å•ä¸ªè‚¡ç¥¨çš„æ•°æ®
        """
        try:
            # æ‰¹é‡æ•°æ®çš„ç´¢å¼•æ˜¯ (è‚¡ç¥¨ä»£ç , æ—¥æœŸ) çš„å¤šé‡ç´¢å¼•
            if isinstance(batch_data.index, pd.MultiIndex):
                # æŸ¥æ‰¾å¯¹åº”çš„è‚¡ç¥¨å’Œæ—¥æœŸ
                mask = (batch_data.index.get_level_values(0) == stock_code)
                if date:
                    date_obj = pd.to_datetime(date)
                    mask = mask & (batch_data.index.get_level_values(1) == date_obj)

                if mask.any():
                    single_stock_data = batch_data[mask].copy()
                    # é‡ç½®ç´¢å¼•ï¼Œç§»é™¤å¤šé‡ç´¢å¼•
                    single_stock_data = single_stock_data.reset_index()
                    # åªä¿ç•™å› å­åˆ—
                    factor_columns = [col for col in single_stock_data.columns if col not in ['level_0', 'level_1', stock_code, date]]
                    if factor_columns:
                        single_stock_data = single_stock_data[factor_columns]
                    return single_stock_data

            return None

        except Exception as e:
            logger.error(f"âŒ ä»æ‰¹é‡æ•°æ®ä¸­æå–å•ä¸ªè‚¡ç¥¨å¤±è´¥ {stock_code}: {e}")
            return None

    def _should_use_previous_trading_date(self, trading_hours_str: str, current_time: datetime) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®

        é€»è¾‘ï¼š
        1. äº¤æ˜“å‰ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
        2. äº¤æ˜“ä¸­ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆå®æ—¶æ•°æ®ä¸ç¨³å®šï¼‰
        3. äº¤æ˜“å3å°æ—¶å†…ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆæ•°æ®å¯èƒ½è¿˜åœ¨æ›´æ–°ï¼‰
        4. äº¤æ˜“å3å°æ—¶åï¼šä½¿ç”¨å½“å¤©æ•°æ®

        Args:
            trading_hours_str: äº¤æ˜“æ—¶æ®µå­—ç¬¦ä¸²ï¼Œå¦‚ "09:31-11:30,13:01-15:00"
            current_time: å½“å‰æ—¶é—´

        Returns:
            æ˜¯å¦åº”è¯¥ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
        """
        try:
            if not trading_hours_str:
                return True

            current_time_only = current_time.time()

            # è§£æäº¤æ˜“æ—¶æ®µ
            periods = trading_hours_str.split(',')
            market_open_time = None
            market_close_time = None

            for period in periods:
                if '-' not in period:
                    continue

                start_str, end_str = period.split('-')
                try:
                    start_time = datetime.strptime(start_str.strip(), '%H:%M').time()
                    end_time = datetime.strptime(end_str.strip(), '%H:%M').time()

                    if market_open_time is None or start_time < market_open_time:
                        market_open_time = start_time
                    if market_close_time is None or end_time > market_close_time:
                        market_close_time = end_time
                except ValueError:
                    continue

            if market_open_time is None or market_close_time is None:
                logger.warning(f"âš ï¸ æ— æ³•è§£æäº¤æ˜“æ—¶æ®µ: {trading_hours_str}")
                return True

            # è®¡ç®—æ”¶ç›˜å3å°æ—¶çš„æ—¶é—´ç‚¹
            close_datetime = datetime.combine(current_time.date(), market_close_time)
            three_hours_after_close = close_datetime + timedelta(hours=3)
            three_hours_after_close_time = three_hours_after_close.time()

            logger.info(f"â° å¸‚åœºå¼€ç›˜: {market_open_time}, æ”¶ç›˜: {market_close_time}")
            logger.info(f"â° å½“å‰æ—¶é—´: {current_time_only}, æ”¶ç›˜å3å°æ—¶: {three_hours_after_close_time}")

            if current_time_only < market_open_time:
                # äº¤æ˜“å‰ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
                logger.info("â° å½“å‰åœ¨äº¤æ˜“å‰ï¼Œä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥")
                return True
            elif market_open_time <= current_time_only <= market_close_time:
                # äº¤æ˜“ä¸­ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆå®æ—¶æ•°æ®ä¸ç¨³å®šï¼‰
                logger.info("â° å½“å‰åœ¨äº¤æ˜“ä¸­ï¼Œä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆå®æ—¶æ•°æ®ä¸ç¨³å®šï¼‰")
                return True
            elif market_close_time < current_time_only <= three_hours_after_close_time:
                # æ”¶ç›˜å3å°æ—¶å†…ï¼šä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆæ•°æ®å¯èƒ½è¿˜åœ¨æ›´æ–°ï¼‰
                logger.info("â° å½“å‰åœ¨æ”¶ç›˜å3å°æ—¶å†…ï¼Œä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆæ•°æ®å¯èƒ½è¿˜åœ¨æ›´æ–°ï¼‰")
                return True
            else:
                # æ”¶ç›˜å3å°æ—¶åï¼šä½¿ç”¨å½“å¤©æ•°æ®
                logger.info("â° å½“å‰åœ¨æ”¶ç›˜å3å°æ—¶åï¼Œä½¿ç”¨å½“å¤©æ•°æ®")
                return False

        except Exception as e:
            logger.warning(f"âš ï¸ åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶ä¿å®ˆä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥

    def _fallback_time_check(self, latest_date: datetime, now: datetime) -> None:
        """
        é™çº§æ—¶é—´æ£€æŸ¥é€»è¾‘ï¼ˆå½“è·å–äº¤æ˜“æ—¶æ®µå¤±è´¥æ—¶ä½¿ç”¨ï¼‰

        Args:
            latest_date: æœ€æ–°äº¤æ˜“æ—¥
            now: å½“å‰æ—¶é—´
        """
        # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´å†…
        current_time = now.time()
        # å‡è®¾Aè‚¡æ”¶ç›˜æ—¶é—´æ˜¯15:00
        market_close_time = datetime.strptime('15:00', '%H:%M').time()

        if current_time < market_close_time:
            # å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ä¸”æœªæ”¶ç›˜ï¼Œæ•°æ®å¿…ç„¶æ— æ•ˆ
            logger.info("â° å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ä¸”æœªæ”¶ç›˜ï¼Œæœ€æ–°äº¤æ˜“æ—¥æ•°æ®å¿…ç„¶æ— æ•ˆ")

            # è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥
            previous_date = rqdatac.get_previous_trading_date(latest_date)  # type: ignore
            if previous_date:
                self.target_trading_date = previous_date.strftime('%Y-%m-%d')
                logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (å‰ä¸€ä¸ªäº¤æ˜“æ—¥)")
            else:
                # å¦‚æœæ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
                self.target_trading_date = latest_date.strftime('%Y-%m-%d')
                logger.warning(f"âš ï¸ æ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥: {self.target_trading_date}")
        else:
            # å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ä½†å·²æ”¶ç›˜ï¼Œæ•°æ®å¯èƒ½æœ‰æ•ˆ
            logger.info("â° å½“å‰åœ¨äº¤æ˜“æ—¥å½“å¤©ä¸”å·²æ”¶ç›˜ï¼Œæœ€æ–°äº¤æ˜“æ—¥æ•°æ®å¯èƒ½æœ‰æ•ˆ")
            self.target_trading_date = latest_date.strftime('%Y-%m-%d')
            logger.info(f"ğŸ¯ ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥: {self.target_trading_date} (æœ€æ–°äº¤æ˜“æ—¥)")

    def _get_previous_trading_date(self, stock_code: str, max_days_back: int = 30) -> Optional[str]:
        """
        ä½¿ç”¨RQDatac APIè·å–æœ€åä¸€ä¸ªå·²å®Œæˆäº¤æ˜“æ—¥çš„æ•°æ®
        æ™ºèƒ½åˆ¤æ–­ï¼šä¼˜å…ˆä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥ï¼Œå¦‚æœæ•°æ®æ— æ•ˆåˆ™ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_days_back: æœ€å¤§å›æº¯å¤©æ•°

        Returns:
            æœ€åä¸€ä¸ªå·²å®Œæˆäº¤æ˜“æ—¥çš„æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD) æˆ– None
        """
        try:
            # å®šä¹‰æ ¸å¿ƒå­—æ®µ
            core_fields = ['pe_ratio', 'pb_ratio', 'market_cap']

            # ä½¿ç”¨RQDatac APIè·å–æœ€æ–°äº¤æ˜“æ—¥
            latest_date = rqdatac.get_latest_trading_date()  # type: ignore
            if not latest_date:
                logger.warning("âš ï¸ æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥")
                return None

            logger.debug(f"ğŸ“… æœ€æ–°äº¤æ˜“æ—¥: {latest_date}")

            # æ£€æŸ¥æœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            valuation_df = self._fetch_data('valuation', stock_code, date=latest_date.strftime('%Y-%m-%d'))

            if valuation_df is not None and not valuation_df.empty:
                # æ£€æŸ¥æ ¸å¿ƒå­—æ®µæ˜¯å¦æœ‰æ•ˆï¼ˆéNaNï¼‰
                has_valid_data = False

                for field in core_fields:
                    if field in valuation_df.columns and not valuation_df[field].isna().all():
                        has_valid_data = True
                        break

                if has_valid_data:
                    logger.debug(f"âœ… æœ€æ–°äº¤æ˜“æ—¥ {latest_date} æ•°æ®æœ‰æ•ˆ")
                    return latest_date.strftime('%Y-%m-%d')

            # å¦‚æœæœ€æ–°äº¤æ˜“æ—¥æ•°æ®æ— æ•ˆï¼Œå°è¯•å‰ä¸€ä¸ªäº¤æ˜“æ—¥
            logger.debug(f"âš ï¸ æœ€æ–°äº¤æ˜“æ—¥ {latest_date} æ•°æ®æ— æ•ˆï¼Œå°è¯•å‰ä¸€ä¸ªäº¤æ˜“æ—¥")

            # ä½¿ç”¨RQDatac APIè·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥
            previous_date = rqdatac.get_previous_trading_date(latest_date)  # type: ignore
            if not previous_date:
                logger.warning("âš ï¸ æ— æ³•è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥")
                return None

            logger.debug(f"ğŸ“… å‰ä¸€ä¸ªäº¤æ˜“æ—¥: {previous_date}")

            # æ£€æŸ¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            prev_valuation_df = self._fetch_data('valuation', stock_code, date=previous_date.strftime('%Y-%m-%d'))

            if prev_valuation_df is not None and not prev_valuation_df.empty:
                # æ£€æŸ¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                has_prev_valid_data = False

                for field in core_fields:
                    if field in prev_valuation_df.columns and not prev_valuation_df[field].isna().all():
                        has_prev_valid_data = True
                        break

                if has_prev_valid_data:
                    logger.debug(f"âœ… å‰ä¸€ä¸ªäº¤æ˜“æ—¥ {previous_date} æ•°æ®æœ‰æ•ˆ")
                    return previous_date.strftime('%Y-%m-%d')

            logger.warning(f"âš ï¸ å‰ä¸€ä¸ªäº¤æ˜“æ—¥ {previous_date} æ•°æ®ä¹Ÿä¸å¯ç”¨")
            return None

        except Exception as e:
            logger.error(f"âŒ è·å–æœ€åä¸€ä¸ªå·²å®Œæˆäº¤æ˜“æ—¥å¤±è´¥ {stock_code}: {e}")
            return None

    def get_smart_data(self, stock_code: str, data_type: str, max_retries: int = 5) -> Optional[pd.DataFrame]:
        """
        æ™ºèƒ½æ•°æ®è·å–å‡½æ•°
        ä½¿ç”¨é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥ï¼Œé¿å…å¯¹æ¯åªè‚¡ç¥¨é‡å¤æ£€æŸ¥
        ä¼˜åŒ–ç‰ˆï¼šå…¨å±€åˆ¤æ–­ä¸€æ¬¡ï¼Œåç»­æ‰€æœ‰è‚¡ç¥¨éƒ½ä½¿ç”¨ç›¸åŒçš„ç»“æœ

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹ ('valuation', 'price')
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            DataFrame: åŒ…å«å®Œæ•´æ•°æ®çš„DataFrameæˆ–None
        """
        try:
            # ä½¿ç”¨é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥ï¼Œé¿å…é‡å¤æ£€æŸ¥
            target_date = self.get_target_trading_date()
            if not target_date:
                logger.warning(f"âš ï¸ æ— æ³•ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥")
                return None

            logger.debug(f"ğŸ¯ ä½¿ç”¨é¢„å…ˆç¡®å®šçš„ç›®æ ‡äº¤æ˜“æ—¥: {target_date}")

            # æ ¹æ®æ•°æ®ç±»å‹è·å–æ•°æ®
            if data_type == 'valuation':
                valuation_df = self._fetch_data('valuation', stock_code, date=target_date)
                if valuation_df is not None and not valuation_df.empty:
                    # éªŒè¯æ•°æ®å®Œæ•´æ€§
                    if self._validate_valuation_completeness(valuation_df):
                        logger.debug(f"âœ… è·å–ä¼°å€¼æ•°æ®æˆåŠŸ: {stock_code} @ {target_date}")
                        return valuation_df
                    else:
                        logger.warning(f"âš ï¸ {stock_code} @ {target_date} ä¼°å€¼æ•°æ®ä¸å®Œæ•´")
                        return None
                else:
                    logger.warning(f"âš ï¸ {stock_code} @ {target_date} æœªè·å–åˆ°ä¼°å€¼æ•°æ®")
                    return None

            elif data_type == 'price':
                # è·å–ä»·æ ¼æ•°æ®éœ€è¦æ›´å¤šå‚æ•° - å¢åŠ åˆ°90å¤©ä»¥ç¡®ä¿æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæ•´æ€§
                start_date = (datetime.strptime(target_date, '%Y-%m-%d') - timedelta(days=90)).strftime('%Y-%m-%d')
                price_df = self._fetch_data('price', stock_code, start_date=start_date, end_date=target_date)
                if price_df is not None and not price_df.empty:
                    # ç¡®ä¿æ•°æ®å®Œæ•´æ€§
                    if self._validate_dataframe_completeness(price_df, data_type):
                        logger.debug(f"âœ… è·å–ä»·æ ¼æ•°æ®æˆåŠŸ: {stock_code}, {len(price_df)}æ¡è®°å½•")
                        return price_df
                    else:
                        logger.warning(f"âš ï¸ {stock_code} ä»·æ ¼æ•°æ®ä¸å®Œæ•´")
                        return None
                else:
                    logger.warning(f"âš ï¸ {stock_code} æœªè·å–åˆ°ä»·æ ¼æ•°æ®")
                    return None

            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
                return None

        except Exception as e:
            logger.error(f"âŒ è·å–æ™ºèƒ½æ•°æ®å¤±è´¥ {stock_code} ({data_type}): {e}")
            return None

    def _fetch_data(self, data_type: str, stock_code: str, **kwargs) -> Optional[pd.DataFrame]:
        """
        ç»Ÿä¸€çš„ç§æœ‰æ•°æ®è·å–æ–¹æ³•

        Args:
            data_type: æ•°æ®ç±»å‹ ('price', 'valuation', 'instruments')
            stock_code: è‚¡ç¥¨ä»£ç 
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            DataFrame: è¯·æ±‚çš„æ•°æ®æˆ–None
        """
        if data_type == 'price':
            start_date = kwargs.get('start_date')
            end_date = kwargs.get('end_date')
            force_refresh = kwargs.get('force_refresh', False)
            if start_date and end_date:
                return self._fetch_price_series(stock_code, start_date, end_date, force_refresh)
            else:
                logger.error("âŒ è·å–ä»·æ ¼æ•°æ®éœ€è¦ start_date å’Œ end_date å‚æ•°")
                return None
        elif data_type == 'valuation':
            date = kwargs.get('date')
            return self._fetch_valuation_series(stock_code, date)
        elif data_type == 'instruments':
            return self._fetch_instruments_info(stock_code)
        else:
            logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
            return None

    def _validate_dataframe_completeness(self, df: pd.DataFrame, data_type: str) -> bool:
        """
        éªŒè¯DataFrameæ•°æ®å®Œæ•´æ€§

        Args:
            df: è¦éªŒè¯çš„DataFrame
            data_type: æ•°æ®ç±»å‹

        Returns:
            bool: æ•°æ®æ˜¯å¦å®Œæ•´
        """
        try:
            if df is None or df.empty:
                return False

            if data_type == 'valuation':
                required_fields = ['pe_ratio', 'pb_ratio', 'market_cap']
                # æ£€æŸ¥æœ€æ–°ä¸€è¡Œæ•°æ®æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µä¸”ä¸ä¸ºNaN
                latest_row = df.iloc[-1] if len(df) > 0 else None
                if latest_row is None:
                    return False

                return all(
                    field in latest_row.index and
                    latest_row[field] is not None and
                    not pd.isna(latest_row[field])
                    for field in required_fields
                )

            elif data_type == 'price':
                required_fields = ['open', 'close', 'high', 'low', 'volume']
                # æ£€æŸ¥æ‰€æœ‰è¡Œæ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µä¸”ä»·æ ¼å¤§äº0
                for _, row in df.iterrows():
                    if not all(
                        field in row.index and
                        row[field] is not None and
                        not pd.isna(row[field]) and
                        (field != 'volume' or row[field] >= 0) and  # volumeå¯ä»¥ä¸º0
                        (field == 'volume' or row[field] > 0)      # ä»·æ ¼å¿…é¡»å¤§äº0
                        for field in required_fields
                    ):
                        return False
                return True

            return False

        except Exception as e:
            logger.error(f"DataFrameæ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
            return False

    def _validate_valuation_completeness(self, df: pd.DataFrame) -> bool:
        """
        æ”¹è¿›çš„ä¼°å€¼æ•°æ®å®Œæ•´æ€§éªŒè¯
        å…è®¸éƒ¨åˆ†å­—æ®µç¼ºå¤±ï¼Œåªè¦æœ‰æ ¸å¿ƒå­—æ®µå³å¯

        Args:
            df: ä¼°å€¼æ•°æ®DataFrame

        Returns:
            bool: æ•°æ®æ˜¯å¦å®Œæ•´
        """
        if df is None or df.empty:
            return False

        # æ ¸å¿ƒå­—æ®µ - è‡³å°‘éœ€è¦è¿™äº›å­—æ®µä¸­çš„ä¸€ä¸ª
        core_fields = ['pe_ratio', 'pb_ratio', 'market_cap']
        available_core_fields = [field for field in core_fields if field in df.columns]

        if not available_core_fields:
            logger.warning(f"âš ï¸ ä¼°å€¼æ•°æ®ç¼ºå°‘æ‰€æœ‰æ ¸å¿ƒå­—æ®µ: {core_fields}")
            return False

        # æ£€æŸ¥æ ¸å¿ƒå­—æ®µæ˜¯å¦æœ‰æœ‰æ•ˆå€¼
        valid_values = 0
        for field in available_core_fields:
            if not df[field].isna().all():
                valid_values += 1

        # è‡³å°‘æœ‰ä¸€ä¸ªæ ¸å¿ƒå­—æ®µæœ‰æœ‰æ•ˆå€¼
        if valid_values == 0:
            logger.warning(f"âš ï¸ ä¼°å€¼æ•°æ®æ ¸å¿ƒå­—æ®µå…¨éƒ¨ä¸ºNaN: {available_core_fields}")
            return False

        logger.debug(f"âœ… ä¼°å€¼æ•°æ®éªŒè¯é€šè¿‡: æ ¸å¿ƒå­—æ®µ {available_core_fields}, æœ‰æ•ˆå€¼ {valid_values}")
        return True

    def _safe_to_numpy(self, series) -> np.ndarray:
        """
        å®‰å…¨åœ°å°†pandas Seriesæˆ–ArrayLikeè½¬æ¢ä¸ºnumpyæ•°ç»„

        Args:
            series: pandas Seriesæˆ–ArrayLikeå¯¹è±¡

        Returns:
            numpyæ•°ç»„
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„pandas Serieså’ŒArrayLike
            if hasattr(series, 'values'):
                return np.array(series.values, dtype=float)
            elif hasattr(series, 'to_numpy'):
                return series.to_numpy().astype(float)
            else:
                return np.array(series, dtype=float)
        except Exception as e:
            logger.warning(f"è½¬æ¢åˆ°numpyæ•°ç»„å¤±è´¥: {e}")
            return np.array([], dtype=float)

import talib

# ============================================================================
# STOCK POOL INDICATOR ENGINE - æŠ€æœ¯æŒ‡æ ‡å¼•æ“
# ============================================================================

class StockPoolIndicatorEngine:
    """
    StockPoolä¸“ç”¨æŠ€æœ¯æŒ‡æ ‡å¼•æ“
    - åŸºäºINDICATOR_CONFIGé…ç½®é©±åŠ¨
    - æä¾›ç»Ÿä¸€çš„æŒ‡æ ‡è®¡ç®—æ¥å£
    - æ”¯æŒæ‰€æœ‰é…ç½®çš„æŠ€æœ¯æŒ‡æ ‡
    """

    # æŒ‡æ ‡è®¡ç®—é…ç½®
    INDICATOR_CONFIG = {
        # ç§»åŠ¨å¹³å‡çº¿
        "SMA": {
            "periods": [5, 10, 20, 30, 60],
            "description": "ç®€å•ç§»åŠ¨å¹³å‡çº¿"
        },
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿
        "EMA": {
            "periods": [5, 10, 12, 20, 26, 30, 60],
            "description": "æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿"
        },
        # RSIæŒ‡æ ‡
        "RSI": {
            "periods": [6, 14, 21],
            "description": "ç›¸å¯¹å¼ºå¼±æŒ‡æ•°"
        },
        # MACDæŒ‡æ ‡
        "MACD": {
            "fast_period": 12,
            "slow_period": 26,
            "signal_period": 9,
            "description": "MACDæŒ‡æ ‡"
        },
        # å¸ƒæ—å¸¦
        "BB": {
            "period": 20,
            "std_dev": 2.0,
            "description": "å¸ƒæ—å¸¦"
        },
        # ATRæŒ‡æ ‡
        "ATR": {
            "periods": [7, 14, 21],
            "description": "å¹³å‡çœŸå®æ³¢å¹…"
        },
        # éšæœºæŒ‡æ ‡
        "STOCH": {
            "fastk_period": 14,
            "slowk_period": 3,
            "slowd_period": 3,
            "description": "éšæœºæŒ‡æ ‡"
        },
        # CCIæŒ‡æ ‡
        "CCI": {
            "periods": [14, 20],
            "description": "é¡ºåŠ¿æŒ‡æ ‡"
        },
        # ROCæŒ‡æ ‡
        "ROC": {
            "periods": [10, 12, 20],
            "description": "å˜åŠ¨ç‡æŒ‡æ ‡"
        },
        # TEMAæŒ‡æ ‡
        "TEMA": {
            "periods": [20, 30],
            "description": "ä¸‰é‡æŒ‡æ•°ç§»åŠ¨å¹³å‡"
        },
        # WMAæŒ‡æ ‡
        "WMA": {
            "periods": [10, 20, 30],
            "description": "åŠ æƒç§»åŠ¨å¹³å‡"
        },
        # DMIæŒ‡æ ‡
        "DMI": {
            "period": 14,
            "description": "åŠ¨å‘æŒ‡æ ‡"
        },
        # OBVæŒ‡æ ‡
        "OBV": {
            "description": "èƒ½é‡æ½®æŒ‡æ ‡"
        },
        # æˆäº¤é‡SMA
        "VOLUME_SMA": {
            "periods": [5, 10, 20],
            "description": "æˆäº¤é‡ç®€å•ç§»åŠ¨å¹³å‡"
        },
        # MFIæŒ‡æ ‡
        "MFI": {
            "period": 14,
            "description": "èµ„é‡‘æµé‡æŒ‡æ ‡"
        },
        # å¨å»‰æŒ‡æ ‡
        "WILLR": {
            "period": 14,
            "description": "å¨å»‰æŒ‡æ ‡"
        },
        # æ³¢åŠ¨ç‡
        "VOLATILITY": {
            "period": 20,
            "description": "ä»·æ ¼æ³¢åŠ¨ç‡"
        }
    }

    def _safe_to_numpy(self, series) -> np.ndarray:
        """
        å®‰å…¨åœ°å°†pandas Seriesæˆ–ArrayLikeè½¬æ¢ä¸ºnumpyæ•°ç»„

        Args:
            series: pandas Seriesæˆ–ArrayLikeå¯¹è±¡

        Returns:
            numpyæ•°ç»„
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„pandas Serieså’ŒArrayLike
            if hasattr(series, 'values'):
                return np.array(series.values, dtype=float)
            elif hasattr(series, 'to_numpy'):
                return series.to_numpy().astype(float)
            else:
                return np.array(series, dtype=float)
        except Exception as e:
            logger.warning(f"è½¬æ¢åˆ°numpyæ•°ç»„å¤±è´¥: {e}")
            return np.array([], dtype=float)

    def __init__(self, data_store: StockPoolDataStore):
        """åˆå§‹åŒ–æŒ‡æ ‡å¼•æ“"""
        self.data_store = data_store
        self.calculation_stats = {
            'total_calculations': 0,
            'successful_calculations': 0,
            'errors': 0
        }

        # æŒ‡æ ‡ç¼“å­˜ - æç®€è®¾è®¡ï¼Œä¸kline_cacheä¿æŒä¸€è‡´
        self.indicator_cache: Dict[str, pd.DataFrame] = {}  # {order_book_id: indicators_df}

        logger.info("ğŸ“ˆ StockPoolIndicatorEngineåˆå§‹åŒ–å®Œæˆ")

    def calculate_all_indicators(self, kline_data: pd.DataFrame, stock_code: Optional[str] = None,
                                force_refresh: bool = False) -> Dict:
        """
        è®¡ç®—è‚¡ç¥¨çš„æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡ï¼ˆåŸºäº INDICATOR_CONFIG é…ç½®ï¼‰

        ç¼“å­˜ç­–ç•¥:
        - å†…å­˜ç¼“å­˜ï¼šå­˜å‚¨å®Œæ•´çš„æŒ‡æ ‡DataFrameç”¨äºåç»­è®¡ç®—
        - ç¼“å­˜é”®ï¼šåŸºäºè‚¡ç¥¨ä»£ç å’Œæ•°æ®æ—¶é—´èŒƒå›´ç”Ÿæˆ
        - æ”¯æŒå¼ºåˆ¶åˆ·æ–°å‚æ•°
        - é¿å…é‡å¤è®¡ç®—ï¼Œæé«˜æ€§èƒ½

        Args:
            kline_data: Kçº¿æ•°æ® (åŒ…å«OHLCV)
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºæ—¥å¿—å’Œç¼“å­˜ï¼‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—æŒ‡æ ‡

        Returns:
            Dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        if kline_data is None or kline_data.empty:
            return {}

        try:
            # è®°å½•è®¡ç®—ç»Ÿè®¡
            self.calculation_stats['total_calculations'] += 1

            # ç”Ÿæˆç¼“å­˜é”®ï¼ˆä¼˜åŒ–è®¾è®¡ï¼šä½¿ç”¨order_book_idï¼‰
            cache_key = None
            if kline_data is not None and not kline_data.empty and 'order_book_id' in kline_data.columns:
                # ä»kçº¿æ•°æ®ä¸­æå–è§„èŒƒåŒ–çš„order_book_idä½œä¸ºç¼“å­˜é”®
                cache_key = kline_data['order_book_id'].iloc[0]
                logger.debug(f"ğŸ“Š æŒ‡æ ‡ç¼“å­˜é”®: {cache_key}")
            elif stock_code:
                # é™çº§ä½¿ç”¨ä¼ å…¥çš„stock_codeï¼ˆå‘åå…¼å®¹ï¼‰
                cache_key = stock_code
                logger.debug(f"ğŸ“Š ä½¿ç”¨ä¼ å…¥çš„è‚¡ç¥¨ä»£ç ä½œä¸ºç¼“å­˜é”®: {stock_code}")

            # æ£€æŸ¥ç¼“å­˜ï¼ˆä»…å½“ä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ä¸”æœ‰æœ‰æ•ˆçš„ç¼“å­˜é”®æ—¶ï¼‰
            if not force_refresh and cache_key:
                # é¦–å…ˆå°è¯•ä½¿ç”¨è§„èŒƒåŒ–çš„ç¼“å­˜é”®æŸ¥æ‰¾
                if cache_key in self.indicator_cache:
                    cached_df = self.indicator_cache[cache_key]

                    # éªŒè¯ç¼“å­˜æ•°æ®çš„æ­£ç¡®æ€§
                    if 'order_book_id' in cached_df.columns and cached_df['order_book_id'].iloc[0] == cache_key:
                        logger.debug(f"âœ… æŒ‡æ ‡ç¼“å­˜å‘½ä¸­: {cache_key}")
                        # ä»ç¼“å­˜çš„DataFrameé‡å»ºç»“æœ
                        latest_values = {}
                        for col in cached_df.columns:
                            if col != 'order_book_id':  # æ’é™¤order_book_idåˆ—
                                latest_val = cached_df[col].iloc[-1] if not cached_df[col].isna().all() else None
                                if latest_val is not None and not pd.isna(latest_val):
                                    latest_values[col] = float(latest_val)

                        result = {
                            'indicators_df': cached_df,
                            'latest_values': latest_values,
                            'calculation_stats': self.calculation_stats.copy(),
                            'metadata': {
                                'stock_code': stock_code,
                                'data_points': len(kline_data),
                                'indicators_count': len(cached_df.columns) - 1,  # æ’é™¤order_book_idåˆ—
                                'calculation_time': datetime.now().isoformat(),
                                'cached': True,
                                'errors': []
                            }
                        }
                        return result
                    else:
                        # ç¼“å­˜æ•°æ®ä¸åŒ¹é…ï¼Œæ¸…é™¤æ— æ•ˆç¼“å­˜
                        del self.indicator_cache[cache_key]
                        logger.debug(f"ğŸ§¹ æ¸…é™¤æ— æ•ˆæŒ‡æ ‡ç¼“å­˜: {cache_key}")
                
                # å¦‚æœè§„èŒƒåŒ–é”®æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„order_book_id
                for existing_key, cached_df in self.indicator_cache.items():
                    if ('order_book_id' in cached_df.columns and 
                        cached_df['order_book_id'].iloc[0] == cache_key):
                        logger.debug(f"âœ… æŒ‡æ ‡ç¼“å­˜å‘½ä¸­ (é€šè¿‡order_book_idåŒ¹é…): {cache_key}")
                        # ä»ç¼“å­˜çš„DataFrameé‡å»ºç»“æœ
                        latest_values = {}
                        for col in cached_df.columns:
                            if col != 'order_book_id':  # æ’é™¤order_book_idåˆ—
                                latest_val = cached_df[col].iloc[-1] if not cached_df[col].isna().all() else None
                                if latest_val is not None and not pd.isna(latest_val):
                                    latest_values[col] = float(latest_val)

                        result = {
                            'indicators_df': cached_df,
                            'latest_values': latest_values,
                            'calculation_stats': self.calculation_stats.copy(),
                            'metadata': {
                                'stock_code': stock_code,
                                'data_points': len(kline_data),
                                'indicators_count': len(cached_df.columns) - 1,  # æ’é™¤order_book_idåˆ—
                                'calculation_time': datetime.now().isoformat(),
                                'cached': True,
                                'errors': []
                            }
                        }
                        return result

            # ç¼“å­˜æœªå‘½ä¸­æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œå¼€å§‹è®¡ç®—
            logger.debug(f"ğŸ”„ è®¡ç®—æŠ€æœ¯æŒ‡æ ‡: {stock_code}")

            # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
            if not isinstance(kline_data.index, pd.DatetimeIndex):
                # å¤„ç†MultiIndexçš„æƒ…å†µï¼Œæå–æ—¶é—´éƒ¨åˆ†ä½œä¸ºæ–°ç´¢å¼•
                if isinstance(kline_data.index, pd.MultiIndex):
                    kline_data = kline_data.reset_index(level=0, drop=True)
                else:
                    kline_data.index = pd.to_datetime(kline_data.index)

            # æŒ‰æ—¶é—´æ’åº
            kline_data = kline_data.sort_index()

            # æå–ä»·æ ¼æ•°æ®
            close_prices = kline_data['close'].values
            high_prices = kline_data['high'].values
            low_prices = kline_data['low'].values
            open_prices = kline_data['open'].values
            volume = kline_data['volume'].values

            # åˆå§‹åŒ–ç»“æœå­—å…¸
            indicators = {}
            calculation_errors = []

            # è®¡ç®—æ‰€æœ‰é…ç½®çš„æŒ‡æ ‡
            for indicator_name, config in self.INDICATOR_CONFIG.items():
                try:
                    if indicator_name == "SMA":
                        for period in config["periods"]:
                            key = f"SMA_{period}"
                            # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥å…¼å®¹talib
                            close_prices_np = self._safe_to_numpy(close_prices)
                            result = talib.SMA(close_prices_np, timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "EMA":
                        for period in config["periods"]:
                            key = f"EMA_{period}"
                            result = talib.EMA(self._safe_to_numpy(close_prices), timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "RSI":
                        for period in config["periods"]:
                            key = f"RSI_{period}"
                            result = talib.RSI(self._safe_to_numpy(close_prices), timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "MACD":
                        macd, macdsignal, macdhist = talib.MACD(
                            self._safe_to_numpy(close_prices),
                            fastperiod=config["fast_period"],
                            slowperiod=config["slow_period"],
                            signalperiod=config["signal_period"]
                        )
                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                        for arr, name in [(macd, "MACD"), (macdsignal, "MACD_SIGNAL"), (macdhist, "MACD_HIST")]:
                            if len(arr) != len(close_prices):
                                arr = np.concatenate([np.full(len(close_prices) - len(arr), np.nan), arr])
                            indicators[name] = arr

                    elif indicator_name == "BB":
                        upper, middle, lower = talib.BBANDS(
                            self._safe_to_numpy(close_prices),
                            timeperiod=config["period"],
                            nbdevup=config["std_dev"],
                            nbdevdn=config["std_dev"]
                        )
                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                        for arr, name in [(upper, "BB_UPPER"), (middle, "BB_MIDDLE"), (lower, "BB_LOWER")]:
                            if len(arr) != len(close_prices):
                                arr = np.concatenate([np.full(len(close_prices) - len(arr), np.nan), arr])
                            indicators[name] = arr

                    elif indicator_name == "ATR":
                        for period in config["periods"]:
                            key = f"ATR_{period}"
                            result = talib.ATR(self._safe_to_numpy(high_prices), self._safe_to_numpy(low_prices), self._safe_to_numpy(close_prices), timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "STOCH":
                        slowk, slowd = talib.STOCH(
                            self._safe_to_numpy(high_prices), self._safe_to_numpy(low_prices), self._safe_to_numpy(close_prices),
                            fastk_period=config["fastk_period"],
                            slowk_period=config["slowk_period"],
                            slowd_period=config["slowd_period"]
                        )
                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                        for arr, name in [(slowk, "STOCH_SLOWK"), (slowd, "STOCH_SLOWD")]:
                            if len(arr) != len(close_prices):
                                arr = np.concatenate([np.full(len(close_prices) - len(arr), np.nan), arr])
                            indicators[name] = arr

                    elif indicator_name == "CCI":
                        for period in config["periods"]:
                            key = f"CCI_{period}"
                            result = talib.CCI(self._safe_to_numpy(high_prices), self._safe_to_numpy(low_prices), self._safe_to_numpy(close_prices), timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "ROC":
                        for period in config["periods"]:
                            key = f"ROC_{period}"
                            result = talib.ROC(self._safe_to_numpy(close_prices), timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "TEMA":
                        for period in config["periods"]:
                            key = f"TEMA_{period}"
                            result = talib.TEMA(self._safe_to_numpy(close_prices), timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "WMA":
                        for period in config["periods"]:
                            key = f"WMA_{period}"
                            result = talib.WMA(self._safe_to_numpy(close_prices), timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "DMI":
                        plus_di = talib.PLUS_DI(self._safe_to_numpy(high_prices), self._safe_to_numpy(low_prices), self._safe_to_numpy(close_prices), timeperiod=config["period"])
                        minus_di = talib.MINUS_DI(self._safe_to_numpy(high_prices), self._safe_to_numpy(low_prices), self._safe_to_numpy(close_prices), timeperiod=config["period"])
                        adx = talib.ADX(self._safe_to_numpy(high_prices), self._safe_to_numpy(low_prices), self._safe_to_numpy(close_prices), timeperiod=config["period"])
                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                        for arr, name in [(plus_di, "PLUS_DI"), (minus_di, "MINUS_DI"), (adx, "ADX")]:
                            if len(arr) != len(close_prices):
                                arr = np.concatenate([np.full(len(close_prices) - len(arr), np.nan), arr])
                            indicators[name] = arr

                    elif indicator_name == "OBV":
                        result = talib.OBV(self._safe_to_numpy(close_prices), self._safe_to_numpy(volume))
                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                        if len(result) != len(close_prices):
                            result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                        indicators["OBV"] = result

                    elif indicator_name == "VOLUME_SMA":
                        for period in config["periods"]:
                            key = f"VOLUME_SMA_{period}"
                            result = talib.SMA(self._safe_to_numpy(volume), timeperiod=period)
                            # ç¡®ä¿é•¿åº¦åŒ¹é…
                            if len(result) != len(close_prices):
                                result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                            indicators[key] = result

                    elif indicator_name == "MFI":
                        result = talib.MFI(self._safe_to_numpy(high_prices), self._safe_to_numpy(low_prices), self._safe_to_numpy(close_prices), self._safe_to_numpy(volume), timeperiod=config["period"])
                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                        if len(result) != len(close_prices):
                            result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                        indicators["MFI"] = result

                    elif indicator_name == "WILLR":
                        result = talib.WILLR(self._safe_to_numpy(high_prices), self._safe_to_numpy(low_prices), self._safe_to_numpy(close_prices), timeperiod=config["period"])
                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                        if len(result) != len(close_prices):
                            result = np.concatenate([np.full(len(close_prices) - len(result), np.nan), result])
                        indicators["WILLR"] = result

                    elif indicator_name == "VOLATILITY":
                        # è®¡ç®—ä»·æ ¼æ³¢åŠ¨ç‡ (ä½¿ç”¨æ”¶ç›˜ä»·çš„æ ‡å‡†å·®)
                        returns = np.diff(self._safe_to_numpy(close_prices)) / self._safe_to_numpy(close_prices)[:-1]
                        volatility = pd.Series(returns).rolling(window=config["period"]).std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
                        # ç¡®ä¿é•¿åº¦åŒ¹é…åŸå§‹æ•°æ®
                        volatility_full = np.full(len(close_prices), np.nan)
                        # volatilityçš„é•¿åº¦æ˜¯ len(close_prices) - config["period"]ï¼Œå› ä¸ºrollingéœ€è¦periodä¸ªæ•°æ®ç‚¹
                        if volatility is not None and not volatility.empty:
                            vol_length = len(volatility.dropna())
                            if vol_length > 0:
                                start_idx = config["period"]
                                end_idx = start_idx + vol_length
                                if end_idx <= len(close_prices):
                                    volatility_full[start_idx:end_idx] = volatility.dropna().values
                        indicators["VOLATILITY"] = volatility_full

                except Exception as e:
                    calculation_errors.append(f"{indicator_name}: {str(e)}")
                    logger.warning(f"âš ï¸ è®¡ç®—æŒ‡æ ‡å¤±è´¥ {indicator_name}: {e}")

            # å¤„ç†è®¡ç®—ç»“æœ
            if calculation_errors:
                logger.warning(f"âš ï¸ æŒ‡æ ‡è®¡ç®—é”™è¯¯: {len(calculation_errors)} ä¸ª")
                for error in calculation_errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    logger.warning(f"  - {error}")

            # è½¬æ¢ä¸ºDataFrameæ ¼å¼
            result_df = pd.DataFrame(indicators, index=kline_data.index)

            # æ·»åŠ order_book_idåˆ—ç”¨äºç¼“å­˜éªŒè¯ï¼ˆä¸kline_cacheä¿æŒä¸€è‡´ï¼‰
            if 'order_book_id' in kline_data.columns:
                result_df['order_book_id'] = kline_data['order_book_id']
                logger.debug(f"ğŸ“Š æŒ‡æ ‡DataFrameæ·»åŠ order_book_idåˆ—: {kline_data['order_book_id'].iloc[0]}")

            # å¤„ç†é•¿åº¦ä¸åŒ¹é…çš„é—®é¢˜ - æŸäº›æŒ‡æ ‡å¯èƒ½äº§ç”Ÿä¸åŒé•¿åº¦
            if len(result_df) != len(kline_data):
                logger.debug(f"ğŸ”§ å¯¹é½æŒ‡æ ‡æ•°æ®é•¿åº¦: {len(result_df)} -> {len(kline_data)}")
                # é‡æ–°ç´¢å¼•ä»¥åŒ¹é…åŸå§‹æ•°æ®çš„é•¿åº¦
                result_df = result_df.reindex(kline_data.index)

            # ç¼“å­˜è®¡ç®—ç»“æœï¼ˆå¦‚æœæœ‰æœ‰æ•ˆçš„ç¼“å­˜é”®ï¼‰
            if cache_key:
                self.indicator_cache[cache_key] = result_df.copy()
                logger.debug(f"ğŸ’¾ æŒ‡æ ‡æ•°æ®å·²ç¼“å­˜: {cache_key}")

            # è·å–æœ€æ–°å€¼
            latest_values = {}
            for col in result_df.columns:
                if col != 'order_book_id':  # æ’é™¤order_book_idåˆ—
                    latest_val = result_df[col].iloc[-1] if not result_df[col].isna().all() else None
                    if latest_val is not None and not pd.isna(latest_val):
                        latest_values[col] = float(latest_val)

            # æ„å»ºè¿”å›ç»“æœ
            result = {
                'indicators_df': result_df,
                'latest_values': latest_values,
                'calculation_stats': self.calculation_stats.copy(),
                'metadata': {
                    'stock_code': stock_code,
                    'data_points': len(kline_data),
                    'indicators_count': len(result_df.columns) - (1 if 'order_book_id' in result_df.columns else 0),
                    'calculation_time': datetime.now().isoformat(),
                    'cached': False,
                    'errors': calculation_errors
                }
            }

            self.calculation_stats['successful_calculations'] += 1
            logger.debug(f"âœ… æŒ‡æ ‡è®¡ç®—å®Œæˆ: {stock_code}, {len(result_df.columns)}ä¸ªæŒ‡æ ‡")

            return result

        except Exception as e:
            self.calculation_stats['errors'] += 1
            logger.error(f"âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}

    def get_indicator_stats(self) -> Dict:
        """è·å–æŒ‡æ ‡è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.calculation_stats.copy()
        total = stats['total_calculations']
        if total > 0:
            stats['success_rate'] = stats['successful_calculations'] / total
            stats['error_rate'] = stats['errors'] / total
        else:
            stats['success_rate'] = 0.0
            stats['error_rate'] = 0.0

        # æ·»åŠ ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        stats['cache_size'] = len(self.indicator_cache)
        stats['cache_keys'] = list(self.indicator_cache.keys())  # ç°åœ¨æ˜¯è‚¡ç¥¨ä»£ç åˆ—è¡¨

        return stats

    def clear_indicator_cache(self, stock_code: Optional[str] = None) -> int:
        """
        æ¸…é™¤æŒ‡æ ‡ç¼“å­˜

        Args:
            stock_code: æŒ‡å®šè‚¡ç¥¨ä»£ç ï¼Œåªæ¸…é™¤è¯¥è‚¡ç¥¨çš„ç¼“å­˜ï¼›å¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜

        Returns:
            int: æ¸…é™¤çš„ç¼“å­˜æ¡ç›®æ•°é‡
        """
        try:
            cleared_count = 0

            if stock_code:
                # åªæ¸…é™¤æŒ‡å®šè‚¡ç¥¨çš„ç¼“å­˜
                if stock_code in self.indicator_cache:
                    del self.indicator_cache[stock_code]
                    cleared_count += 1
                
                # æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„order_book_id
                keys_to_remove = []
                for cache_key, cached_df in self.indicator_cache.items():
                    if ('order_book_id' in cached_df.columns and 
                        cached_df['order_book_id'].iloc[0] == stock_code):
                        keys_to_remove.append(cache_key)
                
                for key in keys_to_remove:
                    del self.indicator_cache[key]
                    cleared_count += 1
                    
                if cleared_count == 0:
                    logger.debug(f"â„¹ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨{stock_code}çš„æŒ‡æ ‡ç¼“å­˜")
            else:
                # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
                cleared_count = len(self.indicator_cache)
                self.indicator_cache.clear()

            if cleared_count > 0:
                logger.info(f"ğŸ§¹ æ¸…é™¤æŒ‡æ ‡ç¼“å­˜: {cleared_count} æ¡è®°å½•")
            else:
                logger.debug("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…é™¤çš„ç¼“å­˜")

            return cleared_count

        except Exception as e:
            logger.error(f"âŒ æ¸…é™¤æŒ‡æ ‡ç¼“å­˜å¤±è´¥: {e}")
            return 0

# ============================================================================
# æ•°æ®è´¨é‡è¯„ä¼°å™¨
# ============================================================================

class DataQualityEvaluator:
    """
    Data Quality Evaluator
    """

    # Data quality standards
    QUALITY_STANDARDS = {
        'valuation': {
            'required_fields': ['pe_ratio', 'pb_ratio', 'ps_ratio', 'pcf_ratio', 'market_cap'],
            'min_valid_ratio': 0.7,
            'max_nan_ratio': 0.5,
            'value_ranges': {
                'pe_ratio': {'min': -100, 'max': 1000},
                'pb_ratio': {'min': 0, 'max': 50},
                'ps_ratio': {'min': 0, 'max': 100},
                'pcf_ratio': {'min': -100, 'max': 1000},
                'market_cap': {'min': 1e6, 'max': 1e13}
            }
        },
        'technical': {
            'required_indicators': ['RSI_14', 'MACD', 'BB_UPPER', 'BB_LOWER', 'SMA_5', 'SMA_10', 'SMA_20'],
            'min_valid_ratio': 0.6,
            'max_nan_ratio': 0.4,
            'value_ranges': {
                'RSI_14': {'min': 0, 'max': 100},
                'MACD': {'min': -100, 'max': 100},
                'BB_UPPER': {'min': 0, 'max': 1e6},
                'BB_LOWER': {'min': 0, 'max': 1e6},
                'SMA_5': {'min': 0, 'max': 1e6},
                'SMA_10': {'min': 0, 'max': 1e6},
                'SMA_20': {'min': 0, 'max': 1e6}
            }
        },
        'price': {
            'required_fields': ['open', 'close', 'high', 'low', 'volume'],
            'min_valid_ratio': 0.8,
            'max_nan_ratio': 0.2,
            'value_ranges': {
                'open': {'min': 0.01, 'max': 1e6},
                'close': {'min': 0.01, 'max': 1e6},
                'high': {'min': 0.01, 'max': 1e6},
                'low': {'min': 0.01, 'max': 1e6},
                'volume': {'min': 0, 'max': 1e10}
            },
            'price_consistency': True
        }
    }

    def __init__(self, manager):
        self.manager = manager
        self.quality_reports = {}

    def evaluate_data_quality(self, stock_info: Dict, technical_indicators: Dict,
                            data_types: List[str] = None) -> Dict[str, bool]:
        """
        Evaluate data quality
        """
        if data_types is None:
            data_types = ['valuation', 'technical', 'price']

        results = {}
        quality_report = {}

        for data_type in data_types:
            if data_type == 'valuation':
                is_valid, report = self._evaluate_valuation_quality(stock_info)
            elif data_type == 'technical':
                is_valid, report = self._evaluate_technical_quality(technical_indicators)
            elif data_type == 'price':
                is_valid, report = self._evaluate_price_quality(technical_indicators)
            else:
                continue

            results[data_type] = is_valid
            quality_report[data_type] = report

        # Store quality report
        stock_code = stock_info.get('stock_code', 'unknown')
        self.quality_reports[stock_code] = quality_report

        return results

    def _evaluate_valuation_quality(self, stock_info: Dict) -> Tuple[bool, Dict]:
        """
        Evaluate valuation data quality
        """
        standards = self.QUALITY_STANDARDS['valuation']
        report = {
            'total_fields': 0,
            'valid_fields': 0,
            'nan_fields': 0,
            'out_of_range_fields': 0,
            'valid_ratio': 0.0,
            'nan_ratio': 0.0,
            'issues': []
        }

        required_fields = standards['required_fields']
        value_ranges = standards['value_ranges']

        for field in required_fields:
            report['total_fields'] += 1
            value = stock_info.get(field)

            if value is None or pd.isna(value):
                report['nan_fields'] += 1
                report['issues'].append(f"{field}: NaN")
                continue

            # Check value range
            if field in value_ranges:
                min_val = value_ranges[field]['min']
                max_val = value_ranges[field]['max']
                if not (min_val <= value <= max_val):
                    report['out_of_range_fields'] += 1
                    report['issues'].append(f"{field}: value {value} out of range [{min_val}, {max_val}]")
                    continue

            report['valid_fields'] += 1

        # Calculate ratios
        if report['total_fields'] > 0:
            report['valid_ratio'] = report['valid_fields'] / report['total_fields']
            report['nan_ratio'] = report['nan_fields'] / report['total_fields']

        # Quality assessment
        is_valid = (
            report['valid_ratio'] >= standards['min_valid_ratio'] and
            report['nan_ratio'] <= standards['max_nan_ratio'] and
            report['out_of_range_fields'] == 0
        )

        report['overall_quality'] = 'PASS' if is_valid else 'FAIL'

        return is_valid, report

    def _evaluate_technical_quality(self, technical_indicators: Dict) -> Tuple[bool, Dict]:
        """
        Evaluate technical indicators quality
        """
        standards = self.QUALITY_STANDARDS['technical']
        report = {
            'total_indicators': 0,
            'valid_indicators': 0,
            'nan_indicators': 0,
            'out_of_range_indicators': 0,
            'valid_ratio': 0.0,
            'nan_ratio': 0.0,
            'issues': []
        }

        required_indicators = standards['required_indicators']
        value_ranges = standards['value_ranges']

        # Get latest values
        latest_values = technical_indicators.get('latest_values', {})

        for indicator in required_indicators:
            report['total_indicators'] += 1
            value = latest_values.get(indicator)

            if value is None or pd.isna(value):
                report['nan_indicators'] += 1
                report['issues'].append(f"{indicator}: NaN")
                continue

            # Check value range
            if indicator in value_ranges:
                min_val = value_ranges[indicator]['min']
                max_val = value_ranges[indicator]['max']
                if not (min_val <= value <= max_val):
                    report['out_of_range_indicators'] += 1
                    report['issues'].append(f"{indicator}: value {value} out of range [{min_val}, {max_val}]")
                    continue

            report['valid_indicators'] += 1

        # Calculate ratios
        if report['total_indicators'] > 0:
            report['valid_ratio'] = report['valid_indicators'] / report['total_indicators']
            report['nan_ratio'] = report['nan_indicators'] / report['total_indicators']

        # Quality assessment
        is_valid = (
            report['valid_ratio'] >= standards['min_valid_ratio'] and
            report['nan_ratio'] <= standards['max_nan_ratio'] and
            report['out_of_range_indicators'] == 0
        )

        report['overall_quality'] = 'PASS' if is_valid else 'FAIL'

        return is_valid, report

    def _evaluate_price_quality(self, technical_indicators: Dict) -> Tuple[bool, Dict]:
        """
        Evaluate price data quality
        """
        standards = self.QUALITY_STANDARDS['price']
        report = {
            'total_fields': 0,
            'valid_fields': 0,
            'nan_fields': 0,
            'out_of_range_fields': 0,
            'consistency_issues': 0,
            'valid_ratio': 0.0,
            'nan_ratio': 0.0,
            'issues': []
        }

        required_fields = standards['required_fields']
        value_ranges = standards['value_ranges']

        # Get latest values
        latest_values = technical_indicators.get('latest_values', {})

        for field in required_fields:
            report['total_fields'] += 1
            value = latest_values.get(field)

            if value is None or pd.isna(value):
                report['nan_fields'] += 1
                report['issues'].append(f"{field}: NaN")
                continue

            # Check value range
            if field in value_ranges:
                min_val = value_ranges[field]['min']
                max_val = value_ranges[field]['max']
                if not (min_val <= value <= max_val):
                    report['out_of_range_fields'] += 1
                    report['issues'].append(f"{field}: value {value} out of range [{min_val}, {max_val}]")
                    continue

            report['valid_fields'] += 1

        # Check price consistency
        if standards.get('price_consistency', False):
            open_price = latest_values.get('open')
            close_price = latest_values.get('close')
            high_price = latest_values.get('high')
            low_price = latest_values.get('low')

            if all(p is not None and not pd.isna(p) for p in [open_price, close_price, high_price, low_price]):
                # Check price logic
                if not (high_price >= close_price >= low_price):
                    report['consistency_issues'] += 1
                    report['issues'].append("Price logic error: high >= close >= low")

                if not (high_price >= open_price >= low_price):
                    report['consistency_issues'] += 1
                    report['issues'].append("Price logic error: high >= open >= low")

        # Calculate ratios
        if report['total_fields'] > 0:
            report['valid_ratio'] = report['valid_fields'] / report['total_fields']
            report['nan_ratio'] = report['nan_fields'] / report['total_fields']

        # Quality assessment
        is_valid = (
            report['valid_ratio'] >= standards['min_valid_ratio'] and
            report['nan_ratio'] <= standards['max_nan_ratio'] and
            report['out_of_range_fields'] == 0 and
            report['consistency_issues'] == 0
        )

        report['overall_quality'] = 'PASS' if is_valid else 'FAIL'

        return is_valid, report

    def get_quality_report(self, stock_code: str = None) -> Dict:
        """
        Get quality report
        """
        if stock_code:
            return self.quality_reports.get(stock_code, {})
        return self.quality_reports

    def is_data_quality_acceptable(self, stock_info: Dict, technical_indicators: Dict,
                                 data_types: List[str] = None) -> bool:
        """
        Check if data quality is acceptable
        """
        quality_results = self.evaluate_data_quality(stock_info, technical_indicators, data_types)
        return all(quality_results.values())

class ScoringEngine:
    """
    Unified Scoring Engine
    """

    def __init__(self, manager):
        self.manager = manager
        # DataQualityEvaluator will be initialized later
        self.quality_evaluator = None
        self.rules = self._define_rules()

    def _init_quality_evaluator(self, manager):
        """Initialize data quality evaluator"""
        self.quality_evaluator = DataQualityEvaluator(manager)

    def _define_rules(self):
        """Define scoring rules"""
        return {
            'pe': {
                'field': 'pe_ratio',
                'data_source': 'stock_info',
                'ideal_range': (8, 25),
                'ideal_score': 15,
                'good_range': (5, 40),
                'good_score': 8
            },
            'pb': {
                'field': 'pb_ratio',
                'data_source': 'stock_info',
                'ideal_range': (0.5, 3.0),
                'ideal_score': 12,
                'good_range': (0.3, 5.0),
                'good_score': 6
            },
            'rsi': {
                'field': 'RSI_14',
                'data_source': 'technical',
                'ideal_range': (40, 60),
                'ideal_score': 10,
                'good_range': (30, 70),
                'good_score': 5
            },
            'turnover': {
                'field': 'turnover_ratio',
                'data_source': 'stock_info',
                'ideal_range': (1.0, 8.0),
                'ideal_score': 8,
                'good_range': (0.5, 15.0),
                'good_score': 4
            }
        }

    def calculate_score(self, stock_info: Dict, technical_indicators: Dict,
    rule_keys: List[str], layer: str = 'basic') -> float:
        """
        Calculate score based on rules (with data quality assessment)
        """
        try:
            # First perform data quality assessment
            stock_code = stock_info.get('stock_code', 'unknown')

            # Determine data types to check
            data_types_to_check = []
            for rule_key in rule_keys:
                if rule_key in self.rules:
                    rule = self.rules[rule_key]
                    data_source = rule.get('data_source')
                    if data_source not in data_types_to_check:
                        data_types_to_check.append(data_source)

            # Data quality assessment
            quality_results = self.quality_evaluator.evaluate_data_quality(
                stock_info, technical_indicators, data_types_to_check
            )

            # Check if all data types pass
            if not all(quality_results.values()):
                self.manager.logger.warning(f"Data quality failed for {stock_code}, skipping scoring")
                return 0.0

            # Data quality passed, start scoring
            self.manager.logger.debug(f"Data quality passed for {stock_code}, starting scoring")

            total_score = 0
            for rule_key in rule_keys:
                if rule_key not in self.rules:
                    continue

                rule = self.rules[rule_key]
                score = self._apply_rule(rule, stock_info, technical_indicators, layer)
                total_score += score

            final_score = max(0, min(100, total_score))
            self.manager.logger.debug(f"Scoring completed for {stock_code}: {final_score:.1f}")

            return final_score

        except Exception as e:
            stock_code = stock_info.get('stock_code', 'unknown')
            self.manager.logger.error(f"Scoring calculation failed for {stock_code}: {e}")
            return 0.0

    def _apply_rule(self, rule: Dict, stock_info: Dict, technical_indicators: Dict, layer: str) -> float:
        """Apply single scoring rule"""
        try:
            # Get data value
            value = self._get_value_for_rule(rule, stock_info, technical_indicators)
            if value is None or pd.isna(value):
                return 0

            # Range scoring
            return self._calculate_range_score(rule, value, layer)

        except Exception as e:
            self.manager.logger.error(f"Rule application failed for {rule.get('field', 'unknown')}: {e}")
            return 0

    def _get_value_for_rule(self, rule: Dict, stock_info: Dict, technical_indicators: Dict):
        """Get data value for rule"""
        if rule['data_source'] == 'stock_info':
            return stock_info.get(rule['field'])
        elif rule['data_source'] == 'technical':
            return technical_indicators.get('latest_values', {}).get(rule['field'])
        return None

    def _calculate_range_score(self, rule: Dict, value: float, layer: str) -> float:
        """Calculate range score"""
        # Ideal range
        if 'ideal_range' in rule:
            min_val, max_val = rule['ideal_range']
            if min_val <= value <= max_val:
                return rule.get('ideal_score', 10)

        # Good range
        if 'good_range' in rule:
            min_val, max_val = rule['good_range']
            if min_val <= value <= max_val:
                return rule.get('good_score', 5)

        return 0

    def _calculate_common_score_factors(self, stock_info: Dict, technical_indicators: Dict) -> Dict:
        """
        è®¡ç®—é€šç”¨è¯„åˆ†å› å­
        """
        try:
            factors = {}

            # ä¼°å€¼å› å­
            pe = stock_info.get('pe_ratio')
            if pe is not None and not pd.isna(pe):
                if 8 <= pe <= 25:
                    factors['pe_score'] = 15
                elif 5 <= pe <= 40:
                    factors['pe_score'] = 8
                else:
                    factors['pe_score'] = 0

            # æŠ€æœ¯å› å­
            rsi = technical_indicators.get('latest_values', {}).get('RSI_14')
            if rsi is not None and not pd.isna(rsi):
                if 40 <= rsi <= 60:
                    factors['rsi_score'] = 10
                elif 30 <= rsi <= 70:
                    factors['rsi_score'] = 5
                else:
                    factors['rsi_score'] = 0

            # æˆäº¤é‡å› å­
            turnover = stock_info.get('turnover_ratio')
            if turnover is not None and not pd.isna(turnover):
                if 1.0 <= turnover <= 8.0:
                    factors['turnover_score'] = 8
                elif 0.5 <= turnover <= 15.0:
                    factors['turnover_score'] = 4
                else:
                    factors['turnover_score'] = 0

            return factors

        except Exception as e:
            self._log_error(f"é€šç”¨è¯„åˆ†å› å­è®¡ç®—å¤±è´¥: {e}")
            return {}

# ============================================================================
# ä¸»è‚¡ç¥¨æ± ç®¡ç†å™¨
# ============================================================================

def main():
    """ä¸»å¯åŠ¨å‡½æ•° - ç›´æ¥å¯åŠ¨è‚¡ç¥¨æ± åŒæ­¥ç¨‹åº"""
    # åˆå§‹åŒ–ç¯å¢ƒç®¡ç†å™¨
    env_manager = EnvironmentManager()
    env_manager.ensure_environment_with_fallback()

    # åˆå§‹åŒ–æ—¥å¿—
    from modules.log_manager import get_system_logger
    logger = get_system_logger()

    logger.info("=" * 60)
    logger.info("ç›´æ¥å¯åŠ¨è‚¡ç¥¨æ± åŒæ­¥ç¨‹åº")
    logger.info("=" * 60)

    # åˆå§‹åŒ–RQDatac
    from stockpool_tool import init_rqdatac
    if not init_rqdatac():
        logger.error("RQDatacåˆå§‹åŒ–å¤±è´¥")
        return False

    try:
        # åˆ›å»ºPoolManagerå®ä¾‹
        pool_manager = PoolManager()

        # æ‰§è¡Œæ¯æ—¥åŒæ­¥è®¡ç®—å»ºæ± 
        logger.info("å¼€å§‹æ‰§è¡Œè‚¡ç¥¨æ± åŒæ­¥...")
        success = pool_manager.process_daily_sync_caculate_buildpool()

        if success:
            logger.info("âœ“ è‚¡ç¥¨æ± åŒæ­¥å®Œæˆ")
            return True
        else:
            logger.error("âœ— è‚¡ç¥¨æ± åŒæ­¥å¤±è´¥")
            return False

    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


def process_daily_sync_caculate_buildpool_optimized() -> bool:
    """è¿è¡Œæ¯æ—¥åŒæ­¥ã€è®¡ç®—å’Œæ„å»ºè‚¡ç¥¨æ± çš„å®Œæ•´æµç¨‹ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘æ•°æ®æ‹·è´"""
    try:
        logger.info("ğŸš€ å¼€å§‹æ¯æ—¥è‚¡ç¥¨æ± åŒæ­¥ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")

        # åˆ›å»ºPoolManagerå®ä¾‹
        manager = PoolManager()

        # ===== ç¬¬ä¸€é˜¶æ®µï¼šè·å–è‚¡ç¥¨åˆ—è¡¨ =====
        logger.info("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        stock_list_df = manager.data_store.fetch_stock_list()

        if stock_list_df is None or stock_list_df.empty:
            logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ŒåŒæ­¥å¤±è´¥")
            return False

        # ç›´æ¥ä½¿ç”¨DataFrameï¼Œé¿å…è½¬æ¢ä¸ºåˆ—è¡¨
        stock_codes = stock_list_df['order_book_id'].tolist()
        logger.info(f"âœ… è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")

        # ===== ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è·å–ä¼°å€¼æ•°æ®ï¼ˆDataFrameæ ¼å¼ï¼‰ =====
        logger.info("ğŸ’° ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è·å–ä¼°å€¼æ•°æ®...")
        target_date = manager.data_store.get_target_trading_date()
        if not target_date:
            logger.error("âŒ æ— æ³•ç¡®å®šç›®æ ‡äº¤æ˜“æ—¥ï¼ŒåŒæ­¥å¤±è´¥")
            return False
        logger.info(f"ğŸ¯ ç›®æ ‡åˆ†ææ—¥æœŸ: {target_date}")

        # è·å–DataFrameæ ¼å¼çš„ä¼°å€¼æ•°æ®
        valuation_df = manager._batch_get_valuation_data(stock_codes, target_date, return_dataframe=True)
        if not isinstance(valuation_df, pd.DataFrame) or valuation_df.empty:
            logger.warning("âš ï¸ æœªè·å–åˆ°ä¼°å€¼æ•°æ®")
            valuation_df = pd.DataFrame()

        # ===== ç¬¬ä¸‰é˜¶æ®µï¼šæ‰¹é‡è·å–ä»·æ ¼æ•°æ® =====
        logger.info("ğŸ“ˆ ç¬¬ä¸‰æ­¥ï¼šæ‰¹é‡è·å–ä»·æ ¼åºåˆ—æ•°æ®...")
        start_date = (datetime.strptime(target_date, '%Y-%m-%d') -
                     timedelta(days=manager.config['history_days'])).strftime('%Y-%m-%d')
        logger.info(f"ğŸ“… å†å²æ•°æ®èŒƒå›´: {start_date} è‡³ {target_date}")

        price_data = manager._batch_get_price_data(stock_codes, start_date, target_date)
        valid_price_stocks = [code for code, df in price_data.items() if df is not None and not df.empty]
        logger.info(f"âœ… è·å–åˆ° {len(valid_price_stocks)} åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®")

        # ===== ç¬¬å››é˜¶æ®µï¼šç›´æ¥æ„å»ºè¯„åˆ†DataFrame =====
        logger.info("ğŸ”§ ç¬¬å››æ­¥ï¼šæ„å»ºè¯„åˆ†DataFrame...")

        # ç›´æ¥åˆ›å»ºè¯„åˆ†DataFrameï¼Œé¿å…ä¸­é—´å­—å…¸è½¬æ¢
        scored_rows = []

        for stock_code in stock_codes:
            try:
                # ä»DataFrameä¸­è·å–è‚¡ç¥¨ä¿¡æ¯
                if isinstance(valuation_df, pd.DataFrame) and not valuation_df.empty:
                    stock_info_row = valuation_df[valuation_df['stock_code'] == stock_code]
                    stock_info = stock_info_row.iloc[0].to_dict() if not stock_info_row.empty else {'stock_code': stock_code}
                else:
                    stock_info = {'stock_code': stock_code}

                # è·å–ä»·æ ¼æ•°æ®
                price_df = price_data.get(stock_code)

                if price_df is not None and not price_df.empty:
                    # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                    technical_indicators = manager.calculate_technical_indicators(price_df, stock_code)

                    # è®¡ç®—è¯„åˆ†
                    basic_score = manager.calculate_basic_layer_score(stock_info, technical_indicators)
                    watch_score = manager.calculate_watch_layer_score(stock_info, technical_indicators)
                    core_score = manager.calculate_core_layer_score(stock_info, technical_indicators)

                    # ç›´æ¥æ·»åŠ åˆ°è¡Œåˆ—è¡¨
                    scored_rows.append({
                        'stock_code': stock_code,
                        'basic_score': basic_score,
                        'watch_score': watch_score,
                        'core_score': core_score,
                        'market_cap': stock_info.get('market_cap'),
                        'pe_ratio': stock_info.get('pe_ratio'),
                        'pb_ratio': stock_info.get('pb_ratio'),
                        'current_price': technical_indicators.get('latest_values', {}).get('current_price'),
                        'rsi': technical_indicators.get('latest_values', {}).get('RSI_14'),
                        'turnover_rate': technical_indicators.get('latest_values', {}).get('turnover_rate'),
                        'volatility': technical_indicators.get('latest_values', {}).get('volatility_20d'),
                        'date': target_date
                    })
                else:
                    logger.debug(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ— ä»·æ ¼æ•°æ®ï¼Œè·³è¿‡")

            except Exception as e:
                logger.warning(f"âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                continue

        # ä¸€æ¬¡æ€§åˆ›å»ºè¯„åˆ†DataFrame
        if scored_rows:
            df_scored = pd.DataFrame(scored_rows)
            logger.info(f"âœ… è¯„åˆ†è®¡ç®—å®Œæˆ: {len(df_scored)} åªè‚¡ç¥¨")
        else:
            df_scored = pd.DataFrame()
            logger.warning("âš ï¸ æ— æœ‰æ•ˆè¯„åˆ†æ•°æ®")
            return False

        # ===== ç¬¬äº”é˜¶æ®µï¼šæ„å»ºè‚¡ç¥¨æ±  =====
        logger.info("ğŸ—ï¸ ç¬¬äº”æ­¥ï¼šæ„å»ºä¸‰ä¸ªè‚¡ç¥¨æ± ...")
        result = manager.build_stock_pool(df_scored, target_date)

        if result and all(not df.empty for df in result.values()):
            logger.info("âœ… è‚¡ç¥¨æ± æ„å»ºæˆåŠŸ")
            return True
        else:
            logger.error("âŒ è‚¡ç¥¨æ± æ„å»ºå¤±è´¥")
            return False

    except Exception as e:
        logger.error(f"âŒ æ¯æ—¥åŒæ­¥å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)
