# Jobä¸Pipelineçš„åŒºåˆ«å’Œç”¨æ³•æŒ‡å—

## ğŸ“‹ æ¦‚å¿µæ¾„æ¸…

### Job (ä½œä¸š) vs Pipeline (ç®¡é“)

åœ¨Dagsteræ¡†æ¶ä¸­ï¼Œ**Job** å’Œ **Pipeline** æ˜¯ä¸¤ä¸ªå¯†åˆ‡ç›¸å…³ä½†æœ‰åŒºåˆ«çš„æ¦‚å¿µï¼š

#### ğŸ”„ Pipeline (ç®¡é“)
- **å®šä¹‰**: æè¿°æ•°æ®å¤„ç†æµç¨‹çš„é€»è¾‘ç»“æ„
- **ç‰¹ç‚¹**:
  - å£°æ˜å¼å®šä¹‰æ•°æ®æµå‘
  - å…³æ³¨"åšä»€ä¹ˆ"å’Œ"æ€ä¹ˆåš"
  - åŒ…å«æ“ä½œ(op)çš„ä¾èµ–å…³ç³»
  - ä¸å¯ç›´æ¥æ‰§è¡Œï¼Œéœ€è¦åŒ…è£…æˆJob

#### âš¡ Job (ä½œä¸š)
- **å®šä¹‰**: Pipelineçš„å¯æ‰§è¡Œå®ä¾‹
- **ç‰¹ç‚¹**:
  - å¯ä»¥ç›´æ¥è¿è¡Œå’Œè°ƒåº¦
  - åŒ…å«è¿è¡Œæ—¶é…ç½®
  - æœ‰æ‰§è¡Œå†å²å’ŒçŠ¶æ€è·Ÿè¸ª
  - æ”¯æŒé‡è¯•ã€ç›‘æ§ç­‰è¿ç»´ç‰¹æ€§

**ç®€å•ç†è§£**: Pipelineæ˜¯"è®¾è®¡å›¾"ï¼ŒJobæ˜¯"å®é™…æ‰§è¡Œçš„ä»»åŠ¡"

## ğŸ—ï¸ å½“å‰æ¶æ„åˆ†æ

### ç°æœ‰ç»„ä»¶

```python
# orchestration/job_definitions.py
@job(name="stock_analysis_job")
def stock_analysis_job():
    """è‚¡ç¥¨åˆ†æä½œä¸š - è¿™æ˜¯Job"""
    raw_data = load_market_data_op()      # æ“ä½œ1
    validated_data = validate_data_quality_op(raw_data)  # æ“ä½œ2
    indicator_data = calculate_indicators_op(validated_data)  # æ“ä½œ3
    scored_data = calculate_scores_op(indicator_data)    # æ“ä½œ4
    save_results_op(scored_data)         # æ“ä½œ5

# orchestration/pipeline_manager.py
class PipelineManager:
    """ç®¡é“ç®¡ç†å™¨ - ç®¡ç†Jobå’ŒAsset"""
    def register_job(self, name, job_def):
        """æ³¨å†ŒJob"""
    def run_job(self, job_name, **kwargs):
        """æ‰§è¡ŒJob"""
```

## ğŸ¯ æ¯æ—¥åŒæ­¥ã€è¡¥å…¨ã€æŒ‡æ ‡è®¡ç®—ã€å­˜ç›˜çš„æœ€ä½³å®è·µ

### æ¨èæ¶æ„è®¾è®¡

#### 1. ç»†ç²’åº¦Jobè®¾è®¡ (æ¨è)

```python
# æ¯æ—¥æ•°æ®åŒæ­¥Job
@job(name="daily_data_sync")
def daily_data_sync_job():
    """æ¯æ—¥æ•°æ®åŒæ­¥"""
    sync_from_rqdatac()
    validate_sync_quality()

# æ•°æ®è¡¥å…¨Job
@job(name="data_completion")
def data_completion_job():
    """æ•°æ®è¡¥å…¨"""
    detect_missing_data()
    fetch_missing_data()
    merge_and_validate()

# æŒ‡æ ‡è®¡ç®—Job
@job(name="indicator_calculation")
def indicator_calculation_job():
    """æŒ‡æ ‡è®¡ç®—"""
    load_base_data()
    calculate_technical_indicators()
    calculate_fundamental_indicators()

# è¯„åˆ†å’Œå­˜ç›˜Job
@job(name="scoring_and_save")
def scoring_and_save_job():
    """è¯„åˆ†è®¡ç®—å’Œå­˜ç›˜"""
    load_indicator_data()
    calculate_scores()
    save_to_storage()
    update_metadata()
```

#### 2. ç«¯åˆ°ç«¯Pipeline Job

```python
@job(name="daily_full_pipeline")
def daily_full_pipeline_job():
    """å®Œæ•´çš„æ¯æ—¥å¤„ç†ç®¡é“"""
    # 1. æ•°æ®åŒæ­¥
    raw_data = sync_data_op()

    # 2. æ•°æ®è¡¥å…¨
    completed_data = complete_data_op(raw_data)

    # 3. æŒ‡æ ‡è®¡ç®—
    indicators = calculate_indicators_op(completed_data)

    # 4. è¯„åˆ†è®¡ç®—
    scores = calculate_scores_op(indicators)

    # 5. å­˜ç›˜
    save_results_op(scores)
```

### å®é™…å®ç°å»ºè®®

#### æ–¹æ¡ˆä¸€ï¼šç‹¬ç«‹Jobæ¨¡å¼ (æ¨èç”¨äºå¤æ‚åœºæ™¯)

```python
# 1. æ¯æ—¥æ•°æ®åŒæ­¥Job
@job(name="daily_sync_job", tags={"type": "sync", "frequency": "daily"})
def daily_sync_job():
    """æ¯æ—¥æ•°æ®åŒæ­¥Job"""
    symbols = get_target_symbols()
    trading_dates = get_trading_dates()

    # å¹¶è¡ŒåŒæ­¥ä¸åŒç±»å‹æ•°æ®
    ohlcv_data = sync_ohlcv_op(symbols, trading_dates)
    fundamental_data = sync_fundamental_op(symbols, trading_dates)

    # æ•°æ®éªŒè¯
    validate_sync_op(ohlcv_data, fundamental_data)

# 2. æ•°æ®è¡¥å…¨Job
@job(name="completion_job", tags={"type": "completion", "depends": "sync"})
def completion_job():
    """æ•°æ®è¡¥å…¨Job"""
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    missing_dates = detect_gaps_op()

    # è¡¥å…¨ç¼ºå¤±æ•°æ®
    if missing_dates:
        completed_data = fetch_missing_op(missing_dates)
        merge_data_op(completed_data)

# 3. æŒ‡æ ‡è®¡ç®—Job
@job(name="indicator_job", tags={"type": "calculation", "depends": "completion"})
def indicator_job():
    """æŒ‡æ ‡è®¡ç®—Job"""
    # åŠ è½½è¡¥å…¨åçš„æ•°æ®
    data = load_processed_data_op()

    # å¹¶è¡Œè®¡ç®—ä¸åŒæŒ‡æ ‡
    technical_indicators = calc_technical_op(data)
    fundamental_indicators = calc_fundamental_op(data)

    # åˆå¹¶æŒ‡æ ‡
    merged_indicators = merge_indicators_op(technical_indicators, fundamental_indicators)

# 4. è¯„åˆ†å’Œå­˜ç›˜Job
@job(name="scoring_save_job", tags={"type": "scoring", "depends": "indicator"})
def scoring_save_job():
    """è¯„åˆ†å’Œå­˜ç›˜Job"""
    # åŠ è½½æŒ‡æ ‡æ•°æ®
    indicators = load_indicators_op()

    # è®¡ç®—è¯„åˆ†
    scores = calculate_scores_op(indicators)

    # å­˜å‚¨ç»“æœ
    save_to_parquet_op(scores)
    save_to_duckdb_op(scores)

    # æ›´æ–°å…ƒæ•°æ®
    update_metadata_op()
```

#### æ–¹æ¡ˆäºŒï¼šç»„åˆJobæ¨¡å¼ (æ¨èç”¨äºç®€å•åœºæ™¯)

```python
@job(name="morning_pipeline", tags={"time": "morning", "priority": "high"})
def morning_pipeline_job():
    """æ—©é—´å¤„ç†ç®¡é“"""
    # æ•°æ®åŒæ­¥ + è¡¥å…¨
    data = sync_and_complete_op()

    # æŒ‡æ ‡è®¡ç®—
    indicators = calculate_indicators_op(data)

    # ä¸´æ—¶å­˜å‚¨
    save_temp_op(indicators)

@job(name="evening_pipeline", tags={"time": "evening", "priority": "medium"})
def evening_pipeline_job():
    """æ™šé—´å¤„ç†ç®¡é“"""
    # åŠ è½½ä¸´æ—¶æ•°æ®
    indicators = load_temp_op()

    # è¯„åˆ†è®¡ç®—
    scores = calculate_scores_op(indicators)

    # æœ€ç»ˆå­˜ç›˜
    save_final_op(scores)
    cleanup_temp_op()
```

## ğŸš€ ä½¿ç”¨å»ºè®®

### 1. Jobè°ƒåº¦ç­–ç•¥

```python
# ä½¿ç”¨Dagster Schedule
from dagster import schedule

@schedule(
    cron_schedule="0 9 * * 1-5",  # å‘¨ä¸€åˆ°å‘¨äº”æ—©ä¸Š9ç‚¹
    job=daily_sync_job,
    execution_timezone="Asia/Shanghai"
)
def daily_sync_schedule():
    """æ¯æ—¥åŒæ­¥è°ƒåº¦"""
    return {}

@schedule(
    cron_schedule="0 15 * * 1-5",  # å‘¨ä¸€åˆ°å‘¨äº”ä¸‹åˆ3ç‚¹
    job=indicator_job,
    execution_timezone="Asia/Shanghai"
)
def afternoon_calculation_schedule():
    """ä¸‹åˆæŒ‡æ ‡è®¡ç®—è°ƒåº¦"""
    return {}
```

### 2. Jobä¾èµ–ç®¡ç†

```python
# ä½¿ç”¨Dagster Sensorç›‘å¬Jobå®Œæˆ
from dagster import sensor, run_status_sensor

@run_status_sensor(
    monitored_jobs=[daily_sync_job],
    run_status=DagsterRunStatus.SUCCESS
)
def trigger_completion_on_sync_success():
    """ç›‘å¬åŒæ­¥JobæˆåŠŸåè§¦å‘è¡¥å…¨"""
    return RunRequest(run_key=None, job_name="completion_job")
```

### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
@op(
    retry_policy=RetryPolicy(max_retries=3, delay=60),
    tags={"importance": "critical"}
)
def sync_ohlcv_op(symbols, dates):
    """å¸¦é‡è¯•çš„æ•°æ®åŒæ­¥æ“ä½œ"""
    try:
        return sync_ohlcv_data(symbols, dates)
    except Exception as e:
        logger.error(f"åŒæ­¥å¤±è´¥: {e}")
        raise
```

## ğŸ“Š ç›‘æ§å’Œè¿ç»´

### JobçŠ¶æ€ç›‘æ§

```python
# åœ¨PipelineManagerä¸­æ·»åŠ ç›‘æ§åŠŸèƒ½
def get_job_status(self, job_name: str) -> Dict[str, Any]:
    """è·å–JobçŠ¶æ€"""
    return {
        "name": job_name,
        "last_run": self._get_last_run_time(job_name),
        "status": self._get_run_status(job_name),
        "duration": self._get_run_duration(job_name),
        "success_rate": self._get_success_rate(job_name)
    }
```

### æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```python
@op
def collect_metrics_op(context, data):
    """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
    metrics = {
        "record_count": len(data),
        "processing_time": context.op_execution_context.get_step_execution_context().step_execution_time,
        "memory_usage": get_memory_usage(),
        "data_quality_score": calculate_quality_score(data)
    }

    # å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
    send_to_monitoring(metrics)

    return data
```

## ğŸ¯ æœ€ä½³å®è·µå»ºè®®

### 1. Jobè®¾è®¡åŸåˆ™
- **å•ä¸€èŒè´£**: æ¯ä¸ªJobåªåšä¸€ä»¶äº‹æƒ…
- **å¯é‡è¯•**: è®¾è®¡ä¸ºå¹‚ç­‰æ“ä½œ
- **å¯ç›‘æ§**: æ·»åŠ è¶³å¤Ÿçš„æ—¥å¿—å’ŒæŒ‡æ ‡
- **å¯é…ç½®**: æ”¯æŒè¿è¡Œæ—¶å‚æ•°é…ç½®

### 2. Pipelineç»„ç»‡åŸåˆ™
- **é€»è¾‘åˆ†ç»„**: æŒ‰ä¸šåŠ¡é€»è¾‘ç»„ç»‡æ“ä½œ
- **ä¾èµ–æ¸…æ™°**: æ˜ç¡®æ•°æ®æµå‘
- **é”™è¯¯éš”ç¦»**: å¤±è´¥ä¸å½±å“å…¶ä»–åˆ†æ”¯
- **èµ„æºä¼˜åŒ–**: åˆç†åˆ†é…è®¡ç®—èµ„æº

### 3. è°ƒåº¦ç­–ç•¥å»ºè®®
- **æ—¶é—´è°ƒåº¦**: åŸºäºä¸šåŠ¡æ—¶é—´å®‰æ’
- **ä¾èµ–è°ƒåº¦**: Jobé—´ä¾èµ–è§¦å‘
- **äº‹ä»¶è°ƒåº¦**: åŸºäºå¤–éƒ¨äº‹ä»¶è§¦å‘
- **æ‰‹åŠ¨è°ƒåº¦**: æ”¯æŒäººå·¥å¹²é¢„

### 4. èµ„æºç®¡ç†
- **å¹¶è¡Œæ‰§è¡Œ**: åˆç†åˆ©ç”¨å¤šæ ¸CPU
- **å†…å­˜ç®¡ç†**: ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
- **å­˜å‚¨ä¼˜åŒ–**: åˆç†ä½¿ç”¨ç¼“å­˜å’Œä¸´æ—¶å­˜å‚¨
- **ç½‘ç»œä¼˜åŒ–**: æ‰¹é‡APIè°ƒç”¨å‡å°‘å»¶è¿Ÿ

## ğŸ”§ å®ç°ç¤ºä¾‹

åŸºäºå½“å‰æ¶æ„ï¼Œæˆ‘å»ºè®®é‡‡ç”¨ä»¥ä¸‹å®ç°æ–¹æ¡ˆï¼š

```python
# åœ¨job_definitions.pyä¸­æ·»åŠ æ–°çš„Jobå®šä¹‰
@job(name="daily_etl_pipeline")
def daily_etl_pipeline():
    """æ¯æ—¥ETLç®¡é“"""
    # 1. æ•°æ®æå– (Extract)
    raw_data = extract_data_op()

    # 2. æ•°æ®è½¬æ¢ (Transform)
    cleaned_data = clean_data_op(raw_data)
    enriched_data = enrich_data_op(cleaned_data)

    # 3. æ•°æ®åŠ è½½ (Load)
    load_to_storage_op(enriched_data)

@job(name="realtime_processing")
def realtime_processing_job():
    """å®æ—¶å¤„ç†Job"""
    # å®æ—¶æ•°æ®æµå¤„ç†
    stream_data = consume_stream_op()
    processed_data = process_realtime_op(stream_data)
    save_realtime_op(processed_data)
```

è¿™ä¸ªè®¾è®¡æ—¢ä¿æŒäº†Jobçš„ç‹¬ç«‹æ€§ï¼Œåˆé€šè¿‡Pipelineæä¾›äº†ç«¯åˆ°ç«¯çš„å¤„ç†èƒ½åŠ›ã€‚</content>
<parameter name="filePath">c:\Users\qxia1\Desktop\äº¤æ˜“\é¡¹ç›®ä»£ç \stockmonitor\JOB_PIPELINE_GUIDE.md