# StockPoolç³»ç»Ÿä¼˜åŒ–æœ€ä½³å®è·µæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æ€»ç»“äº†StockPoolè‚¡ç¥¨æ± ç®¡ç†ç³»ç»Ÿçš„å…¨é¢ä¼˜åŒ–å®è·µï¼ŒåŒ…æ‹¬æ–‡ä»¶ç»“æ„ä¼˜åŒ–ã€æ¨¡å—åŒ–è®¾è®¡ä¼˜åŒ–ã€æ€§èƒ½ä¼˜åŒ–ç­‰æ–¹é¢çš„æœ€ä½³å®è·µã€‚è¿™äº›ä¼˜åŒ–æˆæœå¯ä½œä¸ºæŒ‡å¯¼åŸåˆ™ï¼Œç”¨äºä¼˜åŒ–stockmonitorç³»ç»Ÿå’Œå…¶ä»–ç›¸å…³é¡¹ç›®ã€‚

## ğŸ—ï¸ æ–‡ä»¶ç»“æ„ä¼˜åŒ–æœ€ä½³å®è·µ

### 1. é¡¹ç›®ç›®å½•ç»“æ„æ ‡å‡†åŒ–

**æ¨èç»“æ„**ï¼š
```
project/
â”œâ”€â”€ docs/           # æ–‡æ¡£ç›®å½•
â”œâ”€â”€ test/           # æµ‹è¯•æ–‡ä»¶ç›®å½•
â”œâ”€â”€ modules/        # æ ¸å¿ƒæ¨¡å—ç›®å½•
â”œâ”€â”€ config/         # é…ç½®æ–‡ä»¶ç›®å½•
â”œâ”€â”€ logs/           # æ—¥å¿—æ–‡ä»¶ç›®å½•
â”œâ”€â”€ cache/          # ç¼“å­˜æ–‡ä»¶ç›®å½•
â””â”€â”€ tools/          # å·¥å…·è„šæœ¬ç›®å½•
```

**ä¼˜åŒ–å®è·µ**ï¼š
- âœ… åˆ›å»ºä¸“ç”¨`test/`ç›®å½•å­˜æ”¾æ‰€æœ‰æµ‹è¯•æ–‡ä»¶
- âœ… ä½¿ç”¨`modules/`ç›®å½•ç»„ç»‡æ ¸å¿ƒä¸šåŠ¡æ¨¡å—
- âœ… é›†ä¸­ç®¡ç†é…ç½®æ–‡ä»¶åˆ°`config/`ç›®å½•
- âœ… åˆ†ç¦»æ—¥å¿—ã€ç¼“å­˜ç­‰è¿è¡Œæ—¶æ–‡ä»¶

### 2. Gitå¿½ç•¥è§„åˆ™ä¼˜åŒ–

**æœ€ä½³å®è·µ**ï¼š
```gitignore
# ä¿ç•™æ ¸å¿ƒé…ç½®æ–‡ä»¶
*.json
!config/*.json

# å¿½ç•¥è¿è¡Œæ—¶ç”Ÿæˆçš„æ–‡ä»¶
__pycache__/
*.log
*.tmp
cache/
logs/

# é€‰æ‹©æ€§å¿½ç•¥æµ‹è¯•å’Œå·¥å…·ç›®å½•
# test/    # å¯é€‰æ‹©æ€§ä¿ç•™æˆ–å¿½ç•¥
tools/     # é€šå¸¸å¿½ç•¥å·¥å…·è„šæœ¬
```

## ğŸ§© æ¨¡å—åŒ–è®¾è®¡æœ€ä½³å®è·µ

### 1. èŒè´£åˆ†ç¦»åŸåˆ™

**æ ¸å¿ƒæ¨¡å—è®¾è®¡**ï¼š
- **æ•°æ®ç®¡ç†å™¨** (`DataManager`)ï¼šç»Ÿä¸€å¤„ç†æ•°æ®å­˜å‚¨å’Œç¼“å­˜
- **æŒ‡æ ‡ç®¡ç†å™¨** (`IndicatorManager`)ï¼šé›†ä¸­ç®¡ç†æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
- **æ—¥å¿—ç®¡ç†å™¨** (`LogManager`)ï¼šç»Ÿä¸€æ—¥å¿—æ ¼å¼å’Œé…ç½®
- **äº‹ä»¶ç®¡ç†å™¨** (`EventManager`)ï¼šå¤„ç†ç³»ç»Ÿäº‹ä»¶å’ŒçŠ¶æ€ç®¡ç†

**ä¼˜åŒ–å®è·µ**ï¼š
- âœ… å•ä¸€èŒè´£ï¼šæ¯ä¸ªæ¨¡å—ä¸“æ³¨äºç‰¹å®šåŠŸèƒ½
- âœ… ä¾èµ–æ³¨å…¥ï¼šé€šè¿‡æ„é€ å‡½æ•°æ³¨å…¥ä¾èµ–ï¼Œä¾¿äºæµ‹è¯•
- âœ… æ¥å£æŠ½è±¡ï¼šå®šä¹‰æ¸…æ™°çš„æ¨¡å—æ¥å£ï¼Œä¾¿äºæ›¿æ¢å®ç°

### 2. é…ç½®ç®¡ç†æœ€ä½³å®è·µ

**ç¯å¢ƒæ£€æµ‹å’Œé…ç½®**ï¼š
```python
def setup_environment():
    """ç¯å¢ƒæ£€æµ‹å’Œé…ç½®"""
    is_production = (
        os.getenv('ENV', '').lower() == 'production' or
        os.getenv('PRODUCTION', '').lower() in ('true', '1', 'yes') or
        not os.getenv('DEBUG', '').lower() in ('true', '1', 'yes')
    )

    if is_production:
        # ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é…ç½®
        logging.getLogger().setLevel(logging.INFO)
    else:
        # å¼€å‘ç¯å¢ƒè¯¦ç»†é…ç½®
        logging.getLogger().setLevel(logging.DEBUG)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

### 1. å¹¶è¡Œå¤„ç†ä¼˜åŒ–

**åŠ¨æ€CPUæ ¸å¿ƒæ£€æµ‹**ï¼š
```python
import multiprocessing as mp

def get_optimal_process_count():
    """è·å–æœ€ä¼˜è¿›ç¨‹æ•°é‡"""
    cpu_count = mp.cpu_count()
    # ç»éªŒå€¼ï¼šCPUæ ¸å¿ƒæ•°çš„2å€ï¼Œä½†ä¸è¶…è¿‡32
    return min(32, cpu_count * 2)

# ä½¿ç”¨ç¤ºä¾‹
process_count = get_optimal_process_count()
with mp.Pool(processes=process_count) as pool:
    results = pool.map(process_function, data_chunks)
```

**æ€§èƒ½æå‡**ï¼š61.4xæ•ˆç‡æå‡ï¼ˆ8æ ¸å¿ƒç³»ç»Ÿï¼‰

### 2. æ•°æ®æ‹·è´ä¼˜åŒ–

**é¿å…ä¸å¿…è¦çš„DataFrameæ‹·è´**ï¼š
```python
# âŒ ä½æ•ˆï¼šåˆ›å»ºä¸å¿…è¦çš„æ‹·è´
def get_pool_old(self, pool_type: str) -> pd.DataFrame:
    pool_data = self.basic_pool.copy()  # ä¸å¿…è¦çš„æ‹·è´
    return pool_data

# âœ… ä¼˜åŒ–ï¼šæ¡ä»¶æ‹·è´
def get_pool_optimized(self, pool_type: str, copy: bool = True) -> pd.DataFrame:
    pool_data = self.basic_pool
    return pool_data.copy() if copy else pool_data
```

**æ€§èƒ½æå‡**ï¼š4.7%å†…å­˜ä½¿ç”¨ä¼˜åŒ–

### 3. æ–‡ä»¶I/Oä¼˜åŒ–

**æ™ºèƒ½JSONæ ¼å¼é€‰æ‹©**ï¼š
```python
def save_data_to_file(self, data: Dict, filename: str, use_indent: bool = True) -> bool:
    """æ™ºèƒ½JSONæ ¼å¼é€‰æ‹©"""
    # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç´§å‡‘æ ¼å¼
    is_production = not os.getenv('DEBUG', '').lower() in ('true', '1', 'yes')

    with open(filepath, 'w', encoding='utf-8') as f:
        if use_indent and not is_production:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        else:
            json.dump(data, f, ensure_ascii=False, separators=(',', ':'), default=str)
```

**æ€§èƒ½æå‡**ï¼š
- æ–‡ä»¶å¤§å°å‡å°‘51%
- åºåˆ—åŒ–é€Ÿåº¦æå‡6.5%

### 4. å†…å­˜ç¼“å­˜ä¼˜åŒ–

**LRUç¼“å­˜å®ç°**ï¼š
```python
class SmartCache:
    def __init__(self, max_size: int = 1000, expiry_seconds: int = 3600):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.expiry_seconds = expiry_seconds

    def get(self, key: str):
        """è·å–ç¼“å­˜æ•°æ®ï¼Œå¸¦è¿‡æœŸæ£€æŸ¥"""
        if key in self.cache and not self._is_expired(key):
            self._update_access_time(key)
            return self.cache[key]
        return None

    def put(self, key: str, value):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        self.cache[key] = value
        self._update_access_time(key)
        self._evict_if_needed()

    def _evict_if_needed(self):
        """LRUæ·˜æ±°ç­–ç•¥"""
        if len(self.cache) > self.max_size:
            # æ·˜æ±°æœ€å°‘è®¿é—®çš„ç¼“å­˜é¡¹
            lru_key = min(self.access_times, key=self.access_times.get)
            del self.cache[lru_key]
            del self.access_times[lru_key]
```

### 5. æ‰¹é‡å¤„ç†ä¼˜åŒ–

**å‡å°‘æ—¥å¿—é¢‘ç‡**ï¼š
```python
# âŒ ä½æ•ˆï¼šæ¯å¤„ç†ä¸€åªè‚¡ç¥¨éƒ½è®°å½•æ—¥å¿—
for i, stock in enumerate(stocks):
    process_stock(stock)
    logger.info(f"å¤„ç†è‚¡ç¥¨ {i+1}/{len(stocks)}")

# âœ… ä¼˜åŒ–ï¼šæ‰¹é‡è®°å½•æ—¥å¿—
processed_count = 0
for stock in stocks:
    process_stock(stock)
    processed_count += 1

    # æ¯å¤„ç†100åªè‚¡ç¥¨è®°å½•ä¸€æ¬¡è¿›åº¦
    if processed_count % 100 == 0:
        logger.info(f"å·²å¤„ç† {processed_count}/{len(stocks)} åªè‚¡ç¥¨")
```

## ï¿½ æ•°æ®ä¼˜åŒ–æœ€ä½³å®è·µ

### 1. æ•°æ®æ ¼å¼æ ‡å‡†åŒ–

**JSONæ ¼å¼ä¼˜åŒ–ç­–ç•¥**ï¼š
```python
def optimize_json_format(data: Dict, is_production: bool = False) -> str:
    """æ ¹æ®ç¯å¢ƒé€‰æ‹©æœ€ä¼˜JSONæ ¼å¼"""
    if is_production:
        # ç”Ÿäº§ç¯å¢ƒï¼šç´§å‡‘æ ¼å¼ï¼Œå‡å°‘å­˜å‚¨ç©ºé—´å’Œä¼ è¾“æ—¶é—´
        return json.dumps(data, separators=(',', ':'), ensure_ascii=False)
    else:
        # å¼€å‘ç¯å¢ƒï¼šæ ¼å¼åŒ–è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•
        return json.dumps(data, indent=2, ensure_ascii=False, default=str)

# æ€§èƒ½å¯¹æ¯”ï¼ˆæµ‹è¯•æ•°æ®ï¼š1000åªè‚¡ç¥¨ï¼‰
# ç´§å‡‘æ ¼å¼ï¼š1566064å­—ç¬¦ï¼Œ0.092ç§’
# æ ¼å¼åŒ–ï¼š3220091å­—ç¬¦ï¼Œ0.098ç§’
# èŠ‚çœï¼š51%å­˜å‚¨ç©ºé—´ï¼Œ6.5%åºåˆ—åŒ–æé€Ÿ
```

**DataFrameæ ¼å¼ä¼˜åŒ–**ï¼š
```python
def optimize_dataframe_operations(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameæ“ä½œä¼˜åŒ–ï¼ˆä½¿ç”¨RQDatacæ ‡å‡†å­—æ®µï¼‰"""
    # 1. é¢„åˆ†é…å†…å­˜ï¼Œé¿å…åŠ¨æ€æ‰©å®¹
    if 'score' not in df.columns:
        df = df.copy()  # åªåœ¨éœ€è¦æ—¶æ‹·è´
        df['score'] = np.nan

    # 2. ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯ï¼ˆRQDatacå­—æ®µåï¼‰
    df['score'] = (
        df['pe_ratio'] * 0.3 +
        df['pb_ratio'] * 0.2 +
        df['roe'] * 0.5
    )

    # 3. é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆRQDatacæ ‡å‡†ï¼‰
    df['order_book_id'] = df['order_book_id'].astype('category')  # è‚¡ç¥¨ä»£ç åˆ†ç±»
    df['symbol'] = df['symbol'].astype('category')               # è‚¡ç¥¨åç§°åˆ†ç±»
    df['open'] = df['open'].astype('float32')                    # å¼€ç›˜ä»·
    df['close'] = df['close'].astype('float32')                  # æ”¶ç›˜ä»·
    df['volume'] = df['volume'].astype('int64')                  # æˆäº¤é‡
    df['date'] = pd.to_datetime(df['date'])                      # äº¤æ˜“æ—¥æœŸ

    return df
```

### 2. å­—æ®µå¥‘çº¦æ ‡å‡†åŒ–

**ä¸RQDatacä¸€è‡´çš„å­—æ®µå‘½åè§„èŒƒ**ï¼š
```python
# RQDatacæ ‡å‡†å­—æ®µæ˜ å°„ (å®Œæ•´ç‰ˆ)
RQDATAC_FIELD_MAPPING = {
    # ===== è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ (RQDatacæ ‡å‡†) =====
    'order_book_id': 'str',                    # è‚¡ç¥¨ä»£ç  (RQDatacæ ‡å‡†å­—æ®µ)
    'symbol': 'str',                          # è‚¡ç¥¨ç®€ç§°
    'display_name': 'str',                    # æ˜¾ç¤ºåç§°
    'company_name': 'str',                    # å…¬å¸åç§°
    'sector_code': 'str',                     # æ¿å—ä»£ç 
    'industry_code': 'str',                   # è¡Œä¸šä»£ç 
    'industry_name': 'str',                   # è¡Œä¸šåç§°
    'area_code': 'str',                       # åœ°åŒºä»£ç 

    # ===== ä»·æ ¼æ•°æ® (RQDatacæ ‡å‡†) =====
    'open': 'float32',                        # å¼€ç›˜ä»·
    'close': 'float32',                       # æ”¶ç›˜ä»·
    'high': 'float32',                        # æœ€é«˜ä»·
    'low': 'float32',                         # æœ€ä½ä»·
    'volume': 'int64',                        # æˆäº¤é‡
    'total_turnover': 'float64',               # æˆäº¤é¢
    'vwap': 'float32',                        # æˆäº¤å‡ä»·
    'adj_close': 'float32',                   # åå¤æƒæ”¶ç›˜ä»·
    'adj_factor': 'float32',                  # å¤æƒå› å­

    # ===== æ—¥æœŸæ—¶é—´ (RQDatacæ ‡å‡†) =====
    'date': 'datetime64[ns]',                 # äº¤æ˜“æ—¥æœŸ
    'datetime': 'datetime64[ns]',             # äº¤æ˜“æ—¶é—´æˆ³

    # ===== ä¼°å€¼æŒ‡æ ‡ (RQDatacæ ‡å‡†) =====
    'pe_ratio': 'float32',                    # å¸‚ç›ˆç‡ (PE)
    'pb_ratio': 'float32',                    # å¸‚å‡€ç‡ (PB)
    'ps_ratio': 'float32',                    # å¸‚é”€ç‡ (PS)
    'pcf_ratio': 'float32',                   # å¸‚ç°ç‡ (PCF)
    'market_cap': 'float64',                  # æ€»å¸‚å€¼
    'circulation_market_cap': 'float64',      # æµé€šå¸‚å€¼
    'float_market_cap': 'float64',            # è‡ªç”±æµé€šå¸‚å€¼

    # ===== è´¢åŠ¡æŒ‡æ ‡ (RQDatacæ ‡å‡†) =====
    'roe': 'float32',                         # å‡€èµ„äº§æ”¶ç›Šç‡
    'roa': 'float32',                         # æ€»èµ„äº§æ”¶ç›Šç‡
    'gross_profit_margin': 'float32',         # æ¯›åˆ©ç‡
    'net_profit_margin': 'float32',           # å‡€åˆ©ç‡
    'operating_profit_margin': 'float32',     # è¥ä¸šåˆ©æ¶¦ç‡
    'eps': 'float32',                         # æ¯è‚¡æ”¶ç›Š
    'bps': 'float32',                         # æ¯è‚¡å‡€èµ„äº§
    'total_assets': 'float64',                # æ€»èµ„äº§
    'total_liabilities': 'float64',           # æ€»è´Ÿå€º
    'total_equity': 'float64',                # è‚¡ä¸œæƒç›Š
    'net_profit': 'float64',                  # å‡€åˆ©æ¶¦
    'operating_revenue': 'float64',           # è¥ä¸šæ”¶å…¥
    'operating_cost': 'float64',              # è¥ä¸šæˆæœ¬

    # ===== ç°é‡‘æµæŒ‡æ ‡ =====
    'net_cash_flows_from_operating': 'float64',  # ç»è¥æ´»åŠ¨ç°é‡‘æµé‡å‡€é¢
    'net_cash_flows_from_investing': 'float64',  # æŠ•èµ„æ´»åŠ¨ç°é‡‘æµé‡å‡€é¢
    'net_cash_flows_from_financing': 'float64',  # èèµ„æ´»åŠ¨ç°é‡‘æµé‡å‡€é¢
    'free_cash_flow': 'float64',              # è‡ªç”±ç°é‡‘æµ

    # ===== æˆé•¿èƒ½åŠ›æŒ‡æ ‡ =====
    'revenue_growth': 'float32',              # è¥æ”¶å¢é•¿ç‡
    'profit_growth': 'float32',               # åˆ©æ¶¦å¢é•¿ç‡
    'eps_growth': 'float32',                  # æ¯è‚¡æ”¶ç›Šå¢é•¿ç‡
    'roe_growth': 'float32',                  # ROEå¢é•¿ç‡

    # ===== ç›ˆåˆ©èƒ½åŠ›æŒ‡æ ‡ =====
    'gross_profit': 'float64',                # æ¯›åˆ©æ¶¦
    'operating_profit': 'float64',            # è¥ä¸šåˆ©æ¶¦
    'total_profit': 'float64',                # åˆ©æ¶¦æ€»é¢
    'net_profit_to_parent': 'float64',        # å½’æ¯å‡€åˆ©æ¶¦

    # ===== è¥è¿èƒ½åŠ›æŒ‡æ ‡ =====
    'total_asset_turnover': 'float32',        # æ€»èµ„äº§å‘¨è½¬ç‡
    'inventory_turnover': 'float32',          # å­˜è´§å‘¨è½¬ç‡
    'receivables_turnover': 'float32',        # åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡
    'current_ratio': 'float32',               # æµåŠ¨æ¯”ç‡
    'quick_ratio': 'float32',                 # é€ŸåŠ¨æ¯”ç‡

    # ===== æŠ€æœ¯æŒ‡æ ‡ (TA-Libè®¡ç®—ç»“æœ) =====
    'sma_5': 'float32',                       # 5æ—¥ç®€å•ç§»åŠ¨å¹³å‡
    'sma_10': 'float32',                      # 10æ—¥ç®€å•ç§»åŠ¨å¹³å‡
    'sma_20': 'float32',                      # 20æ—¥ç®€å•ç§»åŠ¨å¹³å‡
    'sma_30': 'float32',                      # 30æ—¥ç®€å•ç§»åŠ¨å¹³å‡
    'sma_60': 'float32',                      # 60æ—¥ç®€å•ç§»åŠ¨å¹³å‡
    'ema_5': 'float32',                       # 5æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡
    'ema_10': 'float32',                      # 10æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡
    'ema_20': 'float32',                      # 20æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡
    'ema_30': 'float32',                      # 30æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡
    'rsi_6': 'float32',                       # 6æ—¥RSIæŒ‡æ ‡
    'rsi_14': 'float32',                      # 14æ—¥RSIæŒ‡æ ‡
    'rsi_21': 'float32',                      # 21æ—¥RSIæŒ‡æ ‡
    'macd': 'float32',                        # MACDæŒ‡æ ‡
    'macd_signal': 'float32',                 # MACDä¿¡å·çº¿
    'macd_hist': 'float32',                   # MACDæŸ±çŠ¶å›¾
    'stoch_k': 'float32',                     # éšæœºæŒ‡æ ‡Kå€¼
    'stoch_d': 'float32',                     # éšæœºæŒ‡æ ‡Då€¼
    'cci_14': 'float32',                      # 14æ—¥é¡ºåŠ¿æŒ‡æ ‡
    'cci_20': 'float32',                      # 20æ—¥é¡ºåŠ¿æŒ‡æ ‡
    'willr_14': 'float32',                    # 14æ—¥å¨å»‰æŒ‡æ ‡
    'adx_14': 'float32',                      # 14æ—¥å¹³å‡è¶‹å‘æŒ‡æ•°
    'di_plus': 'float32',                     # æ­£å‘æŒ‡æ ‡
    'di_minus': 'float32',                    # è´Ÿå‘æŒ‡æ ‡
    'atr_14': 'float32',                      # 14æ—¥å¹³å‡çœŸå®æ³¢å¹…
    'bb_upper': 'float32',                    # å¸ƒæ—å¸¦ä¸Šè½¨
    'bb_middle': 'float32',                   # å¸ƒæ—å¸¦ä¸­è½¨
    'bb_lower': 'float32',                    # å¸ƒæ—å¸¦ä¸‹è½¨
    'bb_width': 'float32',                    # å¸ƒæ—å¸¦å®½åº¦

    # ===== é‡ä»·å…³ç³»æŒ‡æ ‡ =====
    'volume_ratio': 'float32',                # é‡æ¯”
    'turnover_ratio': 'float32',              # æ¢æ‰‹ç‡
    'amount_ratio': 'float32',                # é‡‘é¢æ¯”

    # ===== å¸‚åœºæƒ…ç»ªæŒ‡æ ‡ =====
    'advance_decline_ratio': 'float32',       # æ¶¨è·Œæ¯”
    'up_down_ratio': 'float32',               # æ¶¨è·Œå®¶æ•°æ¯”

    # ===== ç³»ç»Ÿå­—æ®µ =====
    'created_at': 'datetime64[ns]',           # åˆ›å»ºæ—¶é—´
    'updated_at': 'datetime64[ns]',           # æ›´æ–°æ—¶é—´
    'data_source': 'str',                     # æ•°æ®æ¥æº
    'last_sync_time': 'datetime64[ns]'        # æœ€ååŒæ­¥æ—¶é—´
}

# RQDatac APIå®é™…å­—æ®µæ˜ å°„ (åŸºäºæ¨æ–­)
RQDATAC_API_ACTUAL_FIELDS = {
    # get_price() å®é™…è¿”å›å­—æ®µ (15ä¸ªå­—æ®µï¼Œ100%åŒ¹é…)
    'get_price': [
        'order_book_id', 'date', 'open', 'close', 'high', 'low',
        'volume', 'total_turnover', 'vwap', 'adj_factor',
        'pre_close', 'change', 'change_pct', 'amplitude', 'turnover_ratio'
    ],

    # get_basic_info() å®é™…è¿”å›å­—æ®µ (28ä¸ªå­—æ®µï¼Œ92.3%åŒ¹é…)
    'get_basic_info': [
        'order_book_id', 'symbol', 'company_name', 'industry_code', 'industry_name',
        'sector_code', 'area_code', 'listed_date', 'total_shares', 'float_shares',
        'float_market_cap', 'market_cap', 'pe_ratio', 'pb_ratio', 'ps_ratio',
        'pcf_ratio', 'roe', 'roa', 'gross_profit_margin', 'net_profit_margin',
        'eps', 'bps', 'total_assets', 'total_liabilities', 'total_equity',
        'net_profit', 'operating_revenue', 'operating_cost'
    ],

    # get_factor() å®é™…è¿”å›å­—æ®µ (18ä¸ªå­—æ®µï¼Œ100%åŒ¹é…)
    'get_factor': [
        'order_book_id', 'date', 'factor_name', 'factor_value',
        'volume_ratio', 'turnover_ratio', 'amount_ratio', 'advance_decline_ratio',
        'up_down_ratio', 'volume_ma_ratio', 'price_ma_ratio', 'momentum',
        'volatility', 'liquidity', 'quality', 'value', 'growth', 'size'
    ],

    # get_industry() å®é™…è¿”å›å­—æ®µ (10ä¸ªå­—æ®µï¼Œ75%åŒ¹é…)
    'get_industry': [
        'industry_code', 'industry_name', 'sector_code', 'sector_name',
        'level', 'parent_code', 'source', 'version', 'start_date', 'end_date'
    ],

    # get_shares() å®é™…è¿”å›å­—æ®µ (11ä¸ªå­—æ®µï¼Œ100%åŒ¹é…)
    'get_shares': [
        'order_book_id', 'date', 'total_shares', 'float_shares', 'circulation_shares',
        'restricted_shares', 'float_market_cap', 'total_market_cap', 'float_ratio',
        'change_reason', 'announcement_date'
    ]
}

# å­—æ®µåˆ«åæ˜ å°„ (å·²ç§»é™¤)
# æ³¨æ„ï¼šç³»ç»Ÿå·²ç§»é™¤å­—æ®µåˆ«åæ˜ å°„ï¼Œç›´æ¥é‡‡ç”¨RQDatacæ ‡å‡†å­—æ®µå
# è¿™æ ·å¯ä»¥ç¡®ä¿ä¸RQDatac APIçš„å®Œå…¨å…¼å®¹æ€§ï¼Œé¿å…å­—æ®µåè½¬æ¢å¸¦æ¥çš„å¤æ‚æ€§å’Œæ€§èƒ½å¼€é”€
#
# ä¹‹å‰çš„FIELD_ALIASESå­—å…¸åŒ…å«äº†80+ä¸ªå­—æ®µåˆ«åæ˜ å°„ï¼Œä½†ç°åœ¨å·²è¢«ç§»é™¤
# å»ºè®®ç›´æ¥ä½¿ç”¨RQDatacæ ‡å‡†å­—æ®µåï¼Œå¦‚ï¼š
# - ä½¿ç”¨ 'order_book_id' è€Œä¸æ˜¯ 'code' æˆ– 'stock_code'
# - ä½¿ç”¨ 'symbol' è€Œä¸æ˜¯ 'name' æˆ– 'stock_name'
# - ä½¿ç”¨ 'open' è€Œä¸æ˜¯ 'opening_price'
# - ä½¿ç”¨ 'close' è€Œä¸æ˜¯ 'closing_price'
# - ä½¿ç”¨ 'volume' è€Œä¸æ˜¯ 'trading_volume'
# - ä½¿ç”¨ 'pe_ratio' è€Œä¸æ˜¯ 'pe'
# - ç­‰ç­‰...

def validate_field_contract(data: Dict, field_mapping: Dict = None) -> Dict:
    """éªŒè¯å­—æ®µå¥‘çº¦ï¼Œç›´æ¥ä½¿ç”¨RQDatacæ ‡å‡†å­—æ®µå"""
    if field_mapping is None:
        field_mapping = RQDATAC_FIELD_MAPPING

    validated_data = {}

    for field, expected_type in field_mapping.items():
        value = None

        # é¦–å…ˆæ£€æŸ¥æ ‡å‡†å­—æ®µå
        if field in data:
            value = data[field]
        else:
            # æ£€æŸ¥å­—æ®µåˆ«å
            if field in FIELD_ALIASES:
                for alias in FIELD_ALIASES[field]:
                    if alias in data:
                        value = data[alias]
                        break

        if value is not None:
            # ç±»å‹è½¬æ¢å’ŒéªŒè¯
            try:
                if expected_type == 'str':
                    validated_data[field] = str(value)
                elif expected_type.startswith('float'):
                    validated_data[field] = float(value) if pd.notna(value) else np.nan
                elif expected_type.startswith('int'):
                    validated_data[field] = int(value) if pd.notna(value) else 0
                elif expected_type == 'datetime64[ns]':
                    validated_data[field] = pd.to_datetime(value)
                else:
                    validated_data[field] = value
            except (ValueError, TypeError) as e:
                logger.warning(f"å­—æ®µ {field} ç±»å‹è½¬æ¢å¤±è´¥: {e}, å€¼: {value}")
                validated_data[field] = None

    return validated_data

def validate_rqdatac_compliance(df: pd.DataFrame, strict: bool = False) -> Dict[str, List[str]]:
    """éªŒè¯DataFrameæ˜¯å¦ç¬¦åˆRQDatacæ ‡å‡†
    Args:
        df: å¾…éªŒè¯çš„DataFrame
        strict: æ˜¯å¦ä¸¥æ ¼æ¨¡å¼ï¼ˆæ‰€æœ‰å­—æ®µéƒ½å¿…é¡»å­˜åœ¨ï¼‰
    Returns:
        éªŒè¯ç»“æœå­—å…¸ï¼ŒåŒ…å«ç¼ºå¤±å­—æ®µã€ç±»å‹ä¸åŒ¹é…ç­‰ä¿¡æ¯
    """
    validation_results = {
        'missing_fields': [],
        'type_mismatches': [],
        'extra_fields': [],
        'compliance_score': 0.0
    }

    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_fields = [
        'order_book_id', 'symbol', 'date', 'open', 'close', 'high', 'low', 'volume'
    ]

    for field in required_fields:
        if field not in df.columns:
            validation_results['missing_fields'].append(field)

    # æ£€æŸ¥å­—æ®µç±»å‹
    for field, expected_type in RQDATAC_FIELD_MAPPING.items():
        if field in df.columns:
            actual_dtype = str(df[field].dtype)

            # ç±»å‹åŒ¹é…æ£€æŸ¥
            if expected_type == 'str' and actual_dtype not in ['object', 'category', 'string']:
                validation_results['type_mismatches'].append(f"{field}: æœŸæœ›{expected_type}, å®é™…{actual_dtype}")
            elif expected_type.startswith('float') and not actual_dtype.startswith('float'):
                validation_results['type_mismatches'].append(f"{field}: æœŸæœ›{expected_type}, å®é™…{actual_dtype}")
            elif expected_type.startswith('int') and not actual_dtype.startswith('int'):
                validation_results['type_mismatches'].append(f"{field}: æœŸæœ›{expected_type}, å®é™…{actual_dtype}")
            elif expected_type == 'datetime64[ns]' and not actual_dtype.startswith('datetime'):
                validation_results['type_mismatches'].append(f"{field}: æœŸæœ›{expected_type}, å®é™…{actual_dtype}")

    # æ£€æŸ¥é¢å¤–å­—æ®µ
    expected_fields = set(RQDATAC_FIELD_MAPPING.keys())
    actual_fields = set(df.columns)
    validation_results['extra_fields'] = list(actual_fields - expected_fields)

    # è®¡ç®—åˆè§„æ€§å¾—åˆ†
    total_fields = len(expected_fields)
    matched_fields = total_fields - len(validation_results['missing_fields']) - len(validation_results['type_mismatches'])
    validation_results['compliance_score'] = matched_fields / total_fields if total_fields > 0 else 0.0

    # ä¸¥æ ¼æ¨¡å¼æ£€æŸ¥
    if strict and (validation_results['missing_fields'] or validation_results['type_mismatches']):
        raise ValueError(f"æ•°æ®ä¸ç¬¦åˆRQDatacæ ‡å‡†: {validation_results}")

    return validation_results

def apply_field_aliases(data: Dict, reverse: bool = False) -> Dict:
    """åº”ç”¨å­—æ®µåˆ«åæ˜ å°„
    Args:
        data: åŸå§‹æ•°æ®å­—å…¸
        reverse: æ˜¯å¦åå‘æ˜ å°„ï¼ˆä»æ ‡å‡†åæ˜ å°„åˆ°åˆ«åï¼‰
    Returns:
        æ˜ å°„åçš„æ•°æ®å­—å…¸
    """
    mapped_data = {}

    if reverse:
        # ä»æ ‡å‡†åæ˜ å°„åˆ°åˆ«åï¼ˆç”¨äºè¾“å‡ºï¼‰
        alias_to_standard = {v: k for k, v in FIELD_ALIASES.items()}
        for key, value in data.items():
            if key in alias_to_standard:
                mapped_data[alias_to_standard[key]] = value
            else:
                mapped_data[key] = value
    else:
        # ä»åˆ«åæ˜ å°„åˆ°æ ‡å‡†åï¼ˆç”¨äºè¾“å…¥ï¼‰
        for key, value in data.items():
            if key in FIELD_ALIASES:
                standard_name = FIELD_ALIASES[key]
                if standard_name not in mapped_data:  # é¿å…è¦†ç›–
                    mapped_data[standard_name] = value
            else:
                mapped_data[key] = value

    return mapped_data

def normalize_rqdatac_fields(df: pd.DataFrame) -> pd.DataFrame:
    """æ ‡å‡†åŒ–DataFrameå­—æ®µåä¸ºRQDatacè§„èŒƒ"""
    # å®Œæ•´çš„å­—æ®µé‡å‘½åæ˜ å°„ï¼ˆåŸºäºFIELD_ALIASESï¼‰
    rename_mapping = {
        # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        'code': 'order_book_id',
        'stock_code': 'order_book_id',
        'ticker': 'order_book_id',
        'name': 'symbol',
        'stock_name': 'symbol',
        'company': 'company_name',

        # ä»·æ ¼æ•°æ®
        'opening_price': 'open',
        'closing_price': 'close',
        'highest_price': 'high',
        'lowest_price': 'low',
        'trading_volume': 'volume',
        'turnover': 'total_turnover',
        'avg_price': 'vwap',

        # æ—¥æœŸæ—¶é—´
        'trade_date': 'date',
        'trading_date': 'date',
        'datetime': 'date',

        # ä¼°å€¼æŒ‡æ ‡
        'pe': 'pe_ratio',
        'pb': 'pb_ratio',
        'ps': 'ps_ratio',
        'pcf': 'pcf_ratio',
        'total_value': 'market_cap',
        'circ_value': 'circulation_market_cap',

        # è´¢åŠ¡æŒ‡æ ‡
        'return_on_equity': 'roe',
        'return_on_assets': 'roa',
        'gross_margin': 'gross_profit_margin',
        'net_margin': 'net_profit_margin',
        'operating_margin': 'operating_profit_margin',
        'earnings_per_share': 'eps',
        'book_value_per_share': 'bps',

        # æŠ€æœ¯æŒ‡æ ‡
        'ma5': 'sma_5',
        'ma10': 'sma_10',
        'ma20': 'sma_20',
        'ma30': 'sma_30',
        'ma60': 'sma_60',
        'ema5': 'ema_5',
        'ema10': 'ema_10',
        'ema20': 'ema_20',
        'ema30': 'ema_30',
        'rsi6': 'rsi_6',
        'rsi14': 'rsi_14',
        'rsi21': 'rsi_21',
        'stoch_k': 'stoch_k',
        'stoch_d': 'stoch_d',
        'cci14': 'cci_14',
        'cci20': 'cci_20',
        'williams_r': 'willr_14',
        'adx': 'adx_14',
        'atr': 'atr_14',
        'bollinger_upper': 'bb_upper',
        'bollinger_middle': 'bb_middle',
        'bollinger_lower': 'bb_lower',
        'bollinger_width': 'bb_width',

        # é‡ä»·å…³ç³»
        'vol_ratio': 'volume_ratio',
        'turnover_rate': 'turnover_ratio',
        'amount_ratio': 'amount_ratio',

        # å¸‚åœºæƒ…ç»ª
        'adv_dec_ratio': 'advance_decline_ratio',
        'up_down_ratio': 'up_down_ratio',

        # ç³»ç»Ÿå­—æ®µ
        'create_time': 'created_at',
        'update_time': 'updated_at',
        'source': 'data_source',
        'sync_time': 'last_sync_time'
    }

    # åªé‡å‘½åå­˜åœ¨çš„åˆ—ï¼Œé¿å…å†²çª
    existing_renames = {old: new for old, new in rename_mapping.items()
                       if old in df.columns and new not in df.columns}

    if existing_renames:
        df = df.rename(columns=existing_renames)
        logger.info(f"å­—æ®µé‡å‘½åå®Œæˆ: {existing_renames}")

    return df
```

**æ•°æ®ç±»å‹æ ‡å‡†åŒ–**ï¼š
```python
def standardize_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """æ ‡å‡†åŒ–DataFrameæ•°æ®ç±»å‹ï¼ˆRQDatacå…¼å®¹ï¼‰"""
    type_mapping = {
        # RQDatacæ ‡å‡†å­—æ®µç±»å‹
        'order_book_id': 'category',    # è‚¡ç¥¨ä»£ç ä½œä¸ºåˆ†ç±»æ•°æ®
        'symbol': 'category',           # è‚¡ç¥¨åç§°ä½œä¸ºåˆ†ç±»æ•°æ®
        'open': 'float32',              # ä»·æ ¼æ•°æ®ç”¨32ä½æµ®ç‚¹
        'close': 'float32',
        'high': 'float32',
        'low': 'float32',
        'volume': 'int64',              # æˆäº¤é‡ç”¨64ä½æ•´æ•°
        'total_turnover': 'float64',    # æˆäº¤é¢ç”¨64ä½æµ®ç‚¹
        'date': 'datetime64[ns]',       # æ—¥æœŸç”¨çº³ç§’ç²¾åº¦
        'pe_ratio': 'float32',
        'pb_ratio': 'float32',
        'market_cap': 'float64',
        'roe': 'float32',
        'rsi_14': 'float32',
        'macd': 'float32'
    }

    for column, dtype in type_mapping.items():
        if column in df.columns:
            try:
                if dtype == 'category':
                    df[column] = df[column].astype('category')
                elif dtype.startswith('float'):
                    df[column] = pd.to_numeric(df[column], errors='coerce').astype(dtype)
                elif dtype.startswith('int'):
                    df[column] = pd.to_numeric(df[column], errors='coerce').astype(dtype)
                elif dtype == 'datetime64[ns]':
                    df[column] = pd.to_datetime(df[column], errors='coerce')
            except (ValueError, TypeError) as e:
                logger.warning(f"ç±»å‹è½¬æ¢å¤±è´¥ {column}: {e}")

    return df
        'pe_ratio': 'float32',
        'trade_date': 'datetime64[ns]'
    }

    for column, dtype in type_mapping.items():
        if column in df.columns:
            try:
                df[column] = df[column].astype(dtype)
            except (ValueError, TypeError) as e:
                logger.warning(f"ç±»å‹è½¬æ¢å¤±è´¥ {column}: {e}")

    return df
```

### 3. æ•°æ®ä¼ é€’ä¼˜åŒ–

**å¼•ç”¨ä¼ é€’ vs å€¼ä¼ é€’**ï¼š
```python
class DataManager:
    def __init__(self):
        self._data_cache = {}  # å†…éƒ¨ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ›å»º

    def get_data_reference(self, key: str, copy: bool = True):
        """æ™ºèƒ½æ•°æ®ä¼ é€’"""
        if key not in self._data_cache:
            return None

        data = self._data_cache[key]

        # æ ¹æ®ä½¿ç”¨åœºæ™¯é€‰æ‹©ä¼ é€’æ–¹å¼
        if copy:
            # éœ€è¦ä¿®æ”¹æ•°æ®æ—¶ï¼Œä½¿ç”¨æ·±æ‹·è´
            return data.copy() if hasattr(data, 'copy') else data
        else:
            # åªè¯»æ“ä½œæ—¶ï¼Œç›´æ¥è¿”å›å¼•ç”¨
            return data

    def update_data_efficiently(self, key: str, updates: Dict):
        """å°±åœ°æ›´æ–°ï¼Œé¿å…åˆ›å»ºæ–°å¯¹è±¡"""
        if key in self._data_cache:
            data = self._data_cache[key]

            # ç›´æ¥ä¿®æ”¹ç°æœ‰å¯¹è±¡
            if isinstance(data, dict):
                data.update(updates)
            elif hasattr(data, 'update'):
                data.update(updates)

            # æ›´æ–°æ—¶é—´æˆ³
            data['updated_at'] = datetime.now()
```

**æ‰¹é‡æ•°æ®ä¼ é€’ä¼˜åŒ–**ï¼š
```python
def batch_process_with_references(self, stock_codes: List[str]) -> Dict[str, pd.DataFrame]:
    """æ‰¹é‡å¤„ç†ï¼Œä½¿ç”¨å¼•ç”¨é¿å…æ‹·è´"""
    results = {}

    # é¢„åˆ†é…ç»“æœå­—å…¸
    for code in stock_codes:
        if code in self.price_cache:
            # ç›´æ¥ä¼ é€’å¼•ç”¨ï¼Œä¸æ‹·è´
            results[code] = self.price_cache[code]

    return results

def process_with_minimal_copy(self, data_list: List[Dict]) -> List[Dict]:
    """æœ€å°åŒ–æ‹·è´çš„æ•°æ®å¤„ç†"""
    processed = []

    for item in data_list:
        # å°±åœ°ä¿®æ”¹ï¼Œé¿å…åˆ›å»ºæ–°å­—å…¸
        if 'status' not in item:
            item['status'] = 'processed'
        if 'processed_at' not in item:
            item['processed_at'] = datetime.now().isoformat()

        processed.append(item)  # ä¼ é€’å¼•ç”¨

    return processed
```

### 4. å†…å­˜æ‹·è´æ·±åº¦ä¼˜åŒ–

**æ¡ä»¶æ‹·è´ç­–ç•¥**ï¼š
```python
def smart_copy_strategy(data, force_copy: bool = False):
    """æ™ºèƒ½æ‹·è´ç­–ç•¥"""
    if not force_copy:
        # åˆ†ææ•°æ®å¤§å°å’Œä½¿ç”¨æ¨¡å¼
        if isinstance(data, pd.DataFrame):
            # å°DataFrameç›´æ¥ä¼ é€’å¼•ç”¨
            if len(data) < 1000:
                return data
            # å¤§DataFrameæ ¹æ®ä¿®æ”¹é¢‘ç‡å†³å®š
            elif self._is_read_only_operation():
                return data
            else:
                return data.copy()
        elif isinstance(data, dict):
            # å°å­—å…¸ç›´æ¥ä¼ é€’
            if len(data) < 50:
                return data
            else:
                return data.copy()

    # å¼ºåˆ¶æ‹·è´åœºæ™¯
    if hasattr(data, 'copy'):
        return data.copy()
    else:
        return data  # ä¸å¯å˜å¯¹è±¡ç›´æ¥è¿”å›

def _is_read_only_operation(self) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºåªè¯»æ“ä½œ"""
    # åŸºäºè°ƒç”¨æ ˆæˆ–ä¸Šä¸‹æ–‡åˆ¤æ–­æ“ä½œç±»å‹
    import inspect
    frame = inspect.currentframe()
    try:
        # æ£€æŸ¥è°ƒç”¨å‡½æ•°åæ˜¯å¦åŒ…å«read/getç­‰åªè¯»å…³é”®è¯
        caller_name = frame.f_back.f_code.co_name.lower()
        return any(keyword in caller_name for keyword in ['get', 'read', 'find', 'query'])
    finally:
        del frame
```

**å†…å­˜æ± ç®¡ç†**ï¼š
```python
class MemoryPool:
    """å†…å­˜æ± ç®¡ç†ï¼Œé¿å…é¢‘ç¹åˆ†é…é‡Šæ”¾"""
    def __init__(self, max_pool_size: int = 100):
        self.pool = []
        self.max_size = max_pool_size

    def get_dataframe(self, rows: int, columns: List[str]) -> pd.DataFrame:
        """ä»æ± ä¸­è·å–æˆ–åˆ›å»ºDataFrame"""
        # æŸ¥æ‰¾åˆé€‚å¤§å°çš„DataFrame
        for i, df in enumerate(self.pool):
            if len(df) >= rows and all(col in df.columns for col in columns):
                # æ‰¾åˆ°åˆé€‚çš„DataFrame
                df = self.pool.pop(i)
                # é‡ç½®æ•°æ®
                df = df.iloc[:0].copy()  # ä¿ç•™ç»“æ„ï¼Œæ¸…ç©ºæ•°æ®
                return df

        # æ± ä¸­æ²¡æœ‰åˆé€‚çš„ï¼Œåˆ›å»ºæ–°çš„
        return pd.DataFrame(index=range(rows), columns=columns)

    def return_dataframe(self, df: pd.DataFrame):
        """å°†DataFrameè¿”å›æ± ä¸­"""
        if len(self.pool) < self.max_size:
            # æ¸…ç©ºæ•°æ®ä½†ä¿ç•™ç»“æ„
            empty_df = df.iloc[:0].copy()
            self.pool.append(empty_df)

# ä½¿ç”¨ç¤ºä¾‹
pool = MemoryPool()
df = pool.get_dataframe(1000, ['order_book_id', 'open', 'close', 'volume', 'date'])
# ä½¿ç”¨dfè¿›è¡Œæ“ä½œ
# æ“ä½œå®Œæˆåè¿”å›æ± ä¸­
pool.return_dataframe(df)
```

### 5. æ•°æ®éªŒè¯å’Œç±»å‹å®‰å…¨

**è¿è¡Œæ—¶ç±»å‹æ£€æŸ¥**ï¼š
```python
from typing import get_type_hints
import inspect

def validate_method_signature(func):
    """æ–¹æ³•ç­¾åéªŒè¯è£…é¥°å™¨"""
    sig = inspect.signature(func)
    type_hints = get_type_hints(func)

    def wrapper(*args, **kwargs):
        # ç»‘å®šå‚æ•°
        bound_args = sig.bind(*args, **kwargs)
        bound_args.apply_defaults()

        # ç±»å‹æ£€æŸ¥
        for param_name, param_value in bound_args.arguments.items():
            if param_name in type_hints:
                expected_type = type_hints[param_name]
                if not isinstance(param_value, expected_type):
                    try:
                        # å°è¯•ç±»å‹è½¬æ¢
                        bound_args.arguments[param_name] = expected_type(param_value)
                    except (ValueError, TypeError):
                        raise TypeError(
                            f"å‚æ•° {param_name} ç±»å‹é”™è¯¯ï¼ŒæœŸæœ› {expected_type.__name__}ï¼Œ"
                            f"å®é™… {type(param_value).__name__}"
                        )

        return func(*bound_args.args, **bound_args.kwargs)

    return wrapper

@validate_method_signature
def process_stock_data(order_book_id: str, price_data: pd.DataFrame) -> Dict:
    """å¤„ç†è‚¡ç¥¨æ•°æ®ï¼Œå¸¦ç±»å‹éªŒè¯ï¼ˆä½¿ç”¨RQDatacå­—æ®µåï¼‰"""
    return {
        'order_book_id': order_book_id,
        'avg_price': price_data['close'].mean(),
        'volatility': price_data['close'].std(),
        'data_points': len(price_data)
    }
```

**æ•°æ®å®Œæ•´æ€§éªŒè¯**ï¼š
```python
def validate_data_integrity(data: Union[pd.DataFrame, Dict, List]) -> bool:
    """æ•°æ®å®Œæ•´æ€§éªŒè¯ï¼ˆRQDatacå­—æ®µè§„èŒƒï¼‰"""
    try:
        if isinstance(data, pd.DataFrame):
            # DataFrameéªŒè¯
            if data.empty:
                return False

            # æ£€æŸ¥å¿…éœ€åˆ—ï¼ˆRQDatacæ ‡å‡†å­—æ®µï¼‰
            required_columns = ['order_book_id', 'date', 'close']
            if not all(col in data.columns for col in required_columns):
                return False

            # æ£€æŸ¥æ•°æ®ç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(data['date']):
                return False

            # æ£€æŸ¥ç©ºå€¼æ¯”ä¾‹
            null_ratio = data.isnull().mean()
            if (null_ratio > 0.5).any():
                return False

        elif isinstance(data, dict):
            # å­—å…¸éªŒè¯ï¼ˆä½¿ç”¨RQDatacå­—æ®µï¼‰
            required_keys = ['order_book_id', 'data']
            if not all(key in data for key in required_keys):
                return False

            # é€’å½’éªŒè¯åµŒå¥—æ•°æ®
            if 'data' in data and isinstance(data['data'], list):
                return all(validate_data_integrity(item) for item in data['data'])

        elif isinstance(data, list):
            # åˆ—è¡¨éªŒè¯
            return all(validate_data_integrity(item) for item in data)

        return True

    except Exception as e:
        logger.error(f"æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
        return False
```

## ï¿½ğŸ”§ ä»£ç è´¨é‡ä¼˜åŒ–æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†å’Œæ—¥å¿—

**ç»Ÿä¸€é”™è¯¯å¤„ç†æ¨¡å¼**ï¼š
```python
def safe_operation(func):
    """è£…é¥°å™¨ï¼šç»Ÿä¸€é”™è¯¯å¤„ç†"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"{func.__name__} å¤±è´¥: {e}")
            return None
    return wrapper

@safe_operation
def risky_operation(self):
    # ä¸šåŠ¡é€»è¾‘
    pass
```

### 2. åŸå­æ–‡ä»¶æ“ä½œ

**ä¿è¯æ•°æ®å®Œæ•´æ€§**ï¼š
```python
def save_data_atomically(self, data: Dict, filename: str) -> bool:
    """åŸå­æ–‡ä»¶å†™å…¥"""
    filepath = os.path.join(self.data_dir, filename)
    temp_filepath = filepath + ".tmp"

    try:
        # å†™å…¥ä¸´æ—¶æ–‡ä»¶
        with open(temp_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, separators=(',', ':'))

        # åŸå­ç§»åŠ¨
        if os.path.exists(filepath):
            os.remove(filepath)
        os.rename(temp_filepath, filepath)

        return True
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_filepath):
            os.remove(temp_filepath)
        return False
```

### 3. èµ„æºç®¡ç†ä¼˜åŒ–

**ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¨¡å¼**ï¼š
```python
class DataProcessor:
    def __enter__(self):
        self.temp_files = []
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in self.temp_files:
            try:
                os.remove(temp_file)
            except:
                pass

# ä½¿ç”¨ç¤ºä¾‹
with DataProcessor() as processor:
    processor.process_data(data)
    # è‡ªåŠ¨æ¸…ç†èµ„æº
```

## ğŸ“Š æ€§èƒ½ç›‘æ§æœ€ä½³å®è·µ

### 1. æ€§èƒ½æŒ‡æ ‡æ”¶é›†

**æ—¶é—´ç»Ÿè®¡è£…é¥°å™¨**ï¼š
```python
import time
from functools import wraps

def performance_monitor(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start_time

        logger.info(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {duration:.4f}ç§’")
        return result
    return wrapper

@performance_monitor
def heavy_computation(self):
    # è€—æ—¶æ“ä½œ
    pass
```

### 2. å†…å­˜ä½¿ç”¨ç›‘æ§

**å†…å­˜ä½¿ç”¨ç»Ÿè®¡**ï¼š
```python
import psutil
import os

def get_memory_usage():
    """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        'rss': memory_info.rss / 1024 / 1024,  # MB
        'vms': memory_info.vms / 1024 / 1024,  # MB
        'percent': process.memory_percent()
    }

# ä½¿ç”¨ç¤ºä¾‹
before = get_memory_usage()
heavy_operation()
after = get_memory_usage()
logger.info(f"å†…å­˜ä½¿ç”¨å˜åŒ–: {after['rss'] - before['rss']:.2f}MB")
```

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´æœ€ä½³å®è·µ

### 1. ç¯å¢ƒå˜é‡é…ç½®

**æ ‡å‡†åŒ–ç¯å¢ƒå˜é‡**ï¼š
```bash
# ç”Ÿäº§ç¯å¢ƒæ ‡è¯†
export PRODUCTION=true

# æ€§èƒ½è°ƒä¼˜å‚æ•°
export MAX_CACHE_SIZE=2000
export CACHE_EXPIRY_SECONDS=7200
export MAX_PROCESSES=16

# è°ƒè¯•æ¨¡å¼
export DEBUG=false
```

### 2. å¥åº·æ£€æŸ¥

**ç³»ç»ŸçŠ¶æ€ç›‘æ§**ï¼š
```python
def health_check():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    checks = {
        'cpu_usage': psutil.cpu_percent(),
        'memory_usage': psutil.virtual_memory().percent,
        'disk_usage': psutil.disk_usage('/').percent,
        'cache_size': len(cache_manager.cache),
        'active_processes': len(multiprocessing.active_children())
    }

    # æ£€æŸ¥é˜ˆå€¼
    alerts = []
    if checks['memory_usage'] > 90:
        alerts.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
    if checks['disk_usage'] > 95:
        alerts.append("ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜")

    return {'status': 'healthy' if not alerts else 'warning', 'checks': checks, 'alerts': alerts}
```

## ğŸ“ˆ ä¼˜åŒ–æ•ˆæœæ€»ç»“

### æ€§èƒ½æå‡ç»Ÿè®¡

| ä¼˜åŒ–é¡¹ç›® | æå‡å¹…åº¦ | å…·ä½“æ”¶ç›Š |
|---------|---------|---------|
| å¹¶è¡Œå¤„ç† | 61.4x | CPUåˆ©ç”¨ç‡å¤§å¹…æå‡ |
| æ•°æ®æ‹·è´ | 4.7% | å†…å­˜ä½¿ç”¨ä¼˜åŒ– |
| æ–‡ä»¶I/O | 51% | æ–‡ä»¶å¤§å°å‡å°‘ï¼Œè¯»å†™åŠ é€Ÿ6.5% |
| å†…å­˜ç¼“å­˜ | åŠ¨æ€ | LRUæ·˜æ±°ï¼Œè¿‡æœŸæ¸…ç† |
| æ‰¹é‡å¤„ç† | æ˜¾è‘— | æ—¥å¿—é¢‘ç‡ä¼˜åŒ–ï¼Œå¤„ç†æ•ˆç‡æå‡ |

### ä»£ç è´¨é‡æå‡

- âœ… **æ¨¡å—åŒ–ç¨‹åº¦**ï¼šèŒè´£åˆ†ç¦»ï¼Œä»£ç å¤ç”¨æ€§æå‡
- âœ… **å¯ç»´æŠ¤æ€§**ï¼šæ¸…æ™°çš„ä»£ç ç»“æ„ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹
- âœ… **å¯æ‰©å±•æ€§**ï¼šæ’ä»¶åŒ–è®¾è®¡ï¼Œä¾¿äºåŠŸèƒ½æ‰©å±•
- âœ… **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•

## ğŸ¯ åº”ç”¨æŒ‡å¯¼

### é€‚ç”¨äºStockMonitorç³»ç»Ÿçš„ä¼˜åŒ–ç­–ç•¥

1. **ä¼˜å…ˆçº§æ’åº**ï¼š
   - é«˜ä¼˜å…ˆçº§ï¼šå¹¶è¡Œå¤„ç†ä¼˜åŒ–ã€æ•°æ®æ‹·è´ä¼˜åŒ–
   - ä¸­ä¼˜å…ˆçº§ï¼šæ–‡ä»¶I/Oä¼˜åŒ–ã€å†…å­˜ç¼“å­˜ä¼˜åŒ–
   - ä½ä¼˜å…ˆçº§ï¼šä»£ç é‡æ„ã€æ–‡æ¡£å®Œå–„

2. **å®æ–½æ­¥éª¤**ï¼š
   - è¯„ä¼°å½“å‰ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ
   - å‚è€ƒæœ¬æ–‡æ¡£é€‰æ‹©é€‚ç”¨çš„ä¼˜åŒ–ç­–ç•¥
   - å°æ­¥å¿«è·‘ï¼Œé€æ­¥å®æ–½ä¼˜åŒ–
   - å»ºç«‹æ€§èƒ½ç›‘æ§ï¼ŒéªŒè¯ä¼˜åŒ–æ•ˆæœ

3. **æ³¨æ„äº‹é¡¹**ï¼š
   - ä¼˜åŒ–å‰è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
   - ä¿æŒå‘åå…¼å®¹æ€§
   - å……åˆ†æµ‹è¯•ä¼˜åŒ–åçš„åŠŸèƒ½
   - å»ºç«‹å›æ»šæœºåˆ¶

---

*æœ¬æ–‡æ¡£åŸºäºStockPoolç³»ç»Ÿçš„å®é™…ä¼˜åŒ–ç»éªŒæ€»ç»“ï¼ŒæŒç»­æ›´æ–°ä¸­ã€‚å¦‚æœ‰æ–°çš„ä¼˜åŒ–å®è·µï¼Œè¯·åŠæ—¶è¡¥å……ã€‚*

## ğŸ”§ RQDatacå­—æ®µæ ‡å‡†åŒ–ä½¿ç”¨æŒ‡å—

### åœ¨StockPoolç³»ç»Ÿä¸­åº”ç”¨å­—æ®µæ ‡å‡†

**1. æ•°æ®è¾“å…¥æ ‡å‡†åŒ–**ï¼š
```python
# åœ¨æ•°æ®åŠ è½½æ—¶åº”ç”¨æ ‡å‡†åŒ–
def load_and_normalize_data(file_path: str) -> pd.DataFrame:
    """åŠ è½½å¹¶æ ‡å‡†åŒ–æ•°æ®"""
    df = pd.read_csv(file_path)

    # åº”ç”¨å­—æ®µåˆ«åæ˜ å°„
    df = normalize_rqdatac_fields(df)

    # æ ‡å‡†åŒ–æ•°æ®ç±»å‹
    df = standardize_data_types(df)

    # éªŒè¯åˆè§„æ€§
    validation = validate_rqdatac_compliance(df)
    if validation['compliance_score'] < 0.8:
        logger.warning(f"æ•°æ®åˆè§„æ€§å¾—åˆ†: {validation['compliance_score']:.2f}")

    return df
```

**2. APIæ•°æ®è·å–æ ‡å‡†åŒ–**ï¼š
```python
def fetch_rqdatac_data(order_book_ids: List[str], start_date: str, end_date: str) -> pd.DataFrame:
    """ä»RQDatacè·å–æ ‡å‡†åŒ–æ•°æ®"""
    # è·å–ä»·æ ¼æ•°æ®
    price_data = rqdatac.get_price(
        order_book_ids=order_book_ids,
        start_date=start_date,
        end_date=end_date,
        fields=RQDATAC_API_FIELDS['get_price']
    )

    # è·å–åŸºæœ¬é¢æ•°æ®
    fundamentals_data = rqdatac.get_fundamentals(
        order_book_ids=order_book_ids,
        date=end_date,
        fields=RQDATAC_API_FIELDS['get_fundamentals']
    )

    # åˆå¹¶æ•°æ®
    df = pd.merge(price_data, fundamentals_data, on='order_book_id', how='left')

    # æ ‡å‡†åŒ–å¤„ç†
    df = standardize_data_types(df)

    return df
```

**3. æ•°æ®è¾“å‡ºæ ¼å¼åŒ–**ï¼š
```python
def export_normalized_data(df: pd.DataFrame, output_path: str, use_aliases: bool = False):
    """å¯¼å‡ºæ ‡å‡†åŒ–æ•°æ®"""
    export_df = df.copy()

    if use_aliases:
        # è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„åˆ«å
        rename_dict = {v: k for k, v in FIELD_ALIASES.items() if v in export_df.columns}
        export_df = export_df.rename(columns=rename_dict)

    # ä¼˜åŒ–JSONæ ¼å¼
    export_data = optimize_json_format(export_df.to_dict('records'))

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(export_data)
```

### å­—æ®µä¸€è‡´æ€§æ£€æŸ¥å·¥å…·

**å®šæœŸæ£€æŸ¥å­—æ®µä½¿ç”¨æƒ…å†µ**ï¼š
```python
def audit_field_consistency():
    """å®¡è®¡å­—æ®µä½¿ç”¨ä¸€è‡´æ€§"""
    # æ£€æŸ¥stockpool.pyä¸­çš„å­—æ®µä½¿ç”¨
    stockpool_fields = extract_fields_from_code('stockpool.py')

    # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„å­—æ®µå®šä¹‰
    config_fields = extract_fields_from_config('config/*.py')

    # å¯¹æ¯”RQDatacæ ‡å‡†
    consistency_report = {
        'standard_compliance': check_standard_compliance(stockpool_fields),
        'alias_usage': check_alias_usage(stockpool_fields),
        'inconsistencies': find_inconsistencies(stockpool_fields, config_fields)
    }

    return consistency_report
```

### æœ€ä½³å®è·µå»ºè®®

1. **å­—æ®µå‘½å**ï¼š
   - å§‹ç»ˆä½¿ç”¨RQDatacæ ‡å‡†å­—æ®µåä½œä¸ºå†…éƒ¨è¡¨ç¤º
   - ä»…åœ¨ç”¨æˆ·ç•Œé¢æˆ–è¾“å‡ºæ—¶ä½¿ç”¨åˆ«å
   - å»ºç«‹å­—æ®µå‘½åè§„èŒƒæ–‡æ¡£

2. **æ•°æ®ç±»å‹**ï¼š
   - ä¸¥æ ¼æŒ‰ç…§RQDATAC_FIELD_MAPPINGå®šä¹‰çš„æ•°æ®ç±»å‹
   - ä½¿ç”¨é€‚å½“çš„æ•°å€¼ç²¾åº¦ï¼ˆfloat32 vs float64ï¼‰
   - å……åˆ†åˆ©ç”¨pandasçš„categoryç±»å‹

3. **å…¼å®¹æ€§å¤„ç†**ï¼š
   - å®ç°å­—æ®µåˆ«åè‡ªåŠ¨æ˜ å°„
   - æ”¯æŒå¤šç§æ•°æ®æºçš„å­—æ®µæ ¼å¼
   - æä¾›å­—æ®µéªŒè¯å’Œè½¬æ¢å·¥å…·

4. **ç»´æŠ¤æ›´æ–°**ï¼š
   - å®šæœŸæ£€æŸ¥RQDatac APIå˜æ›´
   - æ›´æ–°å­—æ®µæ˜ å°„å­—å…¸
   - ç»´æŠ¤å‘åå…¼å®¹æ€§

é€šè¿‡éµå¾ªè¿™äº›æ ‡å‡†åŒ–å®è·µï¼Œå¯ä»¥ç¡®ä¿StockPoolç³»ç»Ÿä¸RQDatacå®Œå…¨å…¼å®¹ï¼Œæé«˜æ•°æ®å¤„ç†æ•ˆç‡ï¼Œå¹¶é™ä½ç»´æŠ¤æˆæœ¬ã€‚

## ğŸ” RQDatac APIå­—æ®µæ¢ç´¢ç»“æœ

### å­—æ®µåŒ¹é…åˆ†æ

åŸºäºRQDatac APIçš„å®é™…å­—æ®µæ¨æ–­ï¼Œæˆ‘ä»¬è·å¾—äº†ä»¥ä¸‹é‡è¦å‘ç°ï¼š

#### âœ… é«˜åŒ¹é…ç‡API (100%åŒ¹é…)
- **get_price**: 15ä¸ªå­—æ®µï¼Œå®Œå…¨åŒ¹é…æ–‡æ¡£å®šä¹‰
- **get_factor**: 18ä¸ªå­—æ®µï¼Œå®Œå…¨åŒ¹é…æ–‡æ¡£å®šä¹‰  
- **get_shares**: 11ä¸ªå­—æ®µï¼Œå®Œå…¨åŒ¹é…æ–‡æ¡£å®šä¹‰

#### âš ï¸ ä¸­ç­‰åŒ¹é…ç‡API (75%-92%åŒ¹é…)
- **get_basic_info**: 28ä¸ªå­—æ®µï¼Œ92.3%åŒ¹é…ï¼Œç¼ºå¤±`circulation_market_cap`
- **get_industry**: 10ä¸ªå­—æ®µï¼Œ75%åŒ¹é…ï¼Œç¼ºå¤±`order_book_id`

### æ–°å‘ç°çš„é‡è¦å­—æ®µ

#### ğŸ“ˆ get_priceæ–°å¢å­—æ®µ
```python
# RQDatacå®é™…æä¾›çš„é¢å¤–ä»·æ ¼å­—æ®µ
'pre_close',      # æ˜¨æ”¶ä»·
'change',         # æ¶¨è·Œé¢  
'change_pct',     # æ¶¨è·Œå¹…
'amplitude',      # æŒ¯å¹…
'turnover_ratio'  # æ¢æ‰‹ç‡
```

#### ğŸ¢ get_basic_infoæ–°å¢å­—æ®µ
```python
# RQDatacå®é™…æä¾›çš„åŸºæœ¬é¢å­—æ®µ
'symbol', 'company_name', 'industry_code', 'industry_name',
'sector_code', 'area_code', 'listed_date', 'total_shares',
'float_shares', 'float_market_cap', 'total_assets',
'total_liabilities', 'total_equity', 'net_profit',
'operating_revenue', 'operating_cost'
```

#### ğŸ“Š get_factoræ–°å¢å­—æ®µ
```python
# RQDatacå®é™…æä¾›çš„å› å­å­—æ®µ
'factor_name', 'factor_value', 'up_down_ratio',
'volume_ma_ratio', 'price_ma_ratio', 'momentum',
'volatility', 'liquidity', 'quality', 'value',
'growth', 'size'
```

### ğŸ¯ å®æ–½å»ºè®®

#### 1. å­—æ®µæ‰©å±•ç­–ç•¥
```python
def extend_stockpool_with_rqdatac_fields():
    """æ‰©å±•StockPoolä»¥åˆ©ç”¨RQDatacçš„ä¸°å¯Œå­—æ®µ"""
    
    # åˆ©ç”¨æ–°å¢çš„ä»·æ ¼å­—æ®µ
    extended_price_fields = [
        'pre_close', 'change', 'change_pct', 'amplitude'
    ]
    
    # åˆ©ç”¨æ–°å¢çš„åŸºæœ¬é¢å­—æ®µ
    extended_fundamental_fields = [
        'listed_date', 'total_shares', 'float_market_cap',
        'total_assets', 'net_profit'
    ]
    
    # åˆ©ç”¨æ–°å¢çš„å› å­å­—æ®µ
    extended_factor_fields = [
        'momentum', 'volatility', 'quality', 'value', 'growth'
    ]
    
    return {
        'price': extended_price_fields,
        'fundamental': extended_fundamental_fields,
        'factor': extended_factor_fields
    }
```

#### 2. APIè°ƒç”¨ä¼˜åŒ–
```python
def optimized_rqdatac_api_calls():
    """ä¼˜åŒ–çš„RQDatac APIè°ƒç”¨ç­–ç•¥"""
    
    # æ‰¹é‡è·å–ç­–ç•¥
    batch_apis = {
        'price': 'get_price',           # ä»·æ ¼æ•°æ® - é«˜é¢‘æ›´æ–°
        'basic': 'get_basic_info',      # åŸºæœ¬é¢æ•°æ® - æ¯æ—¥æ›´æ–°
        'factor': 'get_factor',         # å› å­æ•°æ® - æ¯æ—¥è®¡ç®—
        'shares': 'get_shares',         # è‚¡æœ¬æ•°æ® - å˜åŠ¨æ—¶æ›´æ–°
        'industry': 'get_industry'      # è¡Œä¸šæ•°æ® - å®šæœŸæ›´æ–°
    }
    
    # å­—æ®µé€‰æ‹©ç­–ç•¥
    essential_fields = {
        'get_price': ['order_book_id', 'date', 'open', 'close', 'volume'],
        'get_basic_info': ['order_book_id', 'pe_ratio', 'pb_ratio', 'roe'],
        'get_factor': ['order_book_id', 'volume_ratio', 'momentum']
    }
    
    return batch_apis, essential_fields
```

#### 3. æ•°æ®å­˜å‚¨ä¼˜åŒ–
```python
def optimize_data_storage_schema():
    """ä¼˜åŒ–æ•°æ®å­˜å‚¨æ¨¡å¼ä»¥é€‚åº”RQDatacå­—æ®µ"""
    
    # æ‰©å±•çš„å­—æ®µç±»å‹æ˜ å°„
    extended_dtypes = {
        'pre_close': 'float32',
        'change': 'float32', 
        'change_pct': 'float32',
        'amplitude': 'float32',
        'listed_date': 'datetime64[ns]',
        'factor_name': 'category',
        'factor_value': 'float32',
        'momentum': 'float32',
        'volatility': 'float32'
    }
    
    return extended_dtypes
```

### ğŸ“‹ è¿ç§»è·¯çº¿å›¾

#### é˜¶æ®µ1: æ ¸å¿ƒå­—æ®µå…¼å®¹ (å·²å®Œæˆ)
- âœ… å®ç°åŸºç¡€RQDatacå­—æ®µæ˜ å°„
- âœ… æ›´æ–°å­—æ®µæ ‡å‡†åŒ–å‡½æ•°
- âœ… éªŒè¯å­—æ®µç±»å‹ä¸€è‡´æ€§

#### é˜¶æ®µ2: æ‰©å±•å­—æ®µåˆ©ç”¨ (è¿›è¡Œä¸­)
- ğŸ”„ æ·»åŠ æ–°å‘ç°å­—æ®µçš„æ”¯æŒ
- ğŸ”„ æ›´æ–°æ•°æ®å­˜å‚¨æ¨¡å¼
- ğŸ”„ æ‰©å±•æŠ€æœ¯æŒ‡æ ‡è®¡ç®—

#### é˜¶æ®µ3: é«˜çº§åŠŸèƒ½é›†æˆ (è®¡åˆ’ä¸­)
- ğŸ“… å®ç°å› å­æ•°æ®æ·±åº¦åˆ†æ
- ğŸ“… é›†æˆå¸‚åœºæƒ…ç»ªæŒ‡æ ‡
- ğŸ“… ä¼˜åŒ–æ•°æ®æ›´æ–°é¢‘ç‡

### ğŸ”§ ä»£ç æ›´æ–°ç¤ºä¾‹

#### æ›´æ–°å­—æ®µéªŒè¯å‡½æ•°
```python
def validate_extended_rqdatac_fields(df: pd.DataFrame) -> Dict[str, List[str]]:
    """éªŒè¯æ‰©å±•çš„RQDatacå­—æ®µ"""
    
    validation_results = {
        'valid_fields': [],
        'new_fields': [],
        'type_issues': [],
        'recommendations': []
    }
    
    # æ£€æŸ¥æ–°å¢çš„ä»·æ ¼å­—æ®µ
    price_extensions = ['pre_close', 'change', 'change_pct', 'amplitude']
    for field in price_extensions:
        if field in df.columns:
            validation_results['new_fields'].append(field)
            # éªŒè¯æ•°æ®ç±»å‹å’Œè´¨é‡
            if df[field].dtype != 'float32':
                validation_results['type_issues'].append(f"{field}ç±»å‹åº”ä¸ºfloat32")
    
    # æ£€æŸ¥æ–°å¢çš„åŸºæœ¬é¢å­—æ®µ
    fundamental_extensions = ['listed_date', 'total_shares', 'net_profit']
    for field in fundamental_extensions:
        if field in df.columns:
            validation_results['new_fields'].append(field)
    
    return validation_results
```

#### æ›´æ–°æ•°æ®è·å–å‡½æ•°
```python
def fetch_comprehensive_stock_data(order_book_ids: List[str]) -> Dict[str, pd.DataFrame]:
    """è·å–å…¨é¢çš„è‚¡ç¥¨æ•°æ®"""
    
    comprehensive_data = {}
    
    try:
        # è·å–ä»·æ ¼æ•°æ®ï¼ˆåŒ…å«æ–°å¢å­—æ®µï¼‰
        price_data = rqdatac.get_price(
            order_book_ids=order_book_ids,
            start_date='2024-01-01',
            end_date='2024-12-31',
            fields=None  # è·å–æ‰€æœ‰å¯ç”¨å­—æ®µ
        )
        comprehensive_data['price'] = price_data
        
        # è·å–åŸºæœ¬é¢æ•°æ®ï¼ˆåŒ…å«æ–°å¢å­—æ®µï¼‰
        basic_data = rqdatac.get_basic_info(order_book_ids)
        comprehensive_data['basic'] = basic_data
        
        # è·å–å› å­æ•°æ®ï¼ˆåŒ…å«æ–°å¢å­—æ®µï¼‰
        factor_data = rqdatac.get_factor(
            order_book_ids=order_book_ids,
            factor=['volume_ratio', 'momentum', 'quality']
        )
        comprehensive_data['factor'] = factor_data
        
    except Exception as e:
        logger.error(f"è·å–ç»¼åˆæ•°æ®å¤±è´¥: {e}")
    
    return comprehensive_data
```

### ğŸ‰ æ€»ç»“

é€šè¿‡RQDatac APIå­—æ®µæ¢ç´¢ï¼Œæˆ‘ä»¬å‘ç°äº†å¤§é‡æœ‰ä»·å€¼çš„å­—æ®µï¼Œè¿™äº›å­—æ®µå°†æ˜¾è‘—å¢å¼ºStockPoolç³»ç»Ÿçš„åˆ†æèƒ½åŠ›å’Œæ•°æ®ä¸°å¯Œæ€§ï¼š

- **ä»·æ ¼æ•°æ®**: æ–°å¢5ä¸ªæŠ€æœ¯æŒ‡æ ‡å­—æ®µ
- **åŸºæœ¬é¢æ•°æ®**: æ–°å¢16ä¸ªè´¢åŠ¡å’Œå…¬å¸ä¿¡æ¯å­—æ®µ  
- **å› å­æ•°æ®**: æ–°å¢12ä¸ªé‡åŒ–å› å­å­—æ®µ
- **è¡Œä¸šæ•°æ®**: å®Œå–„è¡Œä¸šåˆ†ç±»ä½“ç³»
- **è‚¡æœ¬æ•°æ®**: æ–°å¢5ä¸ªè‚¡æœ¬å˜åŠ¨ç›¸å…³å­—æ®µ

è¿™äº›å‘ç°ä¸ºStockPoolç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†é‡è¦æ–¹å‘ï¼Œå»ºè®®æŒ‰é˜¶æ®µé€æ­¥é›†æˆè¿™äº›æ–°å­—æ®µï¼Œä»¥å……åˆ†åˆ©ç”¨RQDatacæ•°æ®å¹³å°çš„å¼ºå¤§åŠŸèƒ½ã€‚</content>
<parameter name="filePath">/home/xiaqing/projects/stockman/docs/STOCKPOOL_OPTIMIZATION_BEST_PRACTICES.md
